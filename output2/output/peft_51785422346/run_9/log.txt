Requirement already satisfied: pip in ./venv/lib/python3.11/site-packages (22.0.2)
Collecting pip
  Using cached pip-25.2-py3-none-any.whl (1.8 MB)
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 22.0.2
    Uninstalling pip-22.0.2:
      Successfully uninstalled pip-22.0.2
Successfully installed pip-25.2
Requirement already satisfied: setuptools in ./venv/lib/python3.11/site-packages (59.6.0)
Obtaining file:///app
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Collecting numpy>=1.17 (from peft==0.17.2.dev0)
  Using cached numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)
Collecting packaging>=20.0 (from peft==0.17.2.dev0)
  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)
Collecting psutil (from peft==0.17.2.dev0)
  Using cached psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)
Collecting pyyaml (from peft==0.17.2.dev0)
  Using cached pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)
Collecting torch>=1.13.0 (from peft==0.17.2.dev0)
  Using cached torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)
Collecting transformers (from peft==0.17.2.dev0)
  Using cached transformers-4.57.0-py3-none-any.whl.metadata (41 kB)
Collecting tqdm (from peft==0.17.2.dev0)
  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)
Collecting accelerate>=0.21.0 (from peft==0.17.2.dev0)
  Using cached accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)
Collecting safetensors (from peft==0.17.2.dev0)
  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
Collecting huggingface_hub>=0.25.0 (from peft==0.17.2.dev0)
  Using cached huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)
Collecting black (from peft==0.17.2.dev0)
  Using cached black-25.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (83 kB)
Collecting hf-doc-builder (from peft==0.17.2.dev0)
  Using cached hf_doc_builder-0.5.0-py3-none-any.whl.metadata (28 kB)
Collecting ruff~=0.12.8 (from peft==0.17.2.dev0)
  Using cached ruff-0.12.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)
Collecting pytest (from peft==0.17.2.dev0)
  Using cached pytest-8.4.2-py3-none-any.whl.metadata (7.7 kB)
Collecting pytest-cov (from peft==0.17.2.dev0)
  Using cached pytest_cov-7.0.0-py3-none-any.whl.metadata (31 kB)
Collecting pytest-xdist (from peft==0.17.2.dev0)
  Using cached pytest_xdist-3.8.0-py3-none-any.whl.metadata (3.0 kB)
Collecting parameterized (from peft==0.17.2.dev0)
  Using cached parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)
Collecting datasets (from peft==0.17.2.dev0)
  Using cached datasets-4.1.1-py3-none-any.whl.metadata (18 kB)
Collecting diffusers (from peft==0.17.2.dev0)
  Using cached diffusers-0.35.1-py3-none-any.whl.metadata (20 kB)
Collecting scipy (from peft==0.17.2.dev0)
  Using cached scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)
Collecting protobuf (from peft==0.17.2.dev0)
  Using cached protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)
Collecting sentencepiece (from peft==0.17.2.dev0)
  Using cached sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)
Collecting filelock (from huggingface_hub>=0.25.0->peft==0.17.2.dev0)
  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)
Collecting fsspec>=2023.5.0 (from huggingface_hub>=0.25.0->peft==0.17.2.dev0)
  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)
Collecting requests (from huggingface_hub>=0.25.0->peft==0.17.2.dev0)
  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)
Collecting typing-extensions>=3.7.4.3 (from huggingface_hub>=0.25.0->peft==0.17.2.dev0)
  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)
Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub>=0.25.0->peft==0.17.2.dev0)
  Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)
Collecting sympy>=1.13.3 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)
Collecting networkx (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)
Collecting jinja2 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)
Collecting nvidia-nccl-cu12==2.27.3 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)
Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting triton==3.4.0 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)
Requirement already satisfied: setuptools>=40.8.0 in ./venv/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.13.0->peft==0.17.2.dev0) (59.6.0)
Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.13.0->peft==0.17.2.dev0)
  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)
Collecting click>=8.0.0 (from black->peft==0.17.2.dev0)
  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)
Collecting mypy-extensions>=0.4.3 (from black->peft==0.17.2.dev0)
  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)
Collecting pathspec>=0.9.0 (from black->peft==0.17.2.dev0)
  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)
Collecting platformdirs>=2 (from black->peft==0.17.2.dev0)
  Using cached platformdirs-4.4.0-py3-none-any.whl.metadata (12 kB)
Collecting pytokens>=0.1.10 (from black->peft==0.17.2.dev0)
  Using cached pytokens-0.1.10-py3-none-any.whl.metadata (2.0 kB)
Collecting pyarrow>=21.0.0 (from datasets->peft==0.17.2.dev0)
  Using cached pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)
Collecting dill<0.4.1,>=0.3.0 (from datasets->peft==0.17.2.dev0)
  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)
Collecting pandas (from datasets->peft==0.17.2.dev0)
  Using cached pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)
Collecting xxhash (from datasets->peft==0.17.2.dev0)
  Using cached xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)
Collecting multiprocess<0.70.17 (from datasets->peft==0.17.2.dev0)
  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)
Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)
Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)
Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)
Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)
Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)
Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached propcache-0.4.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)
Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)
Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)
Collecting charset_normalizer<4,>=2 (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0)
  Using cached charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)
Collecting urllib3<3,>=1.21.1 (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0)
  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)
Collecting certifi>=2017.4.17 (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0)
  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)
Collecting importlib_metadata (from diffusers->peft==0.17.2.dev0)
  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)
Collecting regex!=2019.12.17 (from diffusers->peft==0.17.2.dev0)
  Using cached regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)
Collecting Pillow (from diffusers->peft==0.17.2.dev0)
  Using cached pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)
Collecting GitPython (from hf-doc-builder->peft==0.17.2.dev0)
  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)
Collecting nbformat (from hf-doc-builder->peft==0.17.2.dev0)
  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)
Collecting gitdb<5,>=4.0.1 (from GitPython->hf-doc-builder->peft==0.17.2.dev0)
  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)
Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython->hf-doc-builder->peft==0.17.2.dev0)
  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)
Collecting zipp>=3.20 (from importlib_metadata->diffusers->peft==0.17.2.dev0)
  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)
Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.13.0->peft==0.17.2.dev0)
  Using cached markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)
Collecting fastjsonschema>=2.15 (from nbformat->hf-doc-builder->peft==0.17.2.dev0)
  Using cached fastjsonschema-2.21.2-py3-none-any.whl.metadata (2.3 kB)
Collecting jsonschema>=2.6 (from nbformat->hf-doc-builder->peft==0.17.2.dev0)
  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)
Collecting jupyter-core!=5.0.*,>=4.12 (from nbformat->hf-doc-builder->peft==0.17.2.dev0)
  Using cached jupyter_core-5.8.1-py3-none-any.whl.metadata (1.6 kB)
Collecting traitlets>=5.1 (from nbformat->hf-doc-builder->peft==0.17.2.dev0)
  Using cached traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)
Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.6->nbformat->hf-doc-builder->peft==0.17.2.dev0)
  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)
Collecting referencing>=0.28.4 (from jsonschema>=2.6->nbformat->hf-doc-builder->peft==0.17.2.dev0)
  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)
Collecting rpds-py>=0.7.1 (from jsonschema>=2.6->nbformat->hf-doc-builder->peft==0.17.2.dev0)
  Using cached rpds_py-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
Collecting python-dateutil>=2.8.2 (from pandas->datasets->peft==0.17.2.dev0)
  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
Collecting pytz>=2020.1 (from pandas->datasets->peft==0.17.2.dev0)
  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)
Collecting tzdata>=2022.7 (from pandas->datasets->peft==0.17.2.dev0)
  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)
Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets->peft==0.17.2.dev0)
  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)
Collecting iniconfig>=1 (from pytest->peft==0.17.2.dev0)
  Using cached iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)
Collecting pluggy<2,>=1.5 (from pytest->peft==0.17.2.dev0)
  Using cached pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)
Collecting pygments>=2.7.2 (from pytest->peft==0.17.2.dev0)
  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)
Collecting coverage>=7.10.6 (from coverage[toml]>=7.10.6->pytest-cov->peft==0.17.2.dev0)
  Using cached coverage-7.10.7-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (8.9 kB)
Collecting execnet>=2.1 (from pytest-xdist->peft==0.17.2.dev0)
  Using cached execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)
Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers->peft==0.17.2.dev0)
  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)
Using cached ruff-0.12.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)
Using cached accelerate-1.10.1-py3-none-any.whl (374 kB)
Using cached numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)
Using cached huggingface_hub-0.35.3-py3-none-any.whl (564 kB)
Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)
Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)
Using cached packaging-25.0-py3-none-any.whl (66 kB)
Using cached pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)
Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)
Using cached torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)
Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)
Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)
Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)
Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)
Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)
Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)
Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)
Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)
Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)
Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)
Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)
Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)
Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)
Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)
Using cached triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)
Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)
Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)
Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)
Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)
Using cached black-25.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.6 MB)
Using cached click-8.3.0-py3-none-any.whl (107 kB)
Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)
Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)
Using cached platformdirs-4.4.0-py3-none-any.whl (18 kB)
Using cached pytokens-0.1.10-py3-none-any.whl (12 kB)
Using cached datasets-4.1.1-py3-none-any.whl (503 kB)
Using cached dill-0.4.0-py3-none-any.whl (119 kB)
Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)
Using cached aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)
Using cached multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)
Using cached yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)
Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)
Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)
Using cached attrs-25.3.0-py3-none-any.whl (63 kB)
Using cached frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)
Using cached idna-3.10-py3-none-any.whl (70 kB)
Using cached propcache-0.4.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (209 kB)
Using cached pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)
Using cached requests-2.32.5-py3-none-any.whl (64 kB)
Using cached charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)
Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)
Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)
Using cached diffusers-0.35.1-py3-none-any.whl (4.1 MB)
Using cached regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)
Using cached filelock-3.19.1-py3-none-any.whl (15 kB)
Using cached hf_doc_builder-0.5.0-py3-none-any.whl (67 kB)
Using cached gitpython-3.1.45-py3-none-any.whl (208 kB)
Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)
Using cached smmap-5.0.2-py3-none-any.whl (24 kB)
Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)
Using cached zipp-3.23.0-py3-none-any.whl (10 kB)
Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)
Using cached markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)
Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)
Using cached fastjsonschema-2.21.2-py3-none-any.whl (24 kB)
Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)
Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)
Using cached jupyter_core-5.8.1-py3-none-any.whl (28 kB)
Using cached referencing-0.36.2-py3-none-any.whl (26 kB)
Using cached rpds_py-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)
Using cached traitlets-5.14.3-py3-none-any.whl (85 kB)
Using cached networkx-3.5-py3-none-any.whl (2.0 MB)
Using cached pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)
Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)
Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)
Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)
Using cached parameterized-0.9.0-py2.py3-none-any.whl (20 kB)
Using cached pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)
Using cached protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl (322 kB)
Using cached psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)
Using cached pytest-8.4.2-py3-none-any.whl (365 kB)
Using cached pluggy-1.6.0-py3-none-any.whl (20 kB)
Using cached iniconfig-2.1.0-py3-none-any.whl (6.0 kB)
Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)
Using cached pytest_cov-7.0.0-py3-none-any.whl (22 kB)
Using cached coverage-7.10.7-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (250 kB)
Using cached pytest_xdist-3.8.0-py3-none-any.whl (46 kB)
Using cached execnet-2.1.1-py3-none-any.whl (40 kB)
Using cached scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)
Using cached sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)
Using cached transformers-4.57.0-py3-none-any.whl (12.0 MB)
Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)
Using cached xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)
Building wheels for collected packages: peft
  Building editable for peft (pyproject.toml): started
  Building editable for peft (pyproject.toml): finished with status 'done'
  Created wheel for peft: filename=peft-0.17.2.dev0-0.editable-py3-none-any.whl size=10766 sha256=a2d539d81c0134c4546843ca228174dfe4bf4c6391eff6f22e29d9a8fb3a615b
  Stored in directory: /tmp/pip-ephem-wheel-cache-c26na554/wheels/57/0f/98/bb57b2b57b95807699b822a35c022f139d38a02c27922f27ce
Successfully built peft
Installing collected packages: pytz, nvidia-cusparselt-cu12, mpmath, fastjsonschema, zipp, xxhash, urllib3, tzdata, typing-extensions, triton, traitlets, tqdm, sympy, smmap, six, sentencepiece, safetensors, ruff, rpds-py, regex, pyyaml, pytokens, pygments, pyarrow, psutil, protobuf, propcache, pluggy, platformdirs, Pillow, pathspec, parameterized, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, mypy-extensions, multidict, MarkupSafe, iniconfig, idna, hf-xet, fsspec, frozenlist, filelock, execnet, dill, coverage, click, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, scipy, requests, referencing, python-dateutil, pytest, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, jupyter-core, jinja2, importlib_metadata, gitdb, black, aiosignal, pytest-xdist, pytest-cov, pandas, nvidia-cusolver-cu12, jsonschema-specifications, huggingface_hub, GitPython, aiohttp, torch, tokenizers, jsonschema, diffusers, transformers, nbformat, datasets, accelerate, peft, hf-doc-builder

Successfully installed GitPython-3.1.45 MarkupSafe-3.0.3 Pillow-11.3.0 accelerate-1.10.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 black-25.9.0 certifi-2025.10.5 charset_normalizer-3.4.3 click-8.3.0 coverage-7.10.7 datasets-4.1.1 diffusers-0.35.1 dill-0.4.0 execnet-2.1.1 fastjsonschema-2.21.2 filelock-3.19.1 frozenlist-1.7.0 fsspec-2025.9.0 gitdb-4.0.12 hf-doc-builder-0.5.0 hf-xet-1.1.10 huggingface_hub-0.35.3 idna-3.10 importlib_metadata-8.7.0 iniconfig-2.1.0 jinja2-3.1.6 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 jupyter-core-5.8.1 mpmath-1.3.0 multidict-6.6.4 multiprocess-0.70.16 mypy-extensions-1.1.0 nbformat-5.10.4 networkx-3.5 numpy-2.3.3 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 packaging-25.0 pandas-2.3.3 parameterized-0.9.0 pathspec-0.12.1 peft-0.17.2.dev0 platformdirs-4.4.0 pluggy-1.6.0 propcache-0.4.0 protobuf-6.32.1 psutil-7.1.0 pyarrow-21.0.0 pygments-2.19.2 pytest-8.4.2 pytest-cov-7.0.0 pytest-xdist-3.8.0 python-dateutil-2.9.0.post0 pytokens-0.1.10 pytz-2025.2 pyyaml-6.0.3 referencing-0.36.2 regex-2025.9.18 requests-2.32.5 rpds-py-0.27.1 ruff-0.12.12 safetensors-0.6.2 scipy-1.16.2 sentencepiece-0.2.1 six-1.17.0 smmap-5.0.2 sympy-1.14.0 tokenizers-0.22.1 torch-2.8.0 tqdm-4.67.1 traitlets-5.14.3 transformers-4.57.0 triton-3.4.0 typing-extensions-4.15.0 tzdata-2025.2 urllib3-2.5.0 xxhash-3.6.0 yarl-1.20.1 zipp-3.23.0
Requirement already satisfied: torch in ./venv/lib/python3.11/site-packages (from -r examples/arrow_multitask/requirements.txt (line 1)) (2.8.0)
Requirement already satisfied: transformers in ./venv/lib/python3.11/site-packages (from -r examples/arrow_multitask/requirements.txt (line 2)) (4.57.0)
Requirement already satisfied: accelerate in ./venv/lib/python3.11/site-packages (from -r examples/arrow_multitask/requirements.txt (line 3)) (1.10.1)
Requirement already satisfied: datasets in ./venv/lib/python3.11/site-packages (from -r examples/arrow_multitask/requirements.txt (line 4)) (4.1.1)
Collecting scikit-learn (from -r examples/arrow_multitask/requirements.txt (line 5))
  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)
Requirement already satisfied: tqdm in ./venv/lib/python3.11/site-packages (from -r examples/arrow_multitask/requirements.txt (line 6)) (4.67.1)
Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (from -r examples/arrow_multitask/requirements.txt (line 7)) (2.3.3)
Collecting bitsandbytes (from -r examples/arrow_multitask/requirements.txt (line 8))
  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)
Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (3.19.1)
Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (4.15.0)
Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (1.14.0)
Requirement already satisfied: networkx in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (3.5)
Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (3.1.6)
Requirement already satisfied: fsspec in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (2025.9.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (12.8.93)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (12.8.90)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (12.8.90)
Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (9.10.2.21)
Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (12.8.4.1)
Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (11.3.3.83)
Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (10.3.9.90)
Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (11.7.3.90)
Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (12.5.8.93)
Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (0.7.1)
Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (2.27.3)
Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (12.8.90)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (12.8.93)
Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (1.13.1.3)
Requirement already satisfied: triton==3.4.0 in ./venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (3.4.0)
Requirement already satisfied: setuptools>=40.8.0 in ./venv/lib/python3.11/site-packages (from triton==3.4.0->torch->-r examples/arrow_multitask/requirements.txt (line 1)) (59.6.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./venv/lib/python3.11/site-packages (from transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (0.35.3)
Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.11/site-packages (from transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (25.0)
Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (6.0.3)
Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.11/site-packages (from transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (2025.9.18)
Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (2.32.5)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./venv/lib/python3.11/site-packages (from transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (0.22.1)
Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.11/site-packages (from transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (0.6.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (1.1.10)
Requirement already satisfied: psutil in ./venv/lib/python3.11/site-packages (from accelerate->-r examples/arrow_multitask/requirements.txt (line 3)) (7.1.0)
Requirement already satisfied: pyarrow>=21.0.0 in ./venv/lib/python3.11/site-packages (from datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (21.0.0)
Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./venv/lib/python3.11/site-packages (from datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (0.4.0)
Requirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (from datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (2.3.3)
Requirement already satisfied: xxhash in ./venv/lib/python3.11/site-packages (from datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (3.6.0)
Requirement already satisfied: multiprocess<0.70.17 in ./venv/lib/python3.11/site-packages (from datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (0.70.16)
Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./venv/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (3.12.15)
Requirement already satisfied: scipy>=1.8.0 in ./venv/lib/python3.11/site-packages (from scikit-learn->-r examples/arrow_multitask/requirements.txt (line 5)) (1.16.2)
Collecting joblib>=1.2.0 (from scikit-learn->-r examples/arrow_multitask/requirements.txt (line 5))
  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)
Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r examples/arrow_multitask/requirements.txt (line 5))
  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (0.4.0)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (3.10)
Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests->transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (3.4.3)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (2025.10.5)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch->-r examples/arrow_multitask/requirements.txt (line 1)) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch->-r examples/arrow_multitask/requirements.txt (line 1)) (3.0.3)
Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.11/site-packages (from pandas->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (2025.2)
Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (1.17.0)
Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.7/9.7 MB 35.2 MB/s  0:00:00
Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.1/60.1 MB 80.1 MB/s  0:00:00
Downloading joblib-1.5.2-py3-none-any.whl (308 kB)
Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)
Installing collected packages: threadpoolctl, joblib, scikit-learn, bitsandbytes

Successfully installed bitsandbytes-0.48.1 joblib-1.5.2 scikit-learn-1.7.2 threadpoolctl-3.6.0
python -m pytest -n 3 tests/
============================= test session starts ==============================
platform linux -- Python 3.11.0rc1, pytest-8.4.2, pluggy-1.6.0
rootdir: /app
configfile: pyproject.toml
plugins: xdist-3.8.0, cov-7.0.0
created: 3/3 workers
3 workers [18253 items]

ssssssss.ss.sss.s.s.sss..ss.ss.s.s..ssss..ss...ssss..................... [  0%]
........................................................................ [  0%]
........................................................................ [  1%]
........................................................................ [  1%]
........................................................................ [  1%]
........................................................................ [  2%]
........................................................................ [  2%]
........................................................................ [  3%]
........................................................................ [  3%]
........................................................................ [  3%]
........................................................................ [  4%]
........................................................................ [  4%]
.........................................s.ssssssss.ssssssss.ssssssss.ss [  5%]
sssssss.ssssssss.ssssssss.s.sssssss.sssssssss.sssssssss.ssssssss.sssssss [  5%]
ss.sssssssss.ssssssss.sssssssss.sssss.ssssssssssssssssssssssssssssssssss [  5%]
sssss.sssssssss.sssssssss.ssssssss.sss.................................. [  6%]
........................................................................ [  6%]
.......................................ss............................... [  7%]
........................................................................ [  7%]
....F..............F.........F.............F..............F..........F.. [  7%]
...........F..............F.............F.....................F......... [  8%]
.sssss.sssssssssssssssssssssssssssssssssssFsssssssssssssssssssssssssssss [  8%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss......ssssss [  9%]
ssssssssssssss................................F......................... [  9%]
.......................................................F................ [  9%]
....................................................................F... [ 10%]
..........................................................F............. [ 10%]
........................................................................ [ 11%]
........................................................................ [ 11%]
........................................................................ [ 11%]
........................................................................ [ 12%]
........................................................................ [ 12%]
........................................................................ [ 13%]
...............................................................x........ [ 13%]
......x................................x................................ [ 13%]
........................................................................ [ 14%]
........................................................................ [ 14%]
........................................................................ [ 14%]
........................................................................ [ 15%]
........................................................................ [ 15%]
.............ss......................................................... [ 16%]
........................................................................ [ 16%]
........................................................................ [ 16%]
........................................................................ [ 17%]
........................................................................ [ 17%]
........................................................................ [ 18%]
.......................................................ssss............. [ 18%]
........................................................................ [ 18%]
...............................................................ss.ssssss [ 19%]
s.ssssss..........................................ss.ssss.sss.sss....... [ 19%]
........................................................................ [ 20%]
..................................................ssss.................. [ 20%]
......................................................................ss [ 20%]
ssssss.sssssssssssssssss.ssssssssssssssssss.ssssssssssssssssss.sssssssss [ 21%]
........ssssssssssssssssssss.........................ssss............... [ 21%]
........................................................................ [ 22%]
........................................................................ [ 22%]
..................................................................ssss.. [ 22%]
........................................................................ [ 23%]
........................................................................ [ 23%]
........................................................................ [ 24%]
........s.........................s..................................... [ 24%]
........................................................................ [ 24%]
........................................................................ [ 25%]
........................................................................ [ 25%]
........................................................................ [ 26%]
........................................................................ [ 26%]
........................................................................ [ 26%]
.......................ss............................................... [ 27%]
........................................................................ [ 27%]
.........sssss.............sss.ss.....................ssssssssssssssssss [ 28%]
ss...........................................ss......................... [ 28%]
........................................................................ [ 28%]
....................................sssss..............sssss.ssssss..... [ 29%]
........ssssssssssssss.ssssss........................................... [ 29%]
........................................................................ [ 29%]
........................................................................ [ 30%]
........................................................................ [ 30%]
.................................................sssssssssssssssssssss.s [ 31%]
sssssssssssssssssssssssss.ssssssss.ssssssss.sssssss.ssssssss.ssssssss.ss [ 31%]
ssss.ssssss.ss................ssssss.ssssss.ssssss.ssssss.ssssss.ssssss. [ 31%]
sssssss.................................................ssss............ [ 32%]
........................................................................ [ 32%]
........................F.................F............................. [ 33%]
...............................................................sssssssss [ 33%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 33%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 34%]
sssssssssssssssssssssssssssssssssssssss....s..........s................. [ 34%]
..ss........s..s..............s................s.................s...... [ 35%]
.........s....s...s...s...s..s...s...................s....sss....s...... [ 35%]
..s...sss.....................s.................s..................s.... [ 35%]
................................ss............................ss........ [ 36%]
.....................ss...............................x........sx....... [ 36%]
........................................................s............... [ 37%]
......s....................s..............s............................. [ 37%]
s..........x.x.s...........................ss........................... [ 37%]
...........................sssssssss..s....s..............s............. [ 38%]
s.........................................s....................s........ [ 38%]
..............s..............ssss.sssssssssss.ssssssssssss.sssssssssssss [ 39%]
sss.ssssssssssssssssss.ss.ssssss........................................ [ 39%]
........................................................................ [ 39%]
........................................................................ [ 40%]
..................................................................s..... [ 40%]
....................................s..........................s.sssssss [ 41%]
ssssss.ssssssssss.sssssssssssssssssss.sssssssssssss.sssssssssssss.ssssss [ 41%]
ssssssssssssssssssss.ssssssssss.ssssssssss.ssssssssssss.sssssssss..s.... [ 41%]
........................................................................ [ 42%]
.........................s.........................s.............s...... [ 42%]
.....s....................................s............................. [ 42%]
........................................................................ [ 43%]
........................................................................ [ 43%]
....................................s................................... [ 44%]
................................................................s....... [ 44%]
..s..............s...................................................... [ 44%]
......................s................................................. [ 45%]
........................................................................ [ 45%]
........................................................................ [ 46%]
........................................................................ [ 46%]
........................................................................ [ 46%]
........................................................................ [ 47%]
........................................................................ [ 47%]
........................................................................ [ 48%]
........................................................................ [ 48%]
....................................ssssssss.ssssssssssssssss.sss.ssssss [ 48%]
ssssss.ssssss........................................................... [ 49%]
...............................s........................................ [ 49%]
....s..............ssssssssssssssssssssssssssssss.ssssssss.sssssssssssss [ 50%]
ssssssssssssssssss.......s....................s......................... [ 50%]
.s........................s..........................s.................. [ 50%]
......s...........s............................s................s....... [ 51%]
...........................................s..............s............. [ 51%]
....s.s........................ssss.sssss.....s......................... [ 52%]
....s.s.........................s..................s.................s.. [ 52%]
.................s..............s.........................s............. [ 52%]
.......s................................................................ [ 53%]
....s.........................s.............................s........... [ 53%]
.....................................................s....s....s........ [ 54%]
......s.................................................s............... [ 54%]
..s.......s...........sssssssss.......s..s...s........................s. [ 54%]
...s.................s................s............s...............s.... [ 55%]
..............s...........................s............................. [ 55%]
.s.......s..................s...............s........................... [ 56%]
............................sssssssss....s.s.............s.............. [ 56%]
.................................s........s...........................s. [ 56%]
............s....................s...................................... [ 57%]
..s...................s...........s.............s.......s............... [ 57%]
.....s........s......................................s............s..... [ 57%]
................................................................s....... [ 58%]
..........s..........sssssssssssssssssssssss.sssssssssssssssssssssssssss [ 58%]
ssss.ssssssssssssssssss......................s......s.........s......... [ 59%]
............................s.s...........sssssssssssssssssssssss.ssssss [ 59%]
ssssssssssssssssss.sssssss..s........s..............ssssssssssssssssss.. [ 59%]
..........s.ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 60%]
sss..s...s.......sssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 60%]
ssssssssssssssssssssssssss.......s...s..........ssssssssss...s.......sss [ 61%]
ssssss.....s..s..........s...s............s.............ssssssssssssssss [ 61%]
sss.....sssssssssss.ssssssssssssssssssssssssssssssssssssssssssssssssssss [ 61%]
ssssssssssssssssssssssss.ssssssssssssssssssssssssssssssss.ssssssssssssss [ 62%]
ssssssssssssssssssss.......sssssssssssssssssssssssssssssssssssss..s..... [ 62%]
.......s..s.........s..s..........s...s.........s..s.......sssssssss.... [ 63%]
.s.............sssssssssssssssssssssssssssssssssss.s.sss..s............. [ 63%]
ssss.sssssssssssssssssssssssssssssssssssssssss.sssssssssssssssssssssssss [ 63%]
ss.sssssssssssssssss.sssssssssssssssssssssssssssssssssssssssssssssssssss [ 64%]
ssssssss.sssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 64%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 65%]
sss.s.sssssssssssssssssssssssssssssssssssss..............sssssssssssssss [ 65%]
ssssssssssssssssssssssssssssssssssssssssss.ssssss.................s..... [ 65%]
.........s............s............s.ssssssss....................s...... [ 66%]
..............................ssssssssssssssssssss.s.sssssssssssssssssss [ 66%]
ssssssssssssss............................sssssssssssssssssssss.ssssss.. [ 67%]
s..................s.................s..sssssssssss.ssssssssssssssss.sss [ 67%]
sssssssssss.ssssssssssssssssssssss.ssssssssssssssssss.s..........sss.sss [ 67%]
sss......ssssssssssssssss.sssssssssss...s.........s.............s.s..... [ 68%]
......s....................s............................................ [ 68%]
...........................s....................s.....................ss [ 69%]
s.ssssssss.sssssssssssssssssssssssssssss.sssss.......................... [ 69%]
...ssssssssssssssssss..s........................sssssssssssssss.ssssss.s [ 69%]
ssssssssssss.ssssssssssss...........sssssss.sssssssssssssssssssssssss.ss [ 70%]
sssssssssssss.sssssss...........................s....................sss [ 70%]
ssssss...........ss.ssssssssss.ssss.ss.....s................ssssssssss.s [ 71%]
sssssssssssssssss.ssssssssssss.sssssssssssssssssssssssssssss.ssssssss.ss [ 71%]
ss......s.s....s..................s....s..s..................sssssssss.. [ 71%]
......s....................s........s.............s..................... [ 72%]
..................................s................s................s... [ 72%]
......................s.........................s.ssssssss.......s...... [ 72%]
s.............s.......ssss.ssssssssssssssssssssssssssssssssssssss.ssssss [ 73%]
sssssssssssssss........................ss..........................s.... [ 73%]
................................s.s....................ss............... [ 74%]
....................ss.......................s..........ss.............. [ 74%]
........................s......ssssssssssssssssssssssss.ssssssssssssssss [ 74%]
sssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss. [ 75%]
ssssssss..............s.ssssssssssssssssssssssssssssssssssssssssssssssss [ 75%]
ssssssssssssssssssssssss......................................ssssssssss [ 76%]
sssssssssssssssssssssss..................s...ssssssssssss............... [ 76%]
......ss............s......................ssssssss..................... [ 76%]
......................s.........................ssssssssssssss.......... [ 77%]
..............ssssssssssssssssssssssssssssssss...........sssssssssssssss [ 77%]
ss.sss.................ss....s.......................................... [ 78%]
....................................................................ssss [ 78%]
ssssssss.................ss.....ssssssssssssssssssssssssssssssssss..ssss [ 78%]
ssssssssss...............................ssssss..................sssssss [ 79%]
..ssssssssssss....................................ss...s..ssssssssssssss [ 79%]
ssssssssssssssssss.ssssss..........ssssssssss........................... [ 80%]
....................ssss........................ss...................... [ 80%]
........................................................................ [ 80%]
........................................................................ [ 81%]
......................................sssssssss......................... [ 81%]
.....................ssss.........................................s..... [ 82%]
sssssss..................ssssssssssssssssss............................. [ 82%]
.........ss............................................................. [ 82%]
...........................................ssss......................... [ 83%]
................ssssssssssssss...........ssssssssssss....ssssss......... [ 83%]
.......................ssssssssss...s.......ssss........................ [ 84%]
...........ssssssssss.....ssss......ssssssssss.....ss.sssssssss.sssssss. [ 84%]
.s...................................................................... [ 84%]
sssssssssss.......ss....................ssssssssssssssssssssssssssssssss [ 85%]
ssssssssssssssss........................ssssssssssssssssssssssssssssssss [ 85%]
ssss..sss.....ss...ss..........ss....ss...ss....ss...ss...s.sssss....... [ 85%]
.......................ss....ss.............ssss........................ [ 86%]
................................s..........sssssss.sssssssss............ [ 86%]
....ss..................................s............................... [ 87%]
.........sssssssssssssss.sssssssssssss..............ssssssssssss........ [ 87%]
...ssssssssssssssssssssssss.....................ssssss.ssssss..s........ [ 87%]
.........s...sss.ssssssssssssssssssssssssss.............s..............s [ 88%]
sssssss.........................................s....................... [ 88%]
................ssssssssssssssssssss.......sssssssssss.sssssssssssssssss [ 89%]
sssssssssssssss.sssssssss.ssssssssssssssssss.sssssssss.................. [ 89%]
.sssssssssssssss.ssssssss.sssssssssssssssss.ssssssssssssssssssssssssssss [ 89%]
sssssss..........................ssssssssssssssssssss...........s.ssssss [ 90%]
ss.........................................................s............ [ 90%]
...s............sssssssssssssssssssssssssssssssssssssssssssssssssss.ssss [ 91%]
ssssssssssssssssss...................................................F.. [ 91%]
Fatal Python error: Segmentation fault

Thread 0x00007f268bbcf640 (most recent call first):
  File "/usr/lib/python3.11/ssl.py", line 1134 in read
  File "/usr/lib/python3.11/ssl.py", line 1278 in recv_into
  File "/usr/lib/python3.11/socket.py", line 705 in readinto
  File "/usr/lib/python3.11/http/client.py", line 279 in _read_status
  File "/usr/lib/python3.11/http/client.py", line 318 in begin
  File "/usr/lib/python3.11/http/client.py", line 1374 in getresponse
  File "/app/venv/lib/python3.11/site-packages/urllib3/connection.py", line 565 in getresponse
  File "/app/venv/lib/python3.11/site-packages/urllib3/connectionpool.py", line 534 in _make_request
  File "/app/venv/lib/python3.11/site-packages/urllib3/connectionpool.py", line 787 in urlopen
  File "/app/venv/lib/python3.11/site-packages/requests/adapters.py", line 644 in send
  File "/app/venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 95 in send
  File "/app/venv/lib/python3.11/site-packages/requests/sessions.py", line 703 in send
  File "/app/venv/lib/python3.11/site-packages/requests/sessions.py", line 589 in request
  File "/app/venv/lib/python3.11/site-packages/requests/sessions.py", line 602 in get
  File "/app/venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py", line 2648 in model_info
  File "/app/venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114 in _inner_fn
  File "/app/venv/lib/python3.11/site-packages/transformers/safetensors_conversion.py", line 59 in get_conversion_pr_reference
  File "/app/venv/lib/python3.11/site-packages/transformers/safetensors_conversion.py", line 84 in auto_conversion
  File "/usr/lib/python3.11/threading.py", line 975 in run
  File "/usr/lib/python3.11/threading.py", line 1038 in _bootstrap_inner
  File "/usr/lib/python3.11/threading.py", line 995 in _bootstrap

Thread 0x00007f2688fe9640 (most recent call first):
  File "/usr/lib/python3.11/threading.py", line 324 in wait
  File "/usr/lib/python3.11/threading.py", line 622 in wait
  File "/app/venv/lib/python3.11/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.11/threading.py", line 1038 in _bootstrap_inner
  File "/usr/lib/python3.11/threading.py", line 995 in _bootstrap

Thread 0x00007f278affd640 (most recent call first):
  File "/usr/lib/python3.11/threading.py", line 324 in wait
  File "/usr/lib/python3.11/threading.py", line 622 in wait
  File "/app/venv/lib/python3.11/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.11/threading.py", line 1038 in _bootstrap_inner
  File "/usr/lib/python3.11/threading.py", line 995 in _bootstrap

Thread 0x00007f276b7fe640 (most recent call first):
  File "/usr/lib/python3.11/threading.py", line 324 in wait
  File "/usr/lib/python3.11/threading.py", line 622 in wait
  File "/app/venv/lib/python3.11/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.11/threading.py", line 1038 in _bootstrap_inner
  File "/usr/lib/python3.11/threading.py", line 995 in _bootstrap

Thread 0x00007f278bfff640 (most recent call first):
  File "/usr/lib/python3.11/threading.py", line 324 in wait
  File "/usr/lib/python3.11/threading.py", line 622 in wait
  File "/app/venv/lib/python3.11/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.11/threading.py", line 1038 in _bootstrap_inner
  File "/usr/lib/python3.11/threading.py", line 995 in _bootstrap

Thread 0x00007f279cf18640 (most recent call first):
  File "/app/venv/lib/python3.11/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 61 in _recv_msg
  File "/app/venv/lib/python3.11/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 195 in _read_thread
  File "/usr/lib/python3.11/threading.py", line 975 in run
  File "/usr/lib/python3.11/threading.py", line 1038 in _bootstrap_inner
  File "/usr/lib/python3.11/threading.py", line 995 in _bootstrap

Thread 0x00007f2a039b7640 (most recent call first):
  File "/app/venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 534 in read
  File "/app/venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 567 in from_io
  File "/app/venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 1160 in _thread_receiver
  File "/app/venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 341 in run
  File "/app/venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 411 in _perform_spawn

Thread 0x00007f2a04311280 (most recent call first):
  File "/app/venv/lib/python3.11/site-packages/bitsandbytes/backends/cpu/ops.py", line 43 in _
  File "/app/venv/lib/python3.11/site-packages/torch/library.py", line 752 in func_no_dynamo
  File "/app/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 929 in _fn
  File "/app/venv/lib/python3.11/site-packages/torch/_compile.py", line 53 in inner
  File "/app/venv/lib/python3.11/site-packages/torch/_ops.py", line 829 in __call__
  File "/app/venv/lib/python3.11/site-packages/bitsandbytes/functional.py", line 610 in quantize_blockwise
  File "/app/venv/lib/python3.11/site-packages/bitsandbytes/functional.py", line 875 in quantize_4bit
  File "/app/venv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py", line 296 in _quantize
  File "/app/venv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py", line 337 in to
  File "/app/venv/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py", line 222 in create_quantized_param
  File "/app/venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 774 in _load_state_dict_into_meta_model
  File "/app/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/app/venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 847 in load_shard_file
  File "/app/venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5471 in _load_pretrained_model
  File "/app/venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5051 in from_pretrained
  File "/app/venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 277 in _wrapper
  File "/app/venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 604 in from_pretrained
  File "/app/tests/test_gpu_examples.py", line 4737 in test_low_cpu_mem_usage_with_quantization
  File "/app/venv/lib/python3.11/site-packages/_pytest/python.py", line 157 in pytest_pyfunc_call
  File "/app/venv/lib/python3.11/site-packages/pluggy/_callers.py", line 121 in _multicall
  File "/app/venv/lib/python3.11/site-packages/pluggy/_manager.py", line 120 in _hookexec
  File "/app/venv/lib/python3.11/site-packages/pluggy/_hooks.py", line 512 in __call__
  File "/app/venv/lib/python3.11/site-packages/_pytest/python.py", line 1671 in runtest
  File "/app/venv/lib/python3.11/site-packages/_pytest/runner.py", line 178 in pytest_runtest_call
  File "/app/venv/lib/python3.11/site-packages/pluggy/_callers.py", line 121 in _multicall
  File "/app/venv/lib/python3.11/site-packages/pluggy/_manager.py", line 120 in _hookexec
  File "/app/venv/lib/python3.11/site-packages/pluggy/_hooks.py", line 512 in __call__
  File "/app/venv/lib/python3.11/site-packages/_pytest/runner.py", line 246 in <lambda>
  File "/app/venv/lib/python3.11/site-packages/_pytest/runner.py", line 344 in from_call
  File "/app/venv/lib/python3.11/site-packages/_pytest/runner.py", line 245 in call_and_report
  File "/app/venv/lib/python3.11/site-packages/_pytest/runner.py", line 136 in runtestprotocol
  File "/app/venv/lib/python3.11/site-packages/_pytest/runner.py", line 117 in pytest_runtest_protocol
  File "/app/venv/lib/python3.11/site-packages/pluggy/_callers.py", line 121 in _multicall
  File "/app/venv/lib/python3.11/site-packages/pluggy/_manager.py", line 120 in _hookexec
  File "/app/venv/lib/python3.11/site-packages/pluggy/_hooks.py", line 512 in __call__
  File "/app/venv/lib/python3.11/site-packages/xdist/remote.py", line 227 in run_one_test
  File "/app/venv/lib/python3.11/site-packages/xdist/remote.py", line 206 in pytest_runtestloop
  File "/app/venv/lib/python3.11/site-packages/pluggy/_callers.py", line 121 in _multicall
  File "/app/venv/lib/python3.11/site-packages/pluggy/_manager.py", line 120 in _hookexec
  File "/app/venv/lib/python3.11/site-packages/pluggy/_hooks.py", line 512 in __call__
  File "/app/venv/lib/python3.11/site-packages/_pytest/main.py", line 343 in _main
  File "/app/venv/lib/python3.11/site-packages/_pytest/main.py", line 289 in wrap_session
  File "/app/venv/lib/python3.11/site-packages/_pytest/main.py", line 336 in pytest_cmdline_main
  File "/app/venv/lib/python3.11/site-packages/pluggy/_callers.py", line 121 in _multicall
  File "/app/venv/lib/python3.11/site-packages/pluggy/_manager.py", line 120 in _hookexec
  File "/app/venv/lib/python3.11/site-packages/pluggy/_hooks.py", line 512 in __call__
  File "/app/venv/lib/python3.11/site-packages/xdist/remote.py", line 427 in <module>
  File "/app/venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 1291 in executetask
  File "/app/venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 341 in run
  File "/app/venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 411 in _perform_spawn
  File "/app/venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 389 in integrate_as_primary_thread
  File "/app/venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 1273 in serve
  File "/app/venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 1806 in serve
  File "<string>", line 8 in <module>
  File "<string>", line 1 in <module>

Extension modules: numpy._core._multiarray_umath, numpy.linalg._umath_linalg, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, yaml._yaml, regex._regex, markupsafe._speedups, PIL._imaging, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._pcg64, numpy.random._mt19937, numpy.random._generator, numpy.random._philox, numpy.random._sfc64, numpy.random.mtrand, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, psutil._psutil_linux, psutil._psutil_posix, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._acero, pyarrow._csv, pyarrow._json, pyarrow._substrait, pyarrow._dataset, pyarrow._dataset_orc, pyarrow._parquet_encryption, pyarrow._dataset_parquet_encryption, pyarrow._dataset_parquet, scipy.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils, PIL._imagingmath (total: 208)
sssss...........................[gw1] node down: Not properly terminated
F
replacing crashed worker gw1
.........ssssssssssssssssssssssss....................................... [ 92%]
s...F.F.......................................................X.X....... [ 92%]
.....................................sssssssssssssssssssssssssssssssssss [ 92%]
s....................sssssss.ssssssss................................... [ 93%]
........................................................................ [ 93%]
........................................................................ [ 94%]
............................sss......................................... [ 94%]
................................sss..................................... [ 94%]
............................................ssssssssssssssss............ [ 95%]
.......................................................................s [ 95%]
sssssss...................................ssssssssssssssss.............. [ 96%]
..................F.............F........ssssssssssssssss............... [ 96%]
.........................sssssssssssssss.s.............................. [ 96%]
........................................................................ [ 97%]
..............................sss....................................... [ 97%]
..........................................................ss.....s...... [ 98%]
......F.........................................ssssssssss.............. [ 98%]
........................................................................ [ 98%]
.....................................ss.....sssssss.s..............s...F [ 99%]
.s........sssssssss.....x.......x.....................FF................ [ 99%]
F.......................x....F...........s.............................. [ 99%]
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
....                                                                     [100%]
==================================== ERRORS ====================================
________________ ERROR collecting tests/test_stablediffusion.py ________________
tests/test_stablediffusion.py:222: in <module>
    class TestStableDiffusionModel(PeftCommonTester):
tests/test_stablediffusion.py:228: in TestStableDiffusionModel
    sd_model = StableDiffusionPipeline.from_pretrained("hf-internal-testing/tiny-sd-pipe")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'
------------------------------- Captured stderr --------------------------------
Fetching 12 files:   0%|                                | 0/12 [00:00<?, ?it/s]Fetching 12 files:  17%|████                    | 2/12 [00:00<00:02,  3.60it/s]Fetching 12 files:  33%|████████                | 4/12 [00:02<00:05,  1.38it/s]Fetching 12 files: 100%|███████████████████████| 12/12 [00:02<00:00,  4.56it/s]
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  80%|█████████▌  | 4/5 [00:00<00:00, 23.01it/s]Loading pipeline components...:  80%|█████████▌  | 4/5 [00:00<00:00, 22.50it/s]
________________ ERROR collecting tests/test_stablediffusion.py ________________
tests/test_stablediffusion.py:222: in <module>
    class TestStableDiffusionModel(PeftCommonTester):
tests/test_stablediffusion.py:228: in TestStableDiffusionModel
    sd_model = StableDiffusionPipeline.from_pretrained("hf-internal-testing/tiny-sd-pipe")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'
------------------------------- Captured stderr --------------------------------
Fetching 12 files:   0%|                                | 0/12 [00:00<?, ?it/s]Fetching 12 files:  17%|████                    | 2/12 [00:00<00:02,  3.87it/s]Fetching 12 files:  33%|████████                | 4/12 [00:02<00:05,  1.53it/s]Fetching 12 files: 100%|███████████████████████| 12/12 [00:02<00:00,  5.05it/s]
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  20%|██▍         | 1/5 [00:00<00:01,  2.80it/s]Loading pipeline components...:  40%|████▊       | 2/5 [00:00<00:00,  5.47it/s]
________________ ERROR collecting tests/test_stablediffusion.py ________________
tests/test_stablediffusion.py:222: in <module>
    class TestStableDiffusionModel(PeftCommonTester):
tests/test_stablediffusion.py:228: in TestStableDiffusionModel
    sd_model = StableDiffusionPipeline.from_pretrained("hf-internal-testing/tiny-sd-pipe")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'
------------------------------- Captured stderr --------------------------------
Fetching 12 files:   0%|                                | 0/12 [00:00<?, ?it/s]Fetching 12 files:  17%|████                    | 2/12 [00:00<00:01,  5.38it/s]Fetching 12 files:  33%|████████                | 4/12 [00:02<00:05,  1.57it/s]Fetching 12 files: 100%|███████████████████████| 12/12 [00:02<00:00,  5.28it/s]
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  60%|███████▏    | 3/5 [00:00<00:00, 24.16it/s]Loading pipeline components...:  80%|█████████▌  | 4/5 [00:00<00:00, 29.03it/s]
________________ ERROR collecting tests/test_stablediffusion.py ________________
tests/test_stablediffusion.py:222: in <module>
    class TestStableDiffusionModel(PeftCommonTester):
tests/test_stablediffusion.py:228: in TestStableDiffusionModel
    sd_model = StableDiffusionPipeline.from_pretrained("hf-internal-testing/tiny-sd-pipe")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'
------------------------------- Captured stderr --------------------------------
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  40%|████▊       | 2/5 [00:00<00:00, 43.19it/s]
=================================== FAILURES ===================================
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 1 OFT-MLP-OFTConfig-config_kwargs96] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f2794b3abd0>
test_name = 'Vanilla MLP 1 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'oft_block_size': 0, 'r': 2, 'target_modules': 'lin0', 'use_cayley_neumann': False}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 2, n_elements = 10, block_size = 5
in_features = 10, coft = False, eps = 6e-05, block_share = False
kernel_size = (0, 0), use_cayley_neumann = False, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 2 OFT-MLP-OFTConfig-config_kwargs97] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f2794b3aed0>
test_name = 'Vanilla MLP 2 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'oft_block_size': 0, 'r': 2, 'target_modules': ['lin0'], 'use_cayley_neumann': False}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 2, n_elements = 10, block_size = 5
in_features = 10, coft = False, eps = 6e-05, block_share = False
kernel_size = (0, 0), use_cayley_neumann = False, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 5 OFT-MLP-OFTConfig-config_kwargs98] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f2794b3b1d0>
test_name = 'Vanilla MLP 5 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'modules_to_save': ['lin1'], 'oft_block_size': 0, 'r': 2, 'target_modules': ['lin0'], ...}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 2, n_elements = 10, block_size = 5
in_features = 10, coft = False, eps = 6e-05, block_share = False
kernel_size = (0, 0), use_cayley_neumann = False, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 6 OFT-MLP-OFTConfig-config_kwargs99] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f2794b3b4d0>
test_name = 'Vanilla MLP 6 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'module_dropout': 0.1, 'oft_block_size': 0, 'r': 2, 'target_modules': ['lin0'], ...}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 2, n_elements = 10, block_size = 5
in_features = 10, coft = False, eps = 6e-05, block_share = False
kernel_size = (0, 0), use_cayley_neumann = False, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 7 OFT-MLP-OFTConfig-config_kwargs100] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f2794b3b7d0>
test_name = 'Vanilla MLP 7 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'coft': True, 'eps': 0.01, 'oft_block_size': 0, 'r': 2, ...}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 2, n_elements = 10, block_size = 5
in_features = 10, coft = True, eps = 0.01, block_share = False
kernel_size = (0, 0), use_cayley_neumann = True, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 8 OFT-MLP-OFTConfig-config_kwargs101] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f2794b3bad0>
test_name = 'Vanilla MLP 8 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'block_share': True, 'oft_block_size': 0, 'r': 2, 'target_modules': ['lin0'], ...}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 1, n_elements = 10, block_size = 5
in_features = 10, coft = False, eps = 6e-05, block_share = True
kernel_size = (0, 0), use_cayley_neumann = False, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 9 OFT-MLP-OFTConfig-config_kwargs102] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f2794b3bdd0>
test_name = 'Vanilla MLP 9 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'block_share': True, 'coft': True, 'eps': 0.01, 'oft_block_size': 0, ...}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 1, n_elements = 10, block_size = 5
in_features = 10, coft = True, eps = 0.01, block_share = True
kernel_size = (0, 0), use_cayley_neumann = True, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 10 OFT-MLP-OFTConfig-config_kwargs103] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f2794ac8110>
test_name = 'Vanilla MLP 10 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'oft_block_size': 2, 'r': 0, 'target_modules': ['lin0'], 'use_cayley_neumann': True}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 5, n_elements = 1, block_size = 2
in_features = 10, coft = False, eps = 6e-05, block_share = False
kernel_size = (0, 0), use_cayley_neumann = True, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 11 OFT-MLP-OFTConfig-config_kwargs104] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f2794ac8410>
test_name = 'Vanilla MLP 11 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'oft_block_size': 2, 'r': 0, 'target_modules': ['lin0'], 'use_cayley_neumann': False}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 5, n_elements = 1, block_size = 2
in_features = 10, coft = False, eps = 6e-05, block_share = False
kernel_size = (0, 0), use_cayley_neumann = False, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 12 OFT-MLP-OFTConfig-config_kwargs105] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f2794ac8710>
test_name = 'Vanilla MLP 12 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'block_share': True, 'coft': True, 'eps': 0.01, 'oft_block_size': 2, ...}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 1, n_elements = 1, block_size = 2
in_features = 10, coft = True, eps = 0.01, block_share = True
kernel_size = (0, 0), use_cayley_neumann = True, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 13 OFT-MLP-OFTConfig-config_kwargs106] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f2794ac8a10>
test_name = 'Vanilla MLP 13 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'block_share': True, 'coft': True, 'eps': 0.01, 'oft_block_size': 2, ...}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 1, n_elements = 1, block_size = 2
in_features = 10, coft = True, eps = 0.01, block_share = True
kernel_size = (0, 0), use_cayley_neumann = False, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Conv2d 1 OFT-Conv2d-OFTConfig-config_kwargs107] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f2794ac8d10>
test_name = 'Conv2d 1 OFT', model_id = 'Conv2d'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'oft_block_size': 0, 'r': 5, 'target_modules': ['conv2d']}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:936: in dispatch_default
    new_module = Conv2d(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:703: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:769: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 5, n_elements = 36, block_size = 9
in_features = 45, coft = False, eps = 6e-05, block_share = False
kernel_size = (3, 3), use_cayley_neumann = True, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Conv2d 3 OFT-Conv2d-OFTConfig-config_kwargs108] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f2794ac9010>
test_name = 'Conv2d 3 OFT', model_id = 'Conv2d'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'coft': True, 'oft_block_size': 0, 'r': 5, 'target_modules': ['conv2d']}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:936: in dispatch_default
    new_module = Conv2d(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:703: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:769: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 5, n_elements = 36, block_size = 9
in_features = 45, coft = True, eps = 6e-05, block_share = False
kernel_size = (3, 3), use_cayley_neumann = True, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Conv2d 4 OFT-Conv2d-OFTConfig-config_kwargs109] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f2794ac9310>
test_name = 'Conv2d 4 OFT', model_id = 'Conv2d'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'block_share': True, 'oft_block_size': 0, 'r': 5, 'target_modules': ['conv2d']}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:936: in dispatch_default
    new_module = Conv2d(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:703: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:769: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 1, n_elements = 36, block_size = 9
in_features = 45, coft = False, eps = 6e-05, block_share = True
kernel_size = (3, 3), use_cayley_neumann = True, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Conv2d 5 OFT-Conv2d-OFTConfig-config_kwargs110] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f2794ac9610>
test_name = 'Conv2d 5 OFT', model_id = 'Conv2d'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'block_share': True, 'coft': True, 'oft_block_size': 0, 'r': 5, ...}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:936: in dispatch_default
    new_module = Conv2d(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:703: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:769: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 1, n_elements = 36, block_size = 9
in_features = 45, coft = True, eps = 6e-05, block_share = True
kernel_size = (3, 3), use_cayley_neumann = True, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_disable_adapters_with_merging[Conv2d 1 OFT-Conv2d-OFTConfig-config_kwargs107] _
[gw2] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7fe4de660410>
test_name = 'Conv2d 1 OFT', model_id = 'Conv2d'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'oft_block_size': 0, 'r': 5, 'target_modules': ['conv2d']}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_disable_adapters_with_merging(self, test_name, model_id, config_cls, config_kwargs):
        # Same test as test_disable_adapters, but additionally merge the trained adapter.

        # https://github.com/huggingface/peft/pull/2403
        if model_id in ["Conv2dGroups", "Conv2dGroups2"]:
            pytest.skip(
                f"Skipping test for {model_id} as merging is not supported. (See https://github.com/huggingface/peft/pull/2403 for details)"
            )

        # same as test_disable_adapters, but with merging
        X = self.prepare_inputs_for_testing()
        model = self.transformers_class.from_pretrained(model_id).to(self.torch_device)
        config = config_cls(
            base_model_name_or_path=model_id,
            **config_kwargs,
        )
        model = get_peft_model(model, config)
        if issubclass(config_cls, VBLoRAConfig):
            # Manually set the `vblora_vector_bank` to zero so that VB-LoRA functions as an identity operation.
            torch.nn.init.zeros_(model.vblora_vector_bank["default"])
        model.eval()
        outputs_before = model(**X)

        if issubclass(config_cls, VBLoRAConfig):
            # initialize `vblora_vector_bank` so it can be trained
            model._init_vblora_vector_bank(config, "default")
        model.train()
        if isinstance(config_cls, LNTuningConfig):
            # LayerNorm tuning is slow to learn
            lr = 1.0
            optimizer = torch.optim.SGD(model.parameters(), lr=lr)
        else:
            # Adam optimizer since SGD isn't great for small models with IA3 + Conv1D
            lr = 0.01
            optimizer = torch.optim.Adam(model.parameters(), lr=lr)

        # train at least 3 steps for all parameters to be updated (probably this is required because of symmetry
        # breaking of some LoRA layers that are initialized with constants)
        for _ in range(3):
            optimizer.zero_grad()
            y_pred = model(**X)
            y = torch.arange(len(y_pred)).to(self.torch_device) % 2
            loss = nn.functional.nll_loss(y_pred, y)
            loss.backward()
            optimizer.step()

        model.eval()
        outputs_unmerged = model(**X)
        model.merge_adapter()
        outputs_after = model(**X)

        with model.disable_adapter():
            outputs_disabled = model(**X)

        # check that after leaving the disable_adapter context, everything is enabled again
        outputs_enabled_after_disable = model(**X)

        atol, rtol = 1e-5, 1e-5  # tolerances higher than defaults since merging introduces some numerical instability

        conv_ids = ["Conv2d", "Conv3d", "Conv2d2"]
        if issubclass(config_cls, (IA3Config, LoraConfig)) and model_id in conv_ids:  # more instability with Conv
            atol, rtol = 1e-3, 1e-3

        if issubclass(config_cls, OFTConfig):
            atol, rtol = 1e-4, 1e-4

        if config_kwargs.get("use_dora") and model_id == "EmbConv1D":
            atol, rtol = 1e-4, 1e-4

        # check that there is a difference in results after training
        assert not torch.allclose(outputs_before, outputs_after, atol=atol, rtol=rtol)

        if self.torch_device in ["mlu"] and model_id in conv_ids:
            atol, rtol = 1e-3, 1e-2  # MLU

        # unmerged or merged should make no difference
        assert torch.allclose(outputs_after, outputs_unmerged, atol=atol, rtol=rtol)

        # check that disabling adapters gives the same results as before training
>       assert torch.allclose(outputs_before, outputs_disabled, atol=atol, rtol=rtol)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x7fe73b8c94c0>(tensor([[-1.1555e+01, -9.5367e-06],\n        [-4.0945e+01,  0.0000e+00]], grad_fn=<LogSoftmaxBackward0>), tensor([[-1.1542e+01, -9.7751e-06],\n        [-4.0898e+01,  0.0000e+00]]), atol=0.0001, rtol=0.0001)
E        +    where <built-in method allclose of type object at 0x7fe73b8c94c0> = torch.allclose

tests/test_custom_models.py:2101: AssertionError
_ TestPeftCustomModel.test_disable_adapters_with_merging[Conv2d 4 OFT-Conv2d-OFTConfig-config_kwargs109] _
[gw2] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7fe4de660a10>
test_name = 'Conv2d 4 OFT', model_id = 'Conv2d'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'block_share': True, 'oft_block_size': 0, 'r': 5, 'target_modules': ['conv2d']}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_disable_adapters_with_merging(self, test_name, model_id, config_cls, config_kwargs):
        # Same test as test_disable_adapters, but additionally merge the trained adapter.

        # https://github.com/huggingface/peft/pull/2403
        if model_id in ["Conv2dGroups", "Conv2dGroups2"]:
            pytest.skip(
                f"Skipping test for {model_id} as merging is not supported. (See https://github.com/huggingface/peft/pull/2403 for details)"
            )

        # same as test_disable_adapters, but with merging
        X = self.prepare_inputs_for_testing()
        model = self.transformers_class.from_pretrained(model_id).to(self.torch_device)
        config = config_cls(
            base_model_name_or_path=model_id,
            **config_kwargs,
        )
        model = get_peft_model(model, config)
        if issubclass(config_cls, VBLoRAConfig):
            # Manually set the `vblora_vector_bank` to zero so that VB-LoRA functions as an identity operation.
            torch.nn.init.zeros_(model.vblora_vector_bank["default"])
        model.eval()
        outputs_before = model(**X)

        if issubclass(config_cls, VBLoRAConfig):
            # initialize `vblora_vector_bank` so it can be trained
            model._init_vblora_vector_bank(config, "default")
        model.train()
        if isinstance(config_cls, LNTuningConfig):
            # LayerNorm tuning is slow to learn
            lr = 1.0
            optimizer = torch.optim.SGD(model.parameters(), lr=lr)
        else:
            # Adam optimizer since SGD isn't great for small models with IA3 + Conv1D
            lr = 0.01
            optimizer = torch.optim.Adam(model.parameters(), lr=lr)

        # train at least 3 steps for all parameters to be updated (probably this is required because of symmetry
        # breaking of some LoRA layers that are initialized with constants)
        for _ in range(3):
            optimizer.zero_grad()
            y_pred = model(**X)
            y = torch.arange(len(y_pred)).to(self.torch_device) % 2
            loss = nn.functional.nll_loss(y_pred, y)
            loss.backward()
            optimizer.step()

        model.eval()
        outputs_unmerged = model(**X)
        model.merge_adapter()
        outputs_after = model(**X)

        with model.disable_adapter():
            outputs_disabled = model(**X)

        # check that after leaving the disable_adapter context, everything is enabled again
        outputs_enabled_after_disable = model(**X)

        atol, rtol = 1e-5, 1e-5  # tolerances higher than defaults since merging introduces some numerical instability

        conv_ids = ["Conv2d", "Conv3d", "Conv2d2"]
        if issubclass(config_cls, (IA3Config, LoraConfig)) and model_id in conv_ids:  # more instability with Conv
            atol, rtol = 1e-3, 1e-3

        if issubclass(config_cls, OFTConfig):
            atol, rtol = 1e-4, 1e-4

        if config_kwargs.get("use_dora") and model_id == "EmbConv1D":
            atol, rtol = 1e-4, 1e-4

        # check that there is a difference in results after training
        assert not torch.allclose(outputs_before, outputs_after, atol=atol, rtol=rtol)

        if self.torch_device in ["mlu"] and model_id in conv_ids:
            atol, rtol = 1e-3, 1e-2  # MLU

        # unmerged or merged should make no difference
        assert torch.allclose(outputs_after, outputs_unmerged, atol=atol, rtol=rtol)

        # check that disabling adapters gives the same results as before training
>       assert torch.allclose(outputs_before, outputs_disabled, atol=atol, rtol=rtol)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x7fe73b8c94c0>(tensor([[-1.1555e+01, -9.5367e-06],\n        [-4.0945e+01,  0.0000e+00]], grad_fn=<LogSoftmaxBackward0>), tensor([[-1.1547e+01, -9.6559e-06],\n        [-4.0910e+01,  0.0000e+00]]), atol=0.0001, rtol=0.0001)
E        +    where <built-in method allclose of type object at 0x7fe73b8c94c0> = torch.allclose

tests/test_custom_models.py:2101: AssertionError
_____________________ TestFSDPWrap.test_bnb_4bit_wrap_fsdp _____________________
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_gpu_examples.TestFSDPWrap object at 0x7f2792244bd0>

    @pytest.mark.single_gpu_tests
    @require_bitsandbytes
    def test_bnb_4bit_wrap_fsdp(self):
        quant_config = BitsAndBytesConfig(
            load_in_4bit=True,
            # float32 must be used, or else FSDP will complain about mixed int and float dtypes
            bnb_4bit_compute_dtype=torch.float32,
            bnb_4bit_quant_storage=torch.float32,
            bnb_4bit_use_double_quant=True,
        )
        model = AutoModelForCausalLM.from_pretrained(
            "facebook/opt-125m",
            quantization_config=quant_config,
            torch_dtype=torch.float32,
        )
        # model = prepare_model_for_kbit_training(model)
        config = LoraConfig(
            target_modules=["q_proj", "v_proj"],
            task_type="CAUSAL_LM",
            use_dora=True,
        )
        model = get_peft_model(model, config)

        os.environ["MASTER_ADDR"] = "localhost"
        os.environ["MASTER_PORT"] = "29501"

        init_process_group(world_size=1, rank=0)
        # check that this does not raise:
>       FSDP(model, auto_wrap_policy=fsdp_auto_wrap_policy(model), use_orig_params=False, sync_module_states=True)

tests/test_gpu_examples.py:4599:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:431: in __init__
    _init_device_handle(self, module, self._ignored_params, device_id)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

state = FullyShardedDataParallel()
module = PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): OPTForCausalLM(
      (model): OPTModel(
        (decode...   )
          )
        )
      )
      (lm_head): Linear(in_features=768, out_features=50272, bias=False)
    )
  )
)
ignored_params = set(), device_id = None

    @no_type_check
    def _init_device_handle(
        state: _FSDPState,
        module: nn.Module,
        ignored_params: set[nn.Parameter],
        device_id: Optional[Union[int, torch.device]],
    ) -> _FSDPState:
        """
        Determine device handle used for initializing FSDP.

        If a device is specified by ``device_id``,
        then returns device handle corresponds to that device type. Otherwise, If the
        module is already on a non-CPU device, then the device type is that non-CPU device type.
        If the module is on CPU or meta, then the device type is the current accelerator device.
        See the :ref:`Accelerators<accelerators>` for details.


        This method will be called once ignored parameters was determined, as the device handle maybe needed
        for other initialization.
        """
        determined_device = None
        if device_id is not None:
            determined_device = (
                device_id
                if isinstance(device_id, torch.device)
                else torch.device(device_id)
            )
        if determined_device is None:
            for param in _get_orig_params(module, ignored_params):
                if param.device.type in {"cpu", "meta"}:
                    continue
                if determined_device is None:
                    determined_device = param.device
                else:
                    if param.device.type != determined_device.type:
                        raise RuntimeError(
                            f"FSDP does not support modules with different device types "
                            f"but got params on {determined_device.type} and {param.device.type}"
                        )
            determined_device = determined_device or torch._C._get_accelerator()
            if determined_device.type == "cpu":
>               raise RuntimeError(
                    "FSDP needs a non-CPU accelerator device, but no accelerator device is detected."
                )
E               RuntimeError: FSDP needs a non-CPU accelerator device, but no accelerator device is detected.

venv/lib/python3.11/site-packages/torch/distributed/fsdp/_init_utils.py:388: RuntimeError
----------------------------- Captured stdout call -----------------------------
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
----------------------------- Captured stderr call -----------------------------
`torch_dtype` is deprecated! Use `dtype` instead!
__________________________ tests/test_gpu_examples.py __________________________
[gw1] linux -- Python 3.11.0 /app/venv/bin/python
worker 'gw1' crashed while running 'tests/test_gpu_examples.py::TestLowCpuMemUsageDifferentDevices::test_low_cpu_mem_usage_with_quantization[bnb-4bit]'
___________ TestOft.test_load_outdated_oft_checkpoint_warns[0.17.0] ____________
[gw2] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_initialization.TestOft object at 0x7fe4db6c7210>
peft_version = '0.17.0'
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/popen-gw2/test_load_outdated_oft_checkpo0')
recwarn = WarningsRecorder(record=True)

    @pytest.mark.parametrize("peft_version", ["0.17.0", "0.18.0", None])
    def test_load_outdated_oft_checkpoint_warns(self, peft_version, tmp_path, recwarn):
        # In PEFT v0.18.0, there was a small change in the OFT implementation with Cayley-Neumann enabled. As the
        # outputs change slightly, users need to be warned about it if the checkpoint stems from a PEFT version below
        # 0.18.0. When the 'peft_version' key is not in the config, it means that the version is below 0.18.0.
        config = OFTConfig(target_modules=["lin"], use_cayley_neumann=True)  # only relevant when using Cayley-Neumann
        model = get_peft_model(self.get_model(), config)
        model.save_pretrained(tmp_path)
        del model

        # overwrite the peft_version
        with open(tmp_path / "adapter_config.json") as f:
            config_json = json.load(f)

        if peft_version is None:
            del config_json["peft_version"]
        else:
            config_json["peft_version"] = peft_version

        with open(tmp_path / "adapter_config.json", "w") as f:
            json.dump(config_json, f)

        msg = "TODO"  # <= replace with final warning message
        PeftModel.from_pretrained(self.get_model(), tmp_path)

        warn_messages = [str(w.message) for w in recwarn.list]
        if peft_version == "0.18.0":
            assert not any(w.startswith(msg) for w in warn_messages)
        else:
>           assert any(w.startswith(msg) for w in warn_messages)
E           assert False
E            +  where False = any(<generator object TestOft.test_load_outdated_oft_checkpoint_warns.<locals>.<genexpr> at 0x7fe3efc42180>)

tests/test_initialization.py:1801: AssertionError
____________ TestOft.test_load_outdated_oft_checkpoint_warns[None] _____________
[gw2] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_initialization.TestOft object at 0x7fe4db6c4210>
peft_version = None
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/popen-gw2/test_load_outdated_oft_checkpo2')
recwarn = WarningsRecorder(record=True)

    @pytest.mark.parametrize("peft_version", ["0.17.0", "0.18.0", None])
    def test_load_outdated_oft_checkpoint_warns(self, peft_version, tmp_path, recwarn):
        # In PEFT v0.18.0, there was a small change in the OFT implementation with Cayley-Neumann enabled. As the
        # outputs change slightly, users need to be warned about it if the checkpoint stems from a PEFT version below
        # 0.18.0. When the 'peft_version' key is not in the config, it means that the version is below 0.18.0.
        config = OFTConfig(target_modules=["lin"], use_cayley_neumann=True)  # only relevant when using Cayley-Neumann
        model = get_peft_model(self.get_model(), config)
        model.save_pretrained(tmp_path)
        del model

        # overwrite the peft_version
        with open(tmp_path / "adapter_config.json") as f:
            config_json = json.load(f)

        if peft_version is None:
>           del config_json["peft_version"]
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           KeyError: 'peft_version'

tests/test_initialization.py:1787: KeyError
_______________ TestHotSwapping.test_hotswap_works[True-config0] _______________
[gw2] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_initialization.TestHotSwapping object at 0x7fe4dc4ae210>
config = LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revisio...time_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None)
do_compile = True
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/popen-gw2/test_hotswap_works_True_config0')

    @pytest.mark.parametrize(
        "config",
        [
            LoraConfig(init_lora_weights=0, target_modules=["lin0"]),
            LoraConfig(init_lora_weights=0, target_modules=["lin0", "lin1"]),
        ],
    )
    @pytest.mark.parametrize("do_compile", [False, True])
    def test_hotswap_works(self, config, do_compile, tmp_path):
        # Load 2 different adapters and check that we can hotswap between them, with the model optionally being
        # compiled.
        atol, rtol = 1e-4, 1e-4
        inputs = torch.rand(3, 10).to(self.torch_device)

        # create adapter 0
        model = self.get_model()
        torch.manual_seed(0)
        model = get_peft_model(model, config)
        model = self.compile(model, do_compile=do_compile)
        model.eval()
        with torch.inference_mode():
>           output0 = model(inputs)
                      ^^^^^^^^^^^^^

tests/test_initialization.py:3309:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:375: in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:749: in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:923: in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:907: in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1578: in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1456: in codegen_and_compile
    compiled_module = graph.compile_to_module()
                      ^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2293: in compile_to_module
    return self._compile_to_module()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2299: in _compile_to_module
    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
                                                             ^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2238: in codegen
    self.scheduler.codegen()
venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py:4598: in codegen
    else self._codegen(self.nodes)
         ^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py:4750: in _codegen
    self.get_backend(device).codegen_node(node)
venv/lib/python3.11/site-packages/torch/_inductor/codegen/cpp.py:5076: in codegen_node
    cpp_kernel_proxy = self.kernel_proxy_cls(kernel_group)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/codegen/cpp.py:3816: in __init__
    self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:432: in pick_vec_isa
    _valid_vec_isa_list: list[VecISA] = valid_vec_isa_list()
                                        ^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:419: in valid_vec_isa_list
    isa_list.extend(
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:419: in <genexpr>
    isa_list.extend(
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:147: in __bool__
    return self.__bool__impl(config.cpp.vec_isa_ok)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:157: in __bool__impl
    return self.check_build(VecISA._avx_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:102: in check_build
    extra=_get_isa_dry_compile_fingerprint(self._arch_flags),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:28: in _get_isa_dry_compile_fingerprint
    compiler_info = get_compiler_version_info(get_cpp_compiler())
                                              ^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:155: in get_cpp_compiler
    compiler = cpp_compiler_search(search)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

search = (None, 'g++')

    @functools.lru_cache(1)
    def cpp_compiler_search(search: str) -> str:
        from torch._inductor.codecache import get_lock_dir, LOCK_TIMEOUT

        for cxx in search:
            try:
                if cxx is None:
                    # gxx package is only available for Linux
                    # according to https://anaconda.org/conda-forge/gxx/
                    if sys.platform != "linux":
                        continue
                    # Do not install GXX by default
                    if not os.getenv("TORCH_INDUCTOR_INSTALL_GXX"):
                        continue
                    from torch.utils._filelock import FileLock

                    lock_dir = get_lock_dir()
                    lock = FileLock(
                        os.path.join(lock_dir, "g++.lock"), timeout=LOCK_TIMEOUT
                    )
                    with lock:
                        cxx = install_gcc_via_conda()
                subprocess.check_output([cxx, "--version"])
                return cxx
            except (subprocess.SubprocessError, FileNotFoundError, ImportError):
                continue
>       raise exc.InvalidCxxCompiler
E       torch._inductor.exc.InductorError: InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')
E
E       Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

venv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:101: InductorError
_______________ TestHotSwapping.test_hotswap_works[True-config1] _______________
[gw2] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_initialization.TestHotSwapping object at 0x7fe4dc4ae950>
config = LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revisio...time_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None)
do_compile = True
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/popen-gw2/test_hotswap_works_True_config1')

    @pytest.mark.parametrize(
        "config",
        [
            LoraConfig(init_lora_weights=0, target_modules=["lin0"]),
            LoraConfig(init_lora_weights=0, target_modules=["lin0", "lin1"]),
        ],
    )
    @pytest.mark.parametrize("do_compile", [False, True])
    def test_hotswap_works(self, config, do_compile, tmp_path):
        # Load 2 different adapters and check that we can hotswap between them, with the model optionally being
        # compiled.
        atol, rtol = 1e-4, 1e-4
        inputs = torch.rand(3, 10).to(self.torch_device)

        # create adapter 0
        model = self.get_model()
        torch.manual_seed(0)
        model = get_peft_model(model, config)
        model = self.compile(model, do_compile=do_compile)
        model.eval()
        with torch.inference_mode():
>           output0 = model(inputs)
                      ^^^^^^^^^^^^^

tests/test_initialization.py:3309:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:375: in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:749: in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:923: in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:907: in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1578: in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1456: in codegen_and_compile
    compiled_module = graph.compile_to_module()
                      ^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2293: in compile_to_module
    return self._compile_to_module()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2299: in _compile_to_module
    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
                                                             ^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2238: in codegen
    self.scheduler.codegen()
venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py:4598: in codegen
    else self._codegen(self.nodes)
         ^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py:4750: in _codegen
    self.get_backend(device).codegen_node(node)
venv/lib/python3.11/site-packages/torch/_inductor/codegen/cpp.py:5076: in codegen_node
    cpp_kernel_proxy = self.kernel_proxy_cls(kernel_group)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/codegen/cpp.py:3816: in __init__
    self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:432: in pick_vec_isa
    _valid_vec_isa_list: list[VecISA] = valid_vec_isa_list()
                                        ^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:419: in valid_vec_isa_list
    isa_list.extend(
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:419: in <genexpr>
    isa_list.extend(
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:147: in __bool__
    return self.__bool__impl(config.cpp.vec_isa_ok)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:157: in __bool__impl
    return self.check_build(VecISA._avx_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:102: in check_build
    extra=_get_isa_dry_compile_fingerprint(self._arch_flags),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:28: in _get_isa_dry_compile_fingerprint
    compiler_info = get_compiler_version_info(get_cpp_compiler())
                                              ^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:155: in get_cpp_compiler
    compiler = cpp_compiler_search(search)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

search = (None, 'g++')

    @functools.lru_cache(1)
    def cpp_compiler_search(search: str) -> str:
        from torch._inductor.codecache import get_lock_dir, LOCK_TIMEOUT

        for cxx in search:
            try:
                if cxx is None:
                    # gxx package is only available for Linux
                    # according to https://anaconda.org/conda-forge/gxx/
                    if sys.platform != "linux":
                        continue
                    # Do not install GXX by default
                    if not os.getenv("TORCH_INDUCTOR_INSTALL_GXX"):
                        continue
                    from torch.utils._filelock import FileLock

                    lock_dir = get_lock_dir()
                    lock = FileLock(
                        os.path.join(lock_dir, "g++.lock"), timeout=LOCK_TIMEOUT
                    )
                    with lock:
                        cxx = install_gcc_via_conda()
                subprocess.check_output([cxx, "--version"])
                return cxx
            except (subprocess.SubprocessError, FileNotFoundError, ImportError):
                continue
>       raise exc.InvalidCxxCompiler
E       torch._inductor.exc.InductorError: InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')
E
E       Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

venv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:101: InductorError
____ PeftCustomKwargsTester.test_maybe_include_all_linear_layers_diffusion _____
[gw3] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_tuners_utils.PeftCustomKwargsTester testMethod=test_maybe_include_all_linear_layers_diffusion>

    def test_maybe_include_all_linear_layers_diffusion(self):
        model_id = "hf-internal-testing/tiny-sd-pipe"
        with hub_online_once(model_id):
>           model = StableDiffusionPipeline.from_pretrained(model_id)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_tuners_utils.py:347:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'transformers.models.clip.modeling_clip.CLIPTextModel'>
pretrained_model_name_or_path = '/root/.cache/huggingface/hub/models--hf-internal-testing--tiny-sd-pipe/snapshots/39f7e1819d772fbb8dd7f2aa3da8e2fc5a9bd917/text_encoder'
config = CLIPTextConfig {
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dty...en_layers": 5,
  "pad_token_id": 1,
  "projection_dim": 32,
  "transformers_version": "4.57.0",
  "vocab_size": 1000
}

cache_dir = None, ignore_mismatched_sizes = False, force_download = False
local_files_only = False, token = None, revision = 'main'
use_safetensors = None, weights_only = True, model_args = ()
kwargs = {'offload_state_dict': None}, state_dict = None

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        **kwargs,
    ) -> SpecificPreTrainedModelType:
        r"""
        Instantiate a pretrained pytorch model from a pre-trained model configuration.

        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you should first set it back in training mode with `model.train()`.

        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come
        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
        task.

        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those
        weights are discarded.

        Parameters:
            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):
                Can be either:

                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
                    - A path to a *directory* containing model weights saved using
                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In
                      this case, `from_tf` should be set to `True` and a configuration object should be provided as
                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a
                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.
                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,
                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to
                      `True`.
                    - `None` if you are both providing the configuration and state dictionary (resp. with keyword
                      arguments `config` and `state_dict`).
            model_args (sequence of positional arguments, *optional*):
                All remaining positional arguments will be passed to the underlying model's `__init__` method.
            config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):
                Can be either:

                    - an instance of a class derived from [`PretrainedConfig`],
                    - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].

                Configuration for the model to use instead of an automatically loaded configuration. Configuration can
                be automatically loaded when:

                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained
                      model).
                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the
                      save directory.
                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a
                      configuration JSON file named *config.json* is found in the directory.
            state_dict (`dict[str, torch.Tensor]`, *optional*):
                A state dictionary to use instead of a state dictionary loaded from saved weights file.

                This option can be used if you want to create a model from a pretrained configuration but load your own
                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and
                [`~PreTrainedModel.from_pretrained`] is not a simpler option.
            cache_dir (`Union[str, os.PathLike]`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the
                standard cache should not be used.
            from_tf (`bool`, *optional*, defaults to `False`):
                Load the model weights from a TensorFlow checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            from_flax (`bool`, *optional*, defaults to `False`):
                Load the model weights from a Flax checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):
                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size
                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a
                checkpoint with 3 labels).
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
                cached versions if they exist.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
            output_loading_info(`bool`, *optional*, defaults to `False`):
                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.
            local_files_only(`bool`, *optional*, defaults to `False`):
                Whether or not to only look at local files (i.e., do not try to download the model).
            token (`str` or `bool`, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use
                the token generated when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.

                <Tip>

                To test a pull request you made on the Hub, you can pass `revision="refs/pr/<pr_number>"`.

                </Tip>
            attn_implementation (`str`, *optional*):
                The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)), or `"flash_attention_3"` (using [Dao-AILab/flash-attention/hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.

                Accept HF kernel references in the form:
                  <namespace>/<repo_name>[@<revision>][:<kernel_name>]

                - <namespace> and <repo_name> are any non-"/" and non-":" sequences.
                - "@<revision>" is optional (branch, tag, or commit-ish), e.g. "@main", "@v1.2.0", "@abc123".
                - ":<kernel_name>" is optional and selects a function inside the kernel repo.
                - Both options can appear together and in this order only: @revision first, then :kernel_name.
                - We intentionally allow a leading "<wrapper>|" prefix (e.g., "flash|...") because the code
                  strips it before loading; '|' is not excluded in the character classes here.

                Examples that match:
                  "org/model"
                  "org/model@main"
                  "org/model:custom_kernel"
                  "org/model@v1.2.3:custom_kernel"

            > Parameters for big model inference

            dtype (`str` or `torch.dtype`, *optional*):
                Override the default `torch_dtype` and load the model under a specific `dtype`. The different options
                are:

                1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified
                  `dtype`, ignoring the model's `config.dtype` if one exists. If not specified
                  - the model will get loaded in `torch.float` (fp32).

                2. `"auto"` - A `dtype` or `torch_dtype` entry in the `config.json` file of the model will be
                  attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in
                  the checkpoint that's of a floating point type and use that as `dtype`. This will load the model
                  using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how
                  the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.

                3. A string that is a valid `torch.dtype`. E.g. "float32" loads the model in `torch.float32`, "float16" loads in `torch.float16` etc.

                <Tip>

                For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or
                reach out to the authors and ask them to add this information to the model's card and to insert the
                `dtype` or `torch_dtype` entry in `config.json` on the hub.

                </Tip>

            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):
                A map that specifies where each submodule should go. It doesn't need to be refined to each
                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
                same device. If we only pass the device (*e.g.*, `"cpu"`, `"cuda:1"`, `"mps"`, or a GPU ordinal rank
                like `1`) on which the model will be allocated, the device map will map the entire model to this
                device. Passing `device_map = 0` means put the whole model on GPU 0.

                To have Accelerate compute the most optimized `device_map` automatically, set `device_map="auto"`. For
                more information about each option see [designing a device
                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).
            max_memory (`Dict`, *optional*):
                A dictionary device identifier to maximum memory if using `device_map`. Will default to the maximum memory available for each
                GPU and the available CPU RAM if unset.
            tp_plan (`str`, *optional*):
                A torch tensor parallel plan, see [here](https://pytorch.org/tutorials/intermediate/TP_tutorial.html). Currently, it only accepts
                `tp_plan="auto"` to use predefined plan based on the model. Note that if you use it, you should launch your script accordingly with
                `torchrun [args] script.py`. This will be much faster than using a `device_map`, but has limitations.
            tp_size (`str`, *optional*):
                A torch tensor parallel degree. If not provided would default to world size.
            device_mesh (`torch.distributed.DeviceMesh`, *optional*):
                A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.
                If provided, it has to contain dimension named `"tp"` in case it's > 1 dimensional, this dimension will be used for tensor parallelism
            offload_folder (`str` or `os.PathLike`, *optional*):
                If the `device_map` contains any value `"disk"`, the folder where we will offload weights.
            offload_buffers (`bool`, *optional*):
                Whether or not to offload the buffers with the model parameters.
            quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):
                A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g
                bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and
                `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes
                quantizations and not preferred. consider inserting all such arguments into quantization_config
                instead.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            variant (`str`, *optional*):
                If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is
                ignored when using `from_tf` or `from_flax`.
            use_safetensors (`bool`, *optional*, defaults to `None`):
                Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`
                is not installed, it will be set to `False`.
            weights_only (`bool`, *optional*, defaults to `True`):
                Indicates whether unpickler should be restricted to loading only tensors, primitive types,
                dictionaries and any types added via torch.serialization.add_safe_globals().
                When set to False, we can load wrapper tensor subclass weights.
            key_mapping (`dict[str, str], *optional*):
                A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers
                architecture, but was not converted accordingly.
            kwargs (remaining dictionary of keyword arguments, *optional*):
                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or
                automatically loaded:

                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the
                      underlying model's `__init__` method (we assume all relevant updates to the configuration have
                      already been done)
                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class
                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that
                      corresponds to a configuration attribute will be used to override said attribute with the
                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute
                      will be passed to the underlying model's `__init__` function.

        <Tip>

        Activate the special ["offline-mode"](https://huggingface.co/transformers/installation.html#offline-mode) to
        use this method in a firewalled environment.

        </Tip>

        Examples:

        ```python
        >>> from transformers import BertConfig, BertModel

        >>> # Download model and configuration from huggingface.co and cache.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased")
        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
        >>> model = BertModel.from_pretrained("./test/saved_model/")
        >>> # Update configuration during loading.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", output_attentions=True)
        >>> assert model.config.output_attentions == True
        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).
        >>> config = BertConfig.from_json_file("./tf_model/my_tf_model_config.json")
        >>> model = BertModel.from_pretrained("./tf_model/my_tf_checkpoint.ckpt.index", from_tf=True, config=config)
        >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", from_flax=True)
        ```
        """
        state_dict = kwargs.pop("state_dict", None)
        from_tf = kwargs.pop("from_tf", False)
        from_flax = kwargs.pop("from_flax", False)
        proxies = kwargs.pop("proxies", None)
        output_loading_info = kwargs.pop("output_loading_info", False)
        use_auth_token = kwargs.pop("use_auth_token", None)
        from_pipeline = kwargs.pop("_from_pipeline", None)
        from_auto_class = kwargs.pop("_from_auto", False)
        dtype = kwargs.pop("dtype", None)
        torch_dtype = kwargs.pop("torch_dtype", None)  # kept for BC
        device_map = kwargs.pop("device_map", None)
        max_memory = kwargs.pop("max_memory", None)
        offload_folder = kwargs.pop("offload_folder", None)
        offload_buffers = kwargs.pop("offload_buffers", False)
        load_in_8bit = kwargs.pop("load_in_8bit", False)
        load_in_4bit = kwargs.pop("load_in_4bit", False)
        quantization_config = kwargs.pop("quantization_config", None)
        subfolder = kwargs.pop("subfolder", "")
        commit_hash = kwargs.pop("_commit_hash", None)
        variant = kwargs.pop("variant", None)
        adapter_kwargs = kwargs.pop("adapter_kwargs", {})
        adapter_name = kwargs.pop("adapter_name", "default")
        generation_config = kwargs.pop("generation_config", None)
        gguf_file = kwargs.pop("gguf_file", None)
        tp_plan = kwargs.pop("tp_plan", None)
        tp_size = kwargs.pop("tp_size", None)
        distributed_config: DistributedConfig = kwargs.pop("distributed_config", None)
        device_mesh = kwargs.pop("device_mesh", None)
        trust_remote_code = kwargs.pop("trust_remote_code", None)
        use_kernels = kwargs.pop("use_kernels", False)

        key_mapping = kwargs.pop("key_mapping", None)
        # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model
        if key_mapping is None and any(
            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS
        ):
            key_mapping = cls._checkpoint_conversion_mapping

        if distributed_config is not None:
            tp_plan = "auto"

        # Not used anymore -- remove them from the kwargs
        _ = kwargs.pop("resume_download", None)
        _ = kwargs.pop("mirror", None)
        _ = kwargs.pop("_fast_init", True)
        _ = kwargs.pop("low_cpu_mem_usage", None)

        # For BC on torch_dtype argument
        if torch_dtype is not None:
            logger.warning_once("`torch_dtype` is deprecated! Use `dtype` instead!")
            # If both kwargs are provided, use `dtype`
            dtype = dtype if dtype is not None else torch_dtype

        if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):
            raise ValueError(
                "`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies."
            )
        if tp_size is not None and tp_plan is None:
            raise ValueError("tp_plan has to be set when tp_size is passed.")
        if tp_plan is not None and tp_plan != "auto":
            # TODO: we can relax this check when we support taking tp_plan from a json file, for example.
            raise ValueError(f"tp_plan supports 'auto' only for now but got {tp_plan}.")
        if tp_plan is not None and device_map is not None:
            raise ValueError(
                "`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization."
            )

        if device_map == "auto" and int(os.environ.get("WORLD_SIZE", "0")):
            logger.info(
                "You've set device_map=`auto` while triggering a distributed run with torchrun. This might lead to unexpected behavior. "
                "If your plan is to load the model on each device, you should set device_map={"
                ": PartialState().process_index} where PartialState comes from accelerate library"
            )

        # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple
        # `device_map` pointing to the correct device
        if tp_plan is not None:
            if device_mesh is None:
                tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(tp_plan, tp_size=tp_size)
            else:
                if device_mesh.ndim > 1:
                    if "tp" not in device_mesh.mesh_dim_names:
                        raise ValueError(
                            "When using `tp_plan` and n-d `device_mesh`, it must contain a 'tp' dimension. "
                            "Please provide a valid `device_mesh`."
                        )
                    device_mesh = device_mesh["tp"]
                tp_size = device_mesh.size()
                device_map = torch.device(f"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}")

            if tp_size is None:
                tp_size = torch.distributed.get_world_size()

        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError(
                    "`token` and `use_auth_token` are both specified. Please set only the argument `token`."
                )
            token = use_auth_token

        if token is not None and adapter_kwargs is not None and "token" not in adapter_kwargs:
            adapter_kwargs["token"] = token

        if gguf_file is not None and not is_accelerate_available():
            raise ValueError("accelerate is required when loading a GGUF file `pip install accelerate`.")

        if commit_hash is None:
            if not isinstance(config, PretrainedConfig):
                # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible
                resolved_config_file = cached_file(
                    pretrained_model_name_or_path,
                    CONFIG_NAME,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    token=token,
                    revision=revision,
                    subfolder=subfolder,
                    _raise_exceptions_for_gated_repo=False,
                    _raise_exceptions_for_missing_entries=False,
                    _raise_exceptions_for_connection_errors=False,
                )
                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)
            else:
                commit_hash = getattr(config, "_commit_hash", None)

        if is_peft_available():
            _adapter_model_path = adapter_kwargs.pop("_adapter_model_path", None)

            if _adapter_model_path is None:
                _adapter_model_path = find_adapter_config_file(
                    pretrained_model_name_or_path,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    _commit_hash=commit_hash,
                    **adapter_kwargs,
                )
            if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):
                with open(_adapter_model_path, "r", encoding="utf-8") as f:
                    _adapter_model_path = pretrained_model_name_or_path
                    pretrained_model_name_or_path = json.load(f)["base_model_name_or_path"]
        else:
            _adapter_model_path = None

        # Potentially detect context manager or global device, and use it (only if no device_map was provided)
        if device_map is None and not is_deepspeed_zero3_enabled():
            device_in_context = get_torch_context_manager_or_global_device()
            if device_in_context == torch.device("meta"):
                raise RuntimeError(
                    "You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\n"
                    "This is an anti-pattern as `from_pretrained` wants to load existing weights.\nIf you want to initialize an "
                    "empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`"
                )
            device_map = device_in_context

        # change device_map into a map if we passed an int, a str or a torch.device
        if isinstance(device_map, torch.device):
            device_map = {"": device_map}
        elif isinstance(device_map, str) and device_map not in ["auto", "balanced", "balanced_low_0", "sequential"]:
            try:
                device_map = {"": torch.device(device_map)}
            except RuntimeError:
                raise ValueError(
                    "When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or "
                    f"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}."
                )
        elif isinstance(device_map, int):
            if device_map < 0:
                raise ValueError(
                    "You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' "
                )
            else:
                device_map = {"": device_map}

        if device_map is not None:
            if is_deepspeed_zero3_enabled():
                raise ValueError("DeepSpeed Zero-3 is not compatible with passing a `device_map`.")
            if not is_accelerate_available():
                raise ValueError(
                    "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` "
                    "requires `accelerate`. You can install it with `pip install accelerate`"
                )

        # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.
        if load_in_4bit or load_in_8bit:
            if quantization_config is not None:
                raise ValueError(
                    "You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing "
                    "`quantization_config` argument at the same time."
                )

            # preparing BitsAndBytesConfig from kwargs
            config_dict = {k: v for k, v in kwargs.items() if k in inspect.signature(BitsAndBytesConfig).parameters}
            config_dict = {**config_dict, "load_in_4bit": load_in_4bit, "load_in_8bit": load_in_8bit}
            quantization_config, kwargs = BitsAndBytesConfig.from_dict(
                config_dict=config_dict, return_unused_kwargs=True, **kwargs
            )
            logger.warning(
                "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. "
                "Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead."
            )

        from_pt = not (from_tf | from_flax)

        user_agent = {"file_type": "model", "framework": "pytorch", "from_auto_class": from_auto_class}
        if from_pipeline is not None:
            user_agent["using_pipeline"] = from_pipeline

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True

        # Load config if we don't provide a configuration
        if not isinstance(config, PretrainedConfig):
            config_path = config if config is not None else pretrained_model_name_or_path
            config, model_kwargs = cls.config_class.from_pretrained(
                config_path,
                cache_dir=cache_dir,
                return_unused_kwargs=True,
                force_download=force_download,
                proxies=proxies,
                local_files_only=local_files_only,
                token=token,
                revision=revision,
                subfolder=subfolder,
                gguf_file=gguf_file,
                _from_auto=from_auto_class,
                _from_pipeline=from_pipeline,
                **kwargs,
            )
            if "gguf_file" in model_kwargs:
                model_kwargs.pop("gguf_file")
        else:
            config = copy.deepcopy(config)
            model_kwargs = kwargs

        # Because some composite configs call super().__init__ before instantiating the sub-configs, we need this call
        # to correctly redispatch recursively if the kwarg is provided
        if "attn_implementation" in kwargs:
            config._attn_implementation = kwargs.pop("attn_implementation")

        transformers_explicit_filename = getattr(config, "transformers_weights", None)

        if transformers_explicit_filename is not None:
            if not transformers_explicit_filename.endswith(
                ".safetensors"
            ) and not transformers_explicit_filename.endswith(".safetensors.index.json"):
                raise ValueError(
                    "The transformers file in the config seems to be incorrect: it is neither a safetensors file "
                    "(*.safetensors) nor a safetensors index file (*.safetensors.index.json): "
                    f"{transformers_explicit_filename}"
                )

        hf_quantizer, config, dtype, device_map = get_hf_quantizer(
            config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent
        )

        if gguf_file is not None and hf_quantizer is not None:
            raise ValueError(
                "You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub."
            )

        if (
            gguf_file
            and device_map is not None
            and ((isinstance(device_map, dict) and "disk" in device_map.values()) or "disk" in device_map)
        ):
            raise RuntimeError(
                "One or more modules is configured to be mapped to disk. Disk offload is not supported for models "
                "loaded from GGUF files."
            )

        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
            pretrained_model_name_or_path=pretrained_model_name_or_path,
            subfolder=subfolder,
            variant=variant,
            gguf_file=gguf_file,
            from_tf=from_tf,
            from_flax=from_flax,
            use_safetensors=use_safetensors,
            cache_dir=cache_dir,
            force_download=force_download,
            proxies=proxies,
            local_files_only=local_files_only,
            token=token,
            user_agent=user_agent,
            revision=revision,
            commit_hash=commit_hash,
            is_remote_code=cls._auto_class is not None,
            transformers_explicit_filename=transformers_explicit_filename,
        )

        is_sharded = sharded_metadata is not None
        is_quantized = hf_quantizer is not None
        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None

        if is_from_file and not is_sharded and checkpoint_files[0].endswith(".safetensors"):
            with safe_open(checkpoint_files[0], framework="pt") as f:
                metadata = f.metadata()

            if metadata is None:
                # Assume it's a pytorch checkpoint (introduced for timm checkpoints)
                pass
            elif metadata.get("format") == "pt":
                pass
            elif metadata.get("format") == "tf":
                from_tf = True
                logger.info("A TensorFlow safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "flax":
                from_flax = True
                logger.info("A Flax safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "mlx":
                # This is a mlx file, we assume weights are compatible with pt
                pass
            else:
                raise ValueError(
                    f"Incompatible safetensors file. File metadata is not ['pt', 'tf', 'flax', 'mlx'] but {metadata.get('format')}"
                )

        from_pt = not (from_tf | from_flax)

        if from_pt:
            if gguf_file:
                from .modeling_gguf_pytorch_utils import load_gguf_checkpoint

                # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was
                # passed directly as a kwarg from now on
                with torch.device("meta"):
                    dummy_model = cls(config)
                state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[
                    "tensors"
                ]

            # Find the correct dtype based on current state
            config, dtype, dtype_orig = _get_dtype(
                cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only
            )

        config.name_or_path = pretrained_model_name_or_path
        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)
        config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.
        with ContextManagers(model_init_context):
            # Let's make sure we don't run the init function of buffer modules
>           model = cls(config, *model_args, **model_kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'

venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: TypeError
----------------------------- Captured stderr call -----------------------------
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  40%|████▊       | 2/5 [00:00<00:00, 36.42it/s]
_________________ TestScalingAdapters.test_diffusers_pipeline __________________
[gw0] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_helpers.TestScalingAdapters object at 0x7f8dadb328d0>

    def test_diffusers_pipeline(self):
        model_id = "hf-internal-testing/tiny-sd-pipe"
>       pipeline = StableDiffusionPipeline.from_pretrained(model_id)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_helpers.py:185:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'transformers.models.clip.modeling_clip.CLIPTextModel'>
pretrained_model_name_or_path = '/root/.cache/huggingface/hub/models--hf-internal-testing--tiny-sd-pipe/snapshots/39f7e1819d772fbb8dd7f2aa3da8e2fc5a9bd917/text_encoder'
config = CLIPTextConfig {
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dty...en_layers": 5,
  "pad_token_id": 1,
  "projection_dim": 32,
  "transformers_version": "4.57.0",
  "vocab_size": 1000
}

cache_dir = None, ignore_mismatched_sizes = False, force_download = False
local_files_only = False, token = None, revision = 'main'
use_safetensors = None, weights_only = True, model_args = ()
kwargs = {'offload_state_dict': None}, state_dict = None

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        **kwargs,
    ) -> SpecificPreTrainedModelType:
        r"""
        Instantiate a pretrained pytorch model from a pre-trained model configuration.

        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you should first set it back in training mode with `model.train()`.

        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come
        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
        task.

        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those
        weights are discarded.

        Parameters:
            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):
                Can be either:

                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
                    - A path to a *directory* containing model weights saved using
                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In
                      this case, `from_tf` should be set to `True` and a configuration object should be provided as
                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a
                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.
                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,
                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to
                      `True`.
                    - `None` if you are both providing the configuration and state dictionary (resp. with keyword
                      arguments `config` and `state_dict`).
            model_args (sequence of positional arguments, *optional*):
                All remaining positional arguments will be passed to the underlying model's `__init__` method.
            config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):
                Can be either:

                    - an instance of a class derived from [`PretrainedConfig`],
                    - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].

                Configuration for the model to use instead of an automatically loaded configuration. Configuration can
                be automatically loaded when:

                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained
                      model).
                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the
                      save directory.
                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a
                      configuration JSON file named *config.json* is found in the directory.
            state_dict (`dict[str, torch.Tensor]`, *optional*):
                A state dictionary to use instead of a state dictionary loaded from saved weights file.

                This option can be used if you want to create a model from a pretrained configuration but load your own
                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and
                [`~PreTrainedModel.from_pretrained`] is not a simpler option.
            cache_dir (`Union[str, os.PathLike]`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the
                standard cache should not be used.
            from_tf (`bool`, *optional*, defaults to `False`):
                Load the model weights from a TensorFlow checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            from_flax (`bool`, *optional*, defaults to `False`):
                Load the model weights from a Flax checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):
                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size
                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a
                checkpoint with 3 labels).
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
                cached versions if they exist.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
            output_loading_info(`bool`, *optional*, defaults to `False`):
                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.
            local_files_only(`bool`, *optional*, defaults to `False`):
                Whether or not to only look at local files (i.e., do not try to download the model).
            token (`str` or `bool`, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use
                the token generated when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.

                <Tip>

                To test a pull request you made on the Hub, you can pass `revision="refs/pr/<pr_number>"`.

                </Tip>
            attn_implementation (`str`, *optional*):
                The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)), or `"flash_attention_3"` (using [Dao-AILab/flash-attention/hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.

                Accept HF kernel references in the form:
                  <namespace>/<repo_name>[@<revision>][:<kernel_name>]

                - <namespace> and <repo_name> are any non-"/" and non-":" sequences.
                - "@<revision>" is optional (branch, tag, or commit-ish), e.g. "@main", "@v1.2.0", "@abc123".
                - ":<kernel_name>" is optional and selects a function inside the kernel repo.
                - Both options can appear together and in this order only: @revision first, then :kernel_name.
                - We intentionally allow a leading "<wrapper>|" prefix (e.g., "flash|...") because the code
                  strips it before loading; '|' is not excluded in the character classes here.

                Examples that match:
                  "org/model"
                  "org/model@main"
                  "org/model:custom_kernel"
                  "org/model@v1.2.3:custom_kernel"

            > Parameters for big model inference

            dtype (`str` or `torch.dtype`, *optional*):
                Override the default `torch_dtype` and load the model under a specific `dtype`. The different options
                are:

                1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified
                  `dtype`, ignoring the model's `config.dtype` if one exists. If not specified
                  - the model will get loaded in `torch.float` (fp32).

                2. `"auto"` - A `dtype` or `torch_dtype` entry in the `config.json` file of the model will be
                  attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in
                  the checkpoint that's of a floating point type and use that as `dtype`. This will load the model
                  using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how
                  the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.

                3. A string that is a valid `torch.dtype`. E.g. "float32" loads the model in `torch.float32`, "float16" loads in `torch.float16` etc.

                <Tip>

                For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or
                reach out to the authors and ask them to add this information to the model's card and to insert the
                `dtype` or `torch_dtype` entry in `config.json` on the hub.

                </Tip>

            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):
                A map that specifies where each submodule should go. It doesn't need to be refined to each
                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
                same device. If we only pass the device (*e.g.*, `"cpu"`, `"cuda:1"`, `"mps"`, or a GPU ordinal rank
                like `1`) on which the model will be allocated, the device map will map the entire model to this
                device. Passing `device_map = 0` means put the whole model on GPU 0.

                To have Accelerate compute the most optimized `device_map` automatically, set `device_map="auto"`. For
                more information about each option see [designing a device
                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).
            max_memory (`Dict`, *optional*):
                A dictionary device identifier to maximum memory if using `device_map`. Will default to the maximum memory available for each
                GPU and the available CPU RAM if unset.
            tp_plan (`str`, *optional*):
                A torch tensor parallel plan, see [here](https://pytorch.org/tutorials/intermediate/TP_tutorial.html). Currently, it only accepts
                `tp_plan="auto"` to use predefined plan based on the model. Note that if you use it, you should launch your script accordingly with
                `torchrun [args] script.py`. This will be much faster than using a `device_map`, but has limitations.
            tp_size (`str`, *optional*):
                A torch tensor parallel degree. If not provided would default to world size.
            device_mesh (`torch.distributed.DeviceMesh`, *optional*):
                A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.
                If provided, it has to contain dimension named `"tp"` in case it's > 1 dimensional, this dimension will be used for tensor parallelism
            offload_folder (`str` or `os.PathLike`, *optional*):
                If the `device_map` contains any value `"disk"`, the folder where we will offload weights.
            offload_buffers (`bool`, *optional*):
                Whether or not to offload the buffers with the model parameters.
            quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):
                A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g
                bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and
                `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes
                quantizations and not preferred. consider inserting all such arguments into quantization_config
                instead.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            variant (`str`, *optional*):
                If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is
                ignored when using `from_tf` or `from_flax`.
            use_safetensors (`bool`, *optional*, defaults to `None`):
                Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`
                is not installed, it will be set to `False`.
            weights_only (`bool`, *optional*, defaults to `True`):
                Indicates whether unpickler should be restricted to loading only tensors, primitive types,
                dictionaries and any types added via torch.serialization.add_safe_globals().
                When set to False, we can load wrapper tensor subclass weights.
            key_mapping (`dict[str, str], *optional*):
                A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers
                architecture, but was not converted accordingly.
            kwargs (remaining dictionary of keyword arguments, *optional*):
                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or
                automatically loaded:

                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the
                      underlying model's `__init__` method (we assume all relevant updates to the configuration have
                      already been done)
                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class
                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that
                      corresponds to a configuration attribute will be used to override said attribute with the
                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute
                      will be passed to the underlying model's `__init__` function.

        <Tip>

        Activate the special ["offline-mode"](https://huggingface.co/transformers/installation.html#offline-mode) to
        use this method in a firewalled environment.

        </Tip>

        Examples:

        ```python
        >>> from transformers import BertConfig, BertModel

        >>> # Download model and configuration from huggingface.co and cache.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased")
        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
        >>> model = BertModel.from_pretrained("./test/saved_model/")
        >>> # Update configuration during loading.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", output_attentions=True)
        >>> assert model.config.output_attentions == True
        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).
        >>> config = BertConfig.from_json_file("./tf_model/my_tf_model_config.json")
        >>> model = BertModel.from_pretrained("./tf_model/my_tf_checkpoint.ckpt.index", from_tf=True, config=config)
        >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", from_flax=True)
        ```
        """
        state_dict = kwargs.pop("state_dict", None)
        from_tf = kwargs.pop("from_tf", False)
        from_flax = kwargs.pop("from_flax", False)
        proxies = kwargs.pop("proxies", None)
        output_loading_info = kwargs.pop("output_loading_info", False)
        use_auth_token = kwargs.pop("use_auth_token", None)
        from_pipeline = kwargs.pop("_from_pipeline", None)
        from_auto_class = kwargs.pop("_from_auto", False)
        dtype = kwargs.pop("dtype", None)
        torch_dtype = kwargs.pop("torch_dtype", None)  # kept for BC
        device_map = kwargs.pop("device_map", None)
        max_memory = kwargs.pop("max_memory", None)
        offload_folder = kwargs.pop("offload_folder", None)
        offload_buffers = kwargs.pop("offload_buffers", False)
        load_in_8bit = kwargs.pop("load_in_8bit", False)
        load_in_4bit = kwargs.pop("load_in_4bit", False)
        quantization_config = kwargs.pop("quantization_config", None)
        subfolder = kwargs.pop("subfolder", "")
        commit_hash = kwargs.pop("_commit_hash", None)
        variant = kwargs.pop("variant", None)
        adapter_kwargs = kwargs.pop("adapter_kwargs", {})
        adapter_name = kwargs.pop("adapter_name", "default")
        generation_config = kwargs.pop("generation_config", None)
        gguf_file = kwargs.pop("gguf_file", None)
        tp_plan = kwargs.pop("tp_plan", None)
        tp_size = kwargs.pop("tp_size", None)
        distributed_config: DistributedConfig = kwargs.pop("distributed_config", None)
        device_mesh = kwargs.pop("device_mesh", None)
        trust_remote_code = kwargs.pop("trust_remote_code", None)
        use_kernels = kwargs.pop("use_kernels", False)

        key_mapping = kwargs.pop("key_mapping", None)
        # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model
        if key_mapping is None and any(
            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS
        ):
            key_mapping = cls._checkpoint_conversion_mapping

        if distributed_config is not None:
            tp_plan = "auto"

        # Not used anymore -- remove them from the kwargs
        _ = kwargs.pop("resume_download", None)
        _ = kwargs.pop("mirror", None)
        _ = kwargs.pop("_fast_init", True)
        _ = kwargs.pop("low_cpu_mem_usage", None)

        # For BC on torch_dtype argument
        if torch_dtype is not None:
            logger.warning_once("`torch_dtype` is deprecated! Use `dtype` instead!")
            # If both kwargs are provided, use `dtype`
            dtype = dtype if dtype is not None else torch_dtype

        if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):
            raise ValueError(
                "`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies."
            )
        if tp_size is not None and tp_plan is None:
            raise ValueError("tp_plan has to be set when tp_size is passed.")
        if tp_plan is not None and tp_plan != "auto":
            # TODO: we can relax this check when we support taking tp_plan from a json file, for example.
            raise ValueError(f"tp_plan supports 'auto' only for now but got {tp_plan}.")
        if tp_plan is not None and device_map is not None:
            raise ValueError(
                "`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization."
            )

        if device_map == "auto" and int(os.environ.get("WORLD_SIZE", "0")):
            logger.info(
                "You've set device_map=`auto` while triggering a distributed run with torchrun. This might lead to unexpected behavior. "
                "If your plan is to load the model on each device, you should set device_map={"
                ": PartialState().process_index} where PartialState comes from accelerate library"
            )

        # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple
        # `device_map` pointing to the correct device
        if tp_plan is not None:
            if device_mesh is None:
                tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(tp_plan, tp_size=tp_size)
            else:
                if device_mesh.ndim > 1:
                    if "tp" not in device_mesh.mesh_dim_names:
                        raise ValueError(
                            "When using `tp_plan` and n-d `device_mesh`, it must contain a 'tp' dimension. "
                            "Please provide a valid `device_mesh`."
                        )
                    device_mesh = device_mesh["tp"]
                tp_size = device_mesh.size()
                device_map = torch.device(f"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}")

            if tp_size is None:
                tp_size = torch.distributed.get_world_size()

        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError(
                    "`token` and `use_auth_token` are both specified. Please set only the argument `token`."
                )
            token = use_auth_token

        if token is not None and adapter_kwargs is not None and "token" not in adapter_kwargs:
            adapter_kwargs["token"] = token

        if gguf_file is not None and not is_accelerate_available():
            raise ValueError("accelerate is required when loading a GGUF file `pip install accelerate`.")

        if commit_hash is None:
            if not isinstance(config, PretrainedConfig):
                # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible
                resolved_config_file = cached_file(
                    pretrained_model_name_or_path,
                    CONFIG_NAME,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    token=token,
                    revision=revision,
                    subfolder=subfolder,
                    _raise_exceptions_for_gated_repo=False,
                    _raise_exceptions_for_missing_entries=False,
                    _raise_exceptions_for_connection_errors=False,
                )
                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)
            else:
                commit_hash = getattr(config, "_commit_hash", None)

        if is_peft_available():
            _adapter_model_path = adapter_kwargs.pop("_adapter_model_path", None)

            if _adapter_model_path is None:
                _adapter_model_path = find_adapter_config_file(
                    pretrained_model_name_or_path,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    _commit_hash=commit_hash,
                    **adapter_kwargs,
                )
            if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):
                with open(_adapter_model_path, "r", encoding="utf-8") as f:
                    _adapter_model_path = pretrained_model_name_or_path
                    pretrained_model_name_or_path = json.load(f)["base_model_name_or_path"]
        else:
            _adapter_model_path = None

        # Potentially detect context manager or global device, and use it (only if no device_map was provided)
        if device_map is None and not is_deepspeed_zero3_enabled():
            device_in_context = get_torch_context_manager_or_global_device()
            if device_in_context == torch.device("meta"):
                raise RuntimeError(
                    "You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\n"
                    "This is an anti-pattern as `from_pretrained` wants to load existing weights.\nIf you want to initialize an "
                    "empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`"
                )
            device_map = device_in_context

        # change device_map into a map if we passed an int, a str or a torch.device
        if isinstance(device_map, torch.device):
            device_map = {"": device_map}
        elif isinstance(device_map, str) and device_map not in ["auto", "balanced", "balanced_low_0", "sequential"]:
            try:
                device_map = {"": torch.device(device_map)}
            except RuntimeError:
                raise ValueError(
                    "When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or "
                    f"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}."
                )
        elif isinstance(device_map, int):
            if device_map < 0:
                raise ValueError(
                    "You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' "
                )
            else:
                device_map = {"": device_map}

        if device_map is not None:
            if is_deepspeed_zero3_enabled():
                raise ValueError("DeepSpeed Zero-3 is not compatible with passing a `device_map`.")
            if not is_accelerate_available():
                raise ValueError(
                    "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` "
                    "requires `accelerate`. You can install it with `pip install accelerate`"
                )

        # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.
        if load_in_4bit or load_in_8bit:
            if quantization_config is not None:
                raise ValueError(
                    "You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing "
                    "`quantization_config` argument at the same time."
                )

            # preparing BitsAndBytesConfig from kwargs
            config_dict = {k: v for k, v in kwargs.items() if k in inspect.signature(BitsAndBytesConfig).parameters}
            config_dict = {**config_dict, "load_in_4bit": load_in_4bit, "load_in_8bit": load_in_8bit}
            quantization_config, kwargs = BitsAndBytesConfig.from_dict(
                config_dict=config_dict, return_unused_kwargs=True, **kwargs
            )
            logger.warning(
                "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. "
                "Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead."
            )

        from_pt = not (from_tf | from_flax)

        user_agent = {"file_type": "model", "framework": "pytorch", "from_auto_class": from_auto_class}
        if from_pipeline is not None:
            user_agent["using_pipeline"] = from_pipeline

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True

        # Load config if we don't provide a configuration
        if not isinstance(config, PretrainedConfig):
            config_path = config if config is not None else pretrained_model_name_or_path
            config, model_kwargs = cls.config_class.from_pretrained(
                config_path,
                cache_dir=cache_dir,
                return_unused_kwargs=True,
                force_download=force_download,
                proxies=proxies,
                local_files_only=local_files_only,
                token=token,
                revision=revision,
                subfolder=subfolder,
                gguf_file=gguf_file,
                _from_auto=from_auto_class,
                _from_pipeline=from_pipeline,
                **kwargs,
            )
            if "gguf_file" in model_kwargs:
                model_kwargs.pop("gguf_file")
        else:
            config = copy.deepcopy(config)
            model_kwargs = kwargs

        # Because some composite configs call super().__init__ before instantiating the sub-configs, we need this call
        # to correctly redispatch recursively if the kwarg is provided
        if "attn_implementation" in kwargs:
            config._attn_implementation = kwargs.pop("attn_implementation")

        transformers_explicit_filename = getattr(config, "transformers_weights", None)

        if transformers_explicit_filename is not None:
            if not transformers_explicit_filename.endswith(
                ".safetensors"
            ) and not transformers_explicit_filename.endswith(".safetensors.index.json"):
                raise ValueError(
                    "The transformers file in the config seems to be incorrect: it is neither a safetensors file "
                    "(*.safetensors) nor a safetensors index file (*.safetensors.index.json): "
                    f"{transformers_explicit_filename}"
                )

        hf_quantizer, config, dtype, device_map = get_hf_quantizer(
            config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent
        )

        if gguf_file is not None and hf_quantizer is not None:
            raise ValueError(
                "You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub."
            )

        if (
            gguf_file
            and device_map is not None
            and ((isinstance(device_map, dict) and "disk" in device_map.values()) or "disk" in device_map)
        ):
            raise RuntimeError(
                "One or more modules is configured to be mapped to disk. Disk offload is not supported for models "
                "loaded from GGUF files."
            )

        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
            pretrained_model_name_or_path=pretrained_model_name_or_path,
            subfolder=subfolder,
            variant=variant,
            gguf_file=gguf_file,
            from_tf=from_tf,
            from_flax=from_flax,
            use_safetensors=use_safetensors,
            cache_dir=cache_dir,
            force_download=force_download,
            proxies=proxies,
            local_files_only=local_files_only,
            token=token,
            user_agent=user_agent,
            revision=revision,
            commit_hash=commit_hash,
            is_remote_code=cls._auto_class is not None,
            transformers_explicit_filename=transformers_explicit_filename,
        )

        is_sharded = sharded_metadata is not None
        is_quantized = hf_quantizer is not None
        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None

        if is_from_file and not is_sharded and checkpoint_files[0].endswith(".safetensors"):
            with safe_open(checkpoint_files[0], framework="pt") as f:
                metadata = f.metadata()

            if metadata is None:
                # Assume it's a pytorch checkpoint (introduced for timm checkpoints)
                pass
            elif metadata.get("format") == "pt":
                pass
            elif metadata.get("format") == "tf":
                from_tf = True
                logger.info("A TensorFlow safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "flax":
                from_flax = True
                logger.info("A Flax safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "mlx":
                # This is a mlx file, we assume weights are compatible with pt
                pass
            else:
                raise ValueError(
                    f"Incompatible safetensors file. File metadata is not ['pt', 'tf', 'flax', 'mlx'] but {metadata.get('format')}"
                )

        from_pt = not (from_tf | from_flax)

        if from_pt:
            if gguf_file:
                from .modeling_gguf_pytorch_utils import load_gguf_checkpoint

                # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was
                # passed directly as a kwarg from now on
                with torch.device("meta"):
                    dummy_model = cls(config)
                state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[
                    "tensors"
                ]

            # Find the correct dtype based on current state
            config, dtype, dtype_orig = _get_dtype(
                cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only
            )

        config.name_or_path = pretrained_model_name_or_path
        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)
        config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.
        with ContextManagers(model_init_context):
            # Let's make sure we don't run the init function of buffer modules
>           model = cls(config, *model_args, **model_kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'

venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: TypeError
----------------------------- Captured stderr call -----------------------------
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  40%|████▍      | 2/5 [00:00<00:00, 183.83it/s]
_________ TestBaseModelRevision.test_save_and_load_base_model_revision _________
[gw0] linux -- Python 3.11.0 /app/venv/bin/python

response = <Response [429]>, endpoint_name = None

    def hf_raise_for_status(response: Response, endpoint_name: Optional[str] = None) -> None:
        """
        Internal version of `response.raise_for_status()` that will refine a
        potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.

        This helper is meant to be the unique method to raise_for_status when making a call
        to the Hugging Face Hub.


        Example:
        ```py
            import requests
            from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError

            response = get_session().post(...)
            try:
                hf_raise_for_status(response)
            except HfHubHTTPError as e:
                print(str(e)) # formatted message
                e.request_id, e.server_message # details returned by server

                # Complete the error message with additional information once it's raised
                e.append_to_message("\n`create_commit` expects the repository to exist.")
                raise
        ```

        Args:
            response (`Response`):
                Response from the server.
            endpoint_name (`str`, *optional*):
                Name of the endpoint that has been called. If provided, the error message
                will be more complete.

        <Tip warning={true}>

        Raises when the request has failed:

            - [`~utils.RepositoryNotFoundError`]
                If the repository to download from cannot be found. This may be because it
                doesn't exist, because `repo_type` is not set correctly, or because the repo
                is `private` and you do not have access.
            - [`~utils.GatedRepoError`]
                If the repository exists but is gated and the user is not on the authorized
                list.
            - [`~utils.RevisionNotFoundError`]
                If the repository exists but the revision couldn't be find.
            - [`~utils.EntryNotFoundError`]
                If the repository exists but the entry (e.g. the requested file) couldn't be
                find.
            - [`~utils.BadRequestError`]
                If request failed with a HTTP 400 BadRequest error.
            - [`~utils.HfHubHTTPError`]
                If request failed for a reason not listed above.

        </Tip>
        """
        try:
>           response.raise_for_status()

venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:407:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <Response [429]>

    def raise_for_status(self):
        """Raises :class:`HTTPError`, if one occurred."""

        http_error_msg = ""
        if isinstance(self.reason, bytes):
            # We attempt to decode utf-8 first because some servers
            # choose to localize their reason strings. If the string
            # isn't utf-8, we fall back to iso-8859-1 for all other
            # encodings. (See PR #3538)
            try:
                reason = self.reason.decode("utf-8")
            except UnicodeDecodeError:
                reason = self.reason.decode("iso-8859-1")
        else:
            reason = self.reason

        if 400 <= self.status_code < 500:
            http_error_msg = (
                f"{self.status_code} Client Error: {reason} for url: {self.url}"
            )

        elif 500 <= self.status_code < 600:
            http_error_msg = (
                f"{self.status_code} Server Error: {reason} for url: {self.url}"
            )

        if http_error_msg:
>           raise HTTPError(http_error_msg, response=self)
E           requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/peft-internal-testing/tiny-random-BertModel/xet-read-token/08f3923b4fa5c5b5945392c81ee228b83acd623b

venv/lib/python3.11/site-packages/requests/models.py:1026: HTTPError

The above exception was the direct cause of the following exception:

path_or_repo_id = 'peft-internal-testing/tiny-random-BertModel'
filenames = ['model.safetensors'], cache_dir = '/root/.cache/huggingface/hub'
force_download = False, resume_download = None, proxies = None, token = None
revision = 'main', local_files_only = False, subfolder = '', repo_type = None
user_agent = 'transformers/4.57.0; python/3.11.0rc1; session_id/a3985ba321dc4338b6c4b346b90bfa42; torch/2.8.0; file_type/model; framework/pytorch; from_auto_class/False'
_raise_exceptions_for_gated_repo = False
_raise_exceptions_for_missing_entries = False
_raise_exceptions_for_connection_errors = True
_commit_hash = '08f3923b4fa5c5b5945392c81ee228b83acd623b'
deprecated_kwargs = {}, use_auth_token = None
full_filenames = ['model.safetensors'], existing_files = []
filename = 'model.safetensors', resolved_file = None, file_counter = 0

    def cached_files(
        path_or_repo_id: Union[str, os.PathLike],
        filenames: list[str],
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        force_download: bool = False,
        resume_download: Optional[bool] = None,
        proxies: Optional[dict[str, str]] = None,
        token: Optional[Union[bool, str]] = None,
        revision: Optional[str] = None,
        local_files_only: bool = False,
        subfolder: str = "",
        repo_type: Optional[str] = None,
        user_agent: Optional[Union[str, dict[str, str]]] = None,
        _raise_exceptions_for_gated_repo: bool = True,
        _raise_exceptions_for_missing_entries: bool = True,
        _raise_exceptions_for_connection_errors: bool = True,
        _commit_hash: Optional[str] = None,
        **deprecated_kwargs,
    ) -> Optional[str]:
        """
        Tries to locate several files in a local folder and repo, downloads and cache them if necessary.

        Args:
            path_or_repo_id (`str` or `os.PathLike`):
                This can be either:
                - a string, the *model id* of a model repo on huggingface.co.
                - a path to a *directory* potentially containing the file.
            filenames (`list[str]`):
                The name of all the files to locate in `path_or_repo`.
            cache_dir (`str` or `os.PathLike`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the standard
                cache should not be used.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force to (re-)download the configuration files and override the cached versions if they
                exist.
            resume_download:
                Deprecated and ignored. All downloads are now resumed by default when possible.
                Will be removed in v5 of Transformers.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.
            token (`str` or *bool*, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated
                when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.
            local_files_only (`bool`, *optional*, defaults to `False`):
                If `True`, will only try to load the tokenizer configuration from local files.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            repo_type (`str`, *optional*):
                Specify the repo type (useful when downloading from a space for instance).

        Private args:
            _raise_exceptions_for_gated_repo (`bool`):
                if False, do not raise an exception for gated repo error but return None.
            _raise_exceptions_for_missing_entries (`bool`):
                if False, do not raise an exception for missing entries but return None.
            _raise_exceptions_for_connection_errors (`bool`):
                if False, do not raise an exception for connection errors but return None.
            _commit_hash (`str`, *optional*):
                passed when we are chaining several calls to various files (e.g. when loading a tokenizer or
                a pipeline). If files are cached for this commit hash, avoid calls to head and get from the cache.

        <Tip>

        Passing `token=True` is required when you want to use a private model.

        </Tip>

        Returns:
            `Optional[str]`: Returns the resolved file (to the cache folder if downloaded from a repo).

        Examples:

        ```python
        # Download a model weight from the Hub and cache it.
        model_weights_file = cached_file("google-bert/bert-base-uncased", "pytorch_model.bin")
        ```
        """
        use_auth_token = deprecated_kwargs.pop("use_auth_token", None)
        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
            token = use_auth_token

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True
        if subfolder is None:
            subfolder = ""

        # Add folder to filenames
        full_filenames = [os.path.join(subfolder, file) for file in filenames]

        path_or_repo_id = str(path_or_repo_id)
        existing_files = []
        for filename in full_filenames:
            if os.path.isdir(path_or_repo_id):
                resolved_file = os.path.join(path_or_repo_id, filename)
                if not os.path.isfile(resolved_file):
                    if _raise_exceptions_for_missing_entries and filename != os.path.join(subfolder, "config.json"):
                        revision_ = "main" if revision is None else revision
                        raise OSError(
                            f"{path_or_repo_id} does not appear to have a file named {filename}. Checkout "
                            f"'https://huggingface.co/{path_or_repo_id}/tree/{revision_}' for available files."
                        )
                    else:
                        continue
                existing_files.append(resolved_file)

        if os.path.isdir(path_or_repo_id):
            return existing_files if existing_files else None

        if cache_dir is None:
            cache_dir = TRANSFORMERS_CACHE
        if isinstance(cache_dir, Path):
            cache_dir = str(cache_dir)

        existing_files = []
        file_counter = 0
        if _commit_hash is not None and not force_download:
            for filename in full_filenames:
                # If the file is cached under that commit hash, we return it directly.
                resolved_file = try_to_load_from_cache(
                    path_or_repo_id, filename, cache_dir=cache_dir, revision=_commit_hash, repo_type=repo_type
                )
                if resolved_file is not None:
                    if resolved_file is not _CACHED_NO_EXIST:
                        file_counter += 1
                        existing_files.append(resolved_file)
                    elif not _raise_exceptions_for_missing_entries:
                        file_counter += 1
                    else:
                        raise OSError(f"Could not locate {filename} inside {path_or_repo_id}.")

        # Either all the files were found, or some were _CACHED_NO_EXIST but we do not raise for missing entries
        if file_counter == len(full_filenames):
            return existing_files if len(existing_files) > 0 else None

        user_agent = http_user_agent(user_agent)
        # download the files if needed
        try:
            if len(full_filenames) == 1:
                # This is slightly better for only 1 file
>               hf_hub_download(
                    path_or_repo_id,
                    filenames[0],
                    subfolder=None if len(subfolder) == 0 else subfolder,
                    repo_type=repo_type,
                    revision=revision,
                    cache_dir=cache_dir,
                    user_agent=user_agent,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    token=token,
                    local_files_only=local_files_only,
                )

venv/lib/python3.11/site-packages/transformers/utils/hub.py:479:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1010: in hf_hub_download
    return _hf_hub_download_to_cache_dir(
venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1171: in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1723: in _download_to_tmp_and_move
    xet_get(
venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:594: in xet_get
    connection_info = refresh_xet_connection_info(file_data=xet_file_data, headers=headers)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/utils/_xet.py:116: in refresh_xet_connection_info
    return _fetch_xet_connection_info_with_url(file_data.refresh_route, headers)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/utils/_xet.py:187: in _fetch_xet_connection_info_with_url
    hf_raise_for_status(resp)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

response = <Response [429]>, endpoint_name = None

    def hf_raise_for_status(response: Response, endpoint_name: Optional[str] = None) -> None:
        """
        Internal version of `response.raise_for_status()` that will refine a
        potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.

        This helper is meant to be the unique method to raise_for_status when making a call
        to the Hugging Face Hub.


        Example:
        ```py
            import requests
            from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError

            response = get_session().post(...)
            try:
                hf_raise_for_status(response)
            except HfHubHTTPError as e:
                print(str(e)) # formatted message
                e.request_id, e.server_message # details returned by server

                # Complete the error message with additional information once it's raised
                e.append_to_message("\n`create_commit` expects the repository to exist.")
                raise
        ```

        Args:
            response (`Response`):
                Response from the server.
            endpoint_name (`str`, *optional*):
                Name of the endpoint that has been called. If provided, the error message
                will be more complete.

        <Tip warning={true}>

        Raises when the request has failed:

            - [`~utils.RepositoryNotFoundError`]
                If the repository to download from cannot be found. This may be because it
                doesn't exist, because `repo_type` is not set correctly, or because the repo
                is `private` and you do not have access.
            - [`~utils.GatedRepoError`]
                If the repository exists but is gated and the user is not on the authorized
                list.
            - [`~utils.RevisionNotFoundError`]
                If the repository exists but the revision couldn't be find.
            - [`~utils.EntryNotFoundError`]
                If the repository exists but the entry (e.g. the requested file) couldn't be
                find.
            - [`~utils.BadRequestError`]
                If request failed with a HTTP 400 BadRequest error.
            - [`~utils.HfHubHTTPError`]
                If request failed for a reason not listed above.

        </Tip>
        """
        try:
            response.raise_for_status()
        except HTTPError as e:
            error_code = response.headers.get("X-Error-Code")
            error_message = response.headers.get("X-Error-Message")

            if error_code == "RevisionNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Revision Not Found for url: {response.url}."
                raise _format(RevisionNotFoundError, message, response) from e

            elif error_code == "EntryNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Entry Not Found for url: {response.url}."
                raise _format(EntryNotFoundError, message, response) from e

            elif error_code == "GatedRepo":
                message = (
                    f"{response.status_code} Client Error." + "\n\n" + f"Cannot access gated repo for url {response.url}."
                )
                raise _format(GatedRepoError, message, response) from e

            elif error_message == "Access to this resource is disabled.":
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Cannot access repository for url {response.url}."
                    + "\n"
                    + "Access to this resource is disabled."
                )
                raise _format(DisabledRepoError, message, response) from e

            elif error_code == "RepoNotFound" or (
                response.status_code == 401
                and error_message != "Invalid credentials in Authorization header"
                and response.request is not None
                and response.request.url is not None
                and REPO_API_REGEX.search(response.request.url) is not None
            ):
                # 401 is misleading as it is returned for:
                #    - private and gated repos if user is not authenticated
                #    - missing repos
                # => for now, we process them as `RepoNotFound` anyway.
                # See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Repository Not Found for url: {response.url}."
                    + "\nPlease make sure you specified the correct `repo_id` and"
                    " `repo_type`.\nIf you are trying to access a private or gated repo,"
                    " make sure you are authenticated. For more details, see"
                    " https://huggingface.co/docs/huggingface_hub/authentication"
                )
                raise _format(RepositoryNotFoundError, message, response) from e

            elif response.status_code == 400:
                message = (
                    f"\n\nBad request for {endpoint_name} endpoint:" if endpoint_name is not None else "\n\nBad request:"
                )
                raise _format(BadRequestError, message, response) from e

            elif response.status_code == 403:
                message = (
                    f"\n\n{response.status_code} Forbidden: {error_message}."
                    + f"\nCannot access content at: {response.url}."
                    + "\nMake sure your token has the correct permissions."
                )
                raise _format(HfHubHTTPError, message, response) from e

            elif response.status_code == 416:
                range_header = response.request.headers.get("Range")
                message = f"{e}. Requested range: {range_header}. Content-Range: {response.headers.get('Content-Range')}."
                raise _format(HfHubHTTPError, message, response) from e

            # Convert `HTTPError` into a `HfHubHTTPError` to display request information
            # as well (request id and/or server error message)
>           raise _format(HfHubHTTPError, str(e), response) from e
E           huggingface_hub.errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/peft-internal-testing/tiny-random-BertModel/xet-read-token/08f3923b4fa5c5b5945392c81ee228b83acd623b (Request ID: Root=1-68e20f6d-2324928a57493df359f0107f;fe575371-c299-40a0-8f8a-92facc67bfe7)
E
E           We had to rate limit your IP (169.237.118.139). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API.

venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:480: HfHubHTTPError

The above exception was the direct cause of the following exception:

self = <tests.test_hub_features.TestBaseModelRevision object at 0x7f8dadd0fd90>
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/popen-gw0/test_save_and_load_base_model_0')

    def test_save_and_load_base_model_revision(self, tmp_path):
        r"""
        Test saving a PeftModel with a base model revision and loading with AutoPeftModel to recover the same base
        model
        """
        lora_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.0)
        test_inputs = torch.arange(10).reshape(-1, 1)

        base_model_id = "peft-internal-testing/tiny-random-BertModel"
        revision = "v2.0.0"

        base_model_revision = AutoModelForCausalLM.from_pretrained(base_model_id, revision=revision).eval()
        peft_model_revision = get_peft_model(base_model_revision, lora_config, revision=revision)
        output_revision = peft_model_revision(test_inputs).logits

        # sanity check: the model without revision should be different
>       base_model_no_revision = AutoModelForCausalLM.from_pretrained(base_model_id, revision="main").eval()
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_hub_features.py:86:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:604: in from_pretrained
    return model_class.from_pretrained(
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4903: in from_pretrained
    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:1041: in _get_resolved_checkpoint_files
    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/utils/hub.py:322: in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

path_or_repo_id = 'peft-internal-testing/tiny-random-BertModel'
filenames = ['model.safetensors'], cache_dir = '/root/.cache/huggingface/hub'
force_download = False, resume_download = None, proxies = None, token = None
revision = 'main', local_files_only = False, subfolder = '', repo_type = None
user_agent = 'transformers/4.57.0; python/3.11.0rc1; session_id/a3985ba321dc4338b6c4b346b90bfa42; torch/2.8.0; file_type/model; framework/pytorch; from_auto_class/False'
_raise_exceptions_for_gated_repo = False
_raise_exceptions_for_missing_entries = False
_raise_exceptions_for_connection_errors = True
_commit_hash = '08f3923b4fa5c5b5945392c81ee228b83acd623b'
deprecated_kwargs = {}, use_auth_token = None
full_filenames = ['model.safetensors'], existing_files = []
filename = 'model.safetensors', resolved_file = None, file_counter = 0

    def cached_files(
        path_or_repo_id: Union[str, os.PathLike],
        filenames: list[str],
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        force_download: bool = False,
        resume_download: Optional[bool] = None,
        proxies: Optional[dict[str, str]] = None,
        token: Optional[Union[bool, str]] = None,
        revision: Optional[str] = None,
        local_files_only: bool = False,
        subfolder: str = "",
        repo_type: Optional[str] = None,
        user_agent: Optional[Union[str, dict[str, str]]] = None,
        _raise_exceptions_for_gated_repo: bool = True,
        _raise_exceptions_for_missing_entries: bool = True,
        _raise_exceptions_for_connection_errors: bool = True,
        _commit_hash: Optional[str] = None,
        **deprecated_kwargs,
    ) -> Optional[str]:
        """
        Tries to locate several files in a local folder and repo, downloads and cache them if necessary.

        Args:
            path_or_repo_id (`str` or `os.PathLike`):
                This can be either:
                - a string, the *model id* of a model repo on huggingface.co.
                - a path to a *directory* potentially containing the file.
            filenames (`list[str]`):
                The name of all the files to locate in `path_or_repo`.
            cache_dir (`str` or `os.PathLike`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the standard
                cache should not be used.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force to (re-)download the configuration files and override the cached versions if they
                exist.
            resume_download:
                Deprecated and ignored. All downloads are now resumed by default when possible.
                Will be removed in v5 of Transformers.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.
            token (`str` or *bool*, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated
                when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.
            local_files_only (`bool`, *optional*, defaults to `False`):
                If `True`, will only try to load the tokenizer configuration from local files.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            repo_type (`str`, *optional*):
                Specify the repo type (useful when downloading from a space for instance).

        Private args:
            _raise_exceptions_for_gated_repo (`bool`):
                if False, do not raise an exception for gated repo error but return None.
            _raise_exceptions_for_missing_entries (`bool`):
                if False, do not raise an exception for missing entries but return None.
            _raise_exceptions_for_connection_errors (`bool`):
                if False, do not raise an exception for connection errors but return None.
            _commit_hash (`str`, *optional*):
                passed when we are chaining several calls to various files (e.g. when loading a tokenizer or
                a pipeline). If files are cached for this commit hash, avoid calls to head and get from the cache.

        <Tip>

        Passing `token=True` is required when you want to use a private model.

        </Tip>

        Returns:
            `Optional[str]`: Returns the resolved file (to the cache folder if downloaded from a repo).

        Examples:

        ```python
        # Download a model weight from the Hub and cache it.
        model_weights_file = cached_file("google-bert/bert-base-uncased", "pytorch_model.bin")
        ```
        """
        use_auth_token = deprecated_kwargs.pop("use_auth_token", None)
        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
            token = use_auth_token

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True
        if subfolder is None:
            subfolder = ""

        # Add folder to filenames
        full_filenames = [os.path.join(subfolder, file) for file in filenames]

        path_or_repo_id = str(path_or_repo_id)
        existing_files = []
        for filename in full_filenames:
            if os.path.isdir(path_or_repo_id):
                resolved_file = os.path.join(path_or_repo_id, filename)
                if not os.path.isfile(resolved_file):
                    if _raise_exceptions_for_missing_entries and filename != os.path.join(subfolder, "config.json"):
                        revision_ = "main" if revision is None else revision
                        raise OSError(
                            f"{path_or_repo_id} does not appear to have a file named {filename}. Checkout "
                            f"'https://huggingface.co/{path_or_repo_id}/tree/{revision_}' for available files."
                        )
                    else:
                        continue
                existing_files.append(resolved_file)

        if os.path.isdir(path_or_repo_id):
            return existing_files if existing_files else None

        if cache_dir is None:
            cache_dir = TRANSFORMERS_CACHE
        if isinstance(cache_dir, Path):
            cache_dir = str(cache_dir)

        existing_files = []
        file_counter = 0
        if _commit_hash is not None and not force_download:
            for filename in full_filenames:
                # If the file is cached under that commit hash, we return it directly.
                resolved_file = try_to_load_from_cache(
                    path_or_repo_id, filename, cache_dir=cache_dir, revision=_commit_hash, repo_type=repo_type
                )
                if resolved_file is not None:
                    if resolved_file is not _CACHED_NO_EXIST:
                        file_counter += 1
                        existing_files.append(resolved_file)
                    elif not _raise_exceptions_for_missing_entries:
                        file_counter += 1
                    else:
                        raise OSError(f"Could not locate {filename} inside {path_or_repo_id}.")

        # Either all the files were found, or some were _CACHED_NO_EXIST but we do not raise for missing entries
        if file_counter == len(full_filenames):
            return existing_files if len(existing_files) > 0 else None

        user_agent = http_user_agent(user_agent)
        # download the files if needed
        try:
            if len(full_filenames) == 1:
                # This is slightly better for only 1 file
                hf_hub_download(
                    path_or_repo_id,
                    filenames[0],
                    subfolder=None if len(subfolder) == 0 else subfolder,
                    repo_type=repo_type,
                    revision=revision,
                    cache_dir=cache_dir,
                    user_agent=user_agent,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    token=token,
                    local_files_only=local_files_only,
                )
            else:
                snapshot_download(
                    path_or_repo_id,
                    allow_patterns=full_filenames,
                    repo_type=repo_type,
                    revision=revision,
                    cache_dir=cache_dir,
                    user_agent=user_agent,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    token=token,
                    local_files_only=local_files_only,
                )

        except Exception as e:
            # We cannot recover from them
            if isinstance(e, RepositoryNotFoundError) and not isinstance(e, GatedRepoError):
                raise OSError(
                    f"{path_or_repo_id} is not a local folder and is not a valid model identifier "
                    "listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token "
                    "having permission to this repo either by logging in with `hf auth login` or by passing "
                    "`token=<your_token>`"
                ) from e
            elif isinstance(e, RevisionNotFoundError):
                raise OSError(
                    f"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists "
                    "for this model name. Check the model page at "
                    f"'https://huggingface.co/{path_or_repo_id}' for available revisions."
                ) from e
            elif isinstance(e, PermissionError):
                raise OSError(
                    f"PermissionError at {e.filename} when downloading {path_or_repo_id}. "
                    "Check cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); "
                    "2) a previous download was canceled and the lock file needs manual removal."
                ) from e

            # Now we try to recover if we can find all files correctly in the cache
            resolved_files = [
                _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
                for filename in full_filenames
            ]
            if all(file is not None for file in resolved_files):
                return resolved_files

            # Raise based on the flags. Note that we will raise for missing entries at the very end, even when
            # not entering this Except block, as it may also happen when `snapshot_download` does not raise
            if isinstance(e, GatedRepoError):
                if not _raise_exceptions_for_gated_repo:
                    return None
                raise OSError(
                    "You are trying to access a gated repo.\nMake sure to have access to it at "
                    f"https://huggingface.co/{path_or_repo_id}.\n{str(e)}"
                ) from e
            elif isinstance(e, LocalEntryNotFoundError):
                if not _raise_exceptions_for_connection_errors:
                    return None
                # Here we only raise if both flags for missing entry and connection errors are True (because it can be raised
                # even when `local_files_only` is True, in which case raising for connections errors only would not make sense)
                elif _raise_exceptions_for_missing_entries:
                    raise OSError(
                        f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load the files, and couldn't find them in the"
                        f" cached files.\nCheck your internet connection or see how to run the library in offline mode at"
                        " 'https://huggingface.co/docs/transformers/installation#offline-mode'."
                    ) from e
            # snapshot_download will not raise EntryNotFoundError, but hf_hub_download can. If this is the case, it will be treated
            # later on anyway and re-raised if needed
            elif isinstance(e, HTTPError) and not isinstance(e, EntryNotFoundError):
                if not _raise_exceptions_for_connection_errors:
                    return None
>               raise OSError(f"There was a specific connection error when trying to load {path_or_repo_id}:\n{e}") from e
E               OSError: There was a specific connection error when trying to load peft-internal-testing/tiny-random-BertModel:
E               429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/peft-internal-testing/tiny-random-BertModel/xet-read-token/08f3923b4fa5c5b5945392c81ee228b83acd623b (Request ID: Root=1-68e20f6d-2324928a57493df359f0107f;fe575371-c299-40a0-8f8a-92facc67bfe7)
E
E               We had to rate limit your IP (169.237.118.139). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API.

venv/lib/python3.11/site-packages/transformers/utils/hub.py:563: OSError
----------------------------- Captured stderr call -----------------------------
If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`
We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
_______________________ test_lora_plus_optimizer_sucess ________________________
[gw2] linux -- Python 3.11.0 /app/venv/bin/python

    @require_bitsandbytes
    def test_lora_plus_optimizer_sucess():
        """
        Test if the optimizer is correctly created and step function runs without any exception
        """
        optimizer_cls = bnb.optim.Adam8bit
        optim_config = {
            "eps": 1e-6,
            "betas": (0.9, 0.999),
            "loraplus_weight_decay": 0.0,
        }
        model: SimpleNet = SimpleNet().to(torch_device)
        optim = create_loraplus_optimizer(
            model=model,
            optimizer_cls=optimizer_cls,
            lr=5e-5,
            loraplus_lr_ratio=1.2,
            loraplus_lr_embedding=1e-6,
            **optim_config,
        )
        loss = torch.nn.CrossEntropyLoss()
        bnb.optim.GlobalOptimManager.get_instance().register_parameters(model.parameters())
        x = torch.randint(100, (2, 4, 10)).to(torch_device)
        output = model(x).permute(0, 3, 1, 2)
        label = torch.randint(16, (2, 4, 10)).to(torch_device)
        loss_value = loss(output, label)
        loss_value.backward()
>       optim.step()

tests/test_loraplus.py:99:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.11/site-packages/torch/optim/optimizer.py:516: in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:120: in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/bitsandbytes/optim/optimizer.py:291: in step
    self.update_step(group, p, gindex, pindex)
venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:120: in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/bitsandbytes/optim/optimizer.py:520: in update_step
    F.optimizer_update_32bit(
venv/lib/python3.11/site-packages/bitsandbytes/functional.py:1178: in optimizer_update_32bit
    is_on_gpu([g, p, state1, state2, unorm_vec])
venv/lib/python3.11/site-packages/bitsandbytes/functional.py:361: in is_on_gpu
    f"All input tensors need to be on the same GPU, but found some tensors to not be on a GPU:\n {[(t.shape, t.device) for t in tensors]}",
                                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

.0 = <list_iterator object at 0x7fe3df761c00>

>       f"All input tensors need to be on the same GPU, but found some tensors to not be on a GPU:\n {[(t.shape, t.device) for t in tensors]}",
                                                                                                        ^^^^^^^
    )
E   AttributeError: 'NoneType' object has no attribute 'shape'

venv/lib/python3.11/site-packages/bitsandbytes/functional.py:361: AttributeError
____ TestBaseModelRevision.test_load_different_peft_and_base_model_revision ____
[gw0] linux -- Python 3.11.0 /app/venv/bin/python

response = <Response [429]>, endpoint_name = None

    def hf_raise_for_status(response: Response, endpoint_name: Optional[str] = None) -> None:
        """
        Internal version of `response.raise_for_status()` that will refine a
        potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.

        This helper is meant to be the unique method to raise_for_status when making a call
        to the Hugging Face Hub.


        Example:
        ```py
            import requests
            from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError

            response = get_session().post(...)
            try:
                hf_raise_for_status(response)
            except HfHubHTTPError as e:
                print(str(e)) # formatted message
                e.request_id, e.server_message # details returned by server

                # Complete the error message with additional information once it's raised
                e.append_to_message("\n`create_commit` expects the repository to exist.")
                raise
        ```

        Args:
            response (`Response`):
                Response from the server.
            endpoint_name (`str`, *optional*):
                Name of the endpoint that has been called. If provided, the error message
                will be more complete.

        <Tip warning={true}>

        Raises when the request has failed:

            - [`~utils.RepositoryNotFoundError`]
                If the repository to download from cannot be found. This may be because it
                doesn't exist, because `repo_type` is not set correctly, or because the repo
                is `private` and you do not have access.
            - [`~utils.GatedRepoError`]
                If the repository exists but is gated and the user is not on the authorized
                list.
            - [`~utils.RevisionNotFoundError`]
                If the repository exists but the revision couldn't be find.
            - [`~utils.EntryNotFoundError`]
                If the repository exists but the entry (e.g. the requested file) couldn't be
                find.
            - [`~utils.BadRequestError`]
                If request failed with a HTTP 400 BadRequest error.
            - [`~utils.HfHubHTTPError`]
                If request failed for a reason not listed above.

        </Tip>
        """
        try:
>           response.raise_for_status()

venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:407:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <Response [429]>

    def raise_for_status(self):
        """Raises :class:`HTTPError`, if one occurred."""

        http_error_msg = ""
        if isinstance(self.reason, bytes):
            # We attempt to decode utf-8 first because some servers
            # choose to localize their reason strings. If the string
            # isn't utf-8, we fall back to iso-8859-1 for all other
            # encodings. (See PR #3538)
            try:
                reason = self.reason.decode("utf-8")
            except UnicodeDecodeError:
                reason = self.reason.decode("iso-8859-1")
        else:
            reason = self.reason

        if 400 <= self.status_code < 500:
            http_error_msg = (
                f"{self.status_code} Client Error: {reason} for url: {self.url}"
            )

        elif 500 <= self.status_code < 600:
            http_error_msg = (
                f"{self.status_code} Server Error: {reason} for url: {self.url}"
            )

        if http_error_msg:
>           raise HTTPError(http_error_msg, response=self)
E           requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/peft-internal-testing/tiny-random-BertModel-lora/xet-read-token/725b8056ecf78182ad5b8fae7e2e4aa0a614da95

venv/lib/python3.11/site-packages/requests/models.py:1026: HTTPError

The above exception was the direct cause of the following exception:

self = <tests.test_hub_features.TestBaseModelRevision object at 0x7f8dadd0ff50>
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/popen-gw0/test_load_different_peft_and_b0')

    @pytest.mark.xfail(reason="Test is flaky on CI", raises=ValueError)
    def test_load_different_peft_and_base_model_revision(self, tmp_path):
        r"""
        Test loading an AutoPeftModel from the hub where the base model revision and peft revision differ
        """
        base_model_id = "hf-internal-testing/tiny-random-BertModel"
        base_model_revision = None
        peft_model_id = "peft-internal-testing/tiny-random-BertModel-lora"
        peft_model_revision = "v1.2.3"

>       peft_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, revision=peft_model_revision).eval()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_hub_features.py:114:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src/peft/auto.py:142: in from_pretrained
    return cls._target_peft_class.from_pretrained(
src/peft/peft_model.py:557: in from_pretrained
    load_result = model.load_adapter(
src/peft/peft_model.py:1345: in load_adapter
    adapters_weights = load_peft_weights(
src/peft/utils/save_and_load.py:649: in load_peft_weights
    filename = hf_hub_download(
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1010: in hf_hub_download
    return _hf_hub_download_to_cache_dir(
venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1171: in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1723: in _download_to_tmp_and_move
    xet_get(
venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:594: in xet_get
    connection_info = refresh_xet_connection_info(file_data=xet_file_data, headers=headers)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/utils/_xet.py:116: in refresh_xet_connection_info
    return _fetch_xet_connection_info_with_url(file_data.refresh_route, headers)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/utils/_xet.py:187: in _fetch_xet_connection_info_with_url
    hf_raise_for_status(resp)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

response = <Response [429]>, endpoint_name = None

    def hf_raise_for_status(response: Response, endpoint_name: Optional[str] = None) -> None:
        """
        Internal version of `response.raise_for_status()` that will refine a
        potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.

        This helper is meant to be the unique method to raise_for_status when making a call
        to the Hugging Face Hub.


        Example:
        ```py
            import requests
            from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError

            response = get_session().post(...)
            try:
                hf_raise_for_status(response)
            except HfHubHTTPError as e:
                print(str(e)) # formatted message
                e.request_id, e.server_message # details returned by server

                # Complete the error message with additional information once it's raised
                e.append_to_message("\n`create_commit` expects the repository to exist.")
                raise
        ```

        Args:
            response (`Response`):
                Response from the server.
            endpoint_name (`str`, *optional*):
                Name of the endpoint that has been called. If provided, the error message
                will be more complete.

        <Tip warning={true}>

        Raises when the request has failed:

            - [`~utils.RepositoryNotFoundError`]
                If the repository to download from cannot be found. This may be because it
                doesn't exist, because `repo_type` is not set correctly, or because the repo
                is `private` and you do not have access.
            - [`~utils.GatedRepoError`]
                If the repository exists but is gated and the user is not on the authorized
                list.
            - [`~utils.RevisionNotFoundError`]
                If the repository exists but the revision couldn't be find.
            - [`~utils.EntryNotFoundError`]
                If the repository exists but the entry (e.g. the requested file) couldn't be
                find.
            - [`~utils.BadRequestError`]
                If request failed with a HTTP 400 BadRequest error.
            - [`~utils.HfHubHTTPError`]
                If request failed for a reason not listed above.

        </Tip>
        """
        try:
            response.raise_for_status()
        except HTTPError as e:
            error_code = response.headers.get("X-Error-Code")
            error_message = response.headers.get("X-Error-Message")

            if error_code == "RevisionNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Revision Not Found for url: {response.url}."
                raise _format(RevisionNotFoundError, message, response) from e

            elif error_code == "EntryNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Entry Not Found for url: {response.url}."
                raise _format(EntryNotFoundError, message, response) from e

            elif error_code == "GatedRepo":
                message = (
                    f"{response.status_code} Client Error." + "\n\n" + f"Cannot access gated repo for url {response.url}."
                )
                raise _format(GatedRepoError, message, response) from e

            elif error_message == "Access to this resource is disabled.":
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Cannot access repository for url {response.url}."
                    + "\n"
                    + "Access to this resource is disabled."
                )
                raise _format(DisabledRepoError, message, response) from e

            elif error_code == "RepoNotFound" or (
                response.status_code == 401
                and error_message != "Invalid credentials in Authorization header"
                and response.request is not None
                and response.request.url is not None
                and REPO_API_REGEX.search(response.request.url) is not None
            ):
                # 401 is misleading as it is returned for:
                #    - private and gated repos if user is not authenticated
                #    - missing repos
                # => for now, we process them as `RepoNotFound` anyway.
                # See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Repository Not Found for url: {response.url}."
                    + "\nPlease make sure you specified the correct `repo_id` and"
                    " `repo_type`.\nIf you are trying to access a private or gated repo,"
                    " make sure you are authenticated. For more details, see"
                    " https://huggingface.co/docs/huggingface_hub/authentication"
                )
                raise _format(RepositoryNotFoundError, message, response) from e

            elif response.status_code == 400:
                message = (
                    f"\n\nBad request for {endpoint_name} endpoint:" if endpoint_name is not None else "\n\nBad request:"
                )
                raise _format(BadRequestError, message, response) from e

            elif response.status_code == 403:
                message = (
                    f"\n\n{response.status_code} Forbidden: {error_message}."
                    + f"\nCannot access content at: {response.url}."
                    + "\nMake sure your token has the correct permissions."
                )
                raise _format(HfHubHTTPError, message, response) from e

            elif response.status_code == 416:
                range_header = response.request.headers.get("Range")
                message = f"{e}. Requested range: {range_header}. Content-Range: {response.headers.get('Content-Range')}."
                raise _format(HfHubHTTPError, message, response) from e

            # Convert `HTTPError` into a `HfHubHTTPError` to display request information
            # as well (request id and/or server error message)
>           raise _format(HfHubHTTPError, str(e), response) from e
E           huggingface_hub.errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/peft-internal-testing/tiny-random-BertModel-lora/xet-read-token/725b8056ecf78182ad5b8fae7e2e4aa0a614da95 (Request ID: Root=1-68e20f6f-3864c697690b89980638cb2a;ff732bdf-1a83-48f3-af0b-044779ee0f7b)
E
E           We had to rate limit your IP (169.237.118.139). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API.

venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:480: HfHubHTTPError
----------------------------- Captured stderr call -----------------------------
If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`
Some weights of BertLMHeadModel were not initialized from the model checkpoint at hf-internal-testing/tiny-random-BertModel and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
_ TestInjectAdapterFromStateDict.test_inject_from_state_dict_stable_diffusion __
[gw0] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_low_level_api.TestInjectAdapterFromStateDict object at 0x7f8dad78ce90>

    def test_inject_from_state_dict_stable_diffusion(self):
        # same test as above, but with stable diffusion model and only testing LoRA
        model_id = "hf-internal-testing/tiny-sd-pipe"
        config_text_encoder = LoraConfig(target_modules=["k_proj", "q_proj", "v_proj", "out_proj", "fc1", "fc2"])
        config_unet = LoraConfig(
            target_modules=[
                "proj_in",
                "proj_out",
                "to_k",
                "to_q",
                "to_v",
                "to_out.0",
                "ff.net.0.proj",
                "ff.net.2",
            ]
        )
        with hub_online_once(model_id):
>           pipe = StableDiffusionPipeline.from_pretrained(model_id)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_low_level_api.py:283:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'transformers.models.clip.modeling_clip.CLIPTextModel'>
pretrained_model_name_or_path = '/root/.cache/huggingface/hub/models--hf-internal-testing--tiny-sd-pipe/snapshots/39f7e1819d772fbb8dd7f2aa3da8e2fc5a9bd917/text_encoder'
config = CLIPTextConfig {
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dty...en_layers": 5,
  "pad_token_id": 1,
  "projection_dim": 32,
  "transformers_version": "4.57.0",
  "vocab_size": 1000
}

cache_dir = None, ignore_mismatched_sizes = False, force_download = False
local_files_only = False, token = None, revision = 'main'
use_safetensors = None, weights_only = True, model_args = ()
kwargs = {'offload_state_dict': None}, state_dict = None

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        **kwargs,
    ) -> SpecificPreTrainedModelType:
        r"""
        Instantiate a pretrained pytorch model from a pre-trained model configuration.

        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you should first set it back in training mode with `model.train()`.

        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come
        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
        task.

        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those
        weights are discarded.

        Parameters:
            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):
                Can be either:

                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
                    - A path to a *directory* containing model weights saved using
                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In
                      this case, `from_tf` should be set to `True` and a configuration object should be provided as
                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a
                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.
                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,
                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to
                      `True`.
                    - `None` if you are both providing the configuration and state dictionary (resp. with keyword
                      arguments `config` and `state_dict`).
            model_args (sequence of positional arguments, *optional*):
                All remaining positional arguments will be passed to the underlying model's `__init__` method.
            config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):
                Can be either:

                    - an instance of a class derived from [`PretrainedConfig`],
                    - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].

                Configuration for the model to use instead of an automatically loaded configuration. Configuration can
                be automatically loaded when:

                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained
                      model).
                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the
                      save directory.
                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a
                      configuration JSON file named *config.json* is found in the directory.
            state_dict (`dict[str, torch.Tensor]`, *optional*):
                A state dictionary to use instead of a state dictionary loaded from saved weights file.

                This option can be used if you want to create a model from a pretrained configuration but load your own
                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and
                [`~PreTrainedModel.from_pretrained`] is not a simpler option.
            cache_dir (`Union[str, os.PathLike]`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the
                standard cache should not be used.
            from_tf (`bool`, *optional*, defaults to `False`):
                Load the model weights from a TensorFlow checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            from_flax (`bool`, *optional*, defaults to `False`):
                Load the model weights from a Flax checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):
                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size
                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a
                checkpoint with 3 labels).
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
                cached versions if they exist.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
            output_loading_info(`bool`, *optional*, defaults to `False`):
                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.
            local_files_only(`bool`, *optional*, defaults to `False`):
                Whether or not to only look at local files (i.e., do not try to download the model).
            token (`str` or `bool`, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use
                the token generated when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.

                <Tip>

                To test a pull request you made on the Hub, you can pass `revision="refs/pr/<pr_number>"`.

                </Tip>
            attn_implementation (`str`, *optional*):
                The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)), or `"flash_attention_3"` (using [Dao-AILab/flash-attention/hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.

                Accept HF kernel references in the form:
                  <namespace>/<repo_name>[@<revision>][:<kernel_name>]

                - <namespace> and <repo_name> are any non-"/" and non-":" sequences.
                - "@<revision>" is optional (branch, tag, or commit-ish), e.g. "@main", "@v1.2.0", "@abc123".
                - ":<kernel_name>" is optional and selects a function inside the kernel repo.
                - Both options can appear together and in this order only: @revision first, then :kernel_name.
                - We intentionally allow a leading "<wrapper>|" prefix (e.g., "flash|...") because the code
                  strips it before loading; '|' is not excluded in the character classes here.

                Examples that match:
                  "org/model"
                  "org/model@main"
                  "org/model:custom_kernel"
                  "org/model@v1.2.3:custom_kernel"

            > Parameters for big model inference

            dtype (`str` or `torch.dtype`, *optional*):
                Override the default `torch_dtype` and load the model under a specific `dtype`. The different options
                are:

                1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified
                  `dtype`, ignoring the model's `config.dtype` if one exists. If not specified
                  - the model will get loaded in `torch.float` (fp32).

                2. `"auto"` - A `dtype` or `torch_dtype` entry in the `config.json` file of the model will be
                  attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in
                  the checkpoint that's of a floating point type and use that as `dtype`. This will load the model
                  using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how
                  the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.

                3. A string that is a valid `torch.dtype`. E.g. "float32" loads the model in `torch.float32`, "float16" loads in `torch.float16` etc.

                <Tip>

                For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or
                reach out to the authors and ask them to add this information to the model's card and to insert the
                `dtype` or `torch_dtype` entry in `config.json` on the hub.

                </Tip>

            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):
                A map that specifies where each submodule should go. It doesn't need to be refined to each
                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
                same device. If we only pass the device (*e.g.*, `"cpu"`, `"cuda:1"`, `"mps"`, or a GPU ordinal rank
                like `1`) on which the model will be allocated, the device map will map the entire model to this
                device. Passing `device_map = 0` means put the whole model on GPU 0.

                To have Accelerate compute the most optimized `device_map` automatically, set `device_map="auto"`. For
                more information about each option see [designing a device
                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).
            max_memory (`Dict`, *optional*):
                A dictionary device identifier to maximum memory if using `device_map`. Will default to the maximum memory available for each
                GPU and the available CPU RAM if unset.
            tp_plan (`str`, *optional*):
                A torch tensor parallel plan, see [here](https://pytorch.org/tutorials/intermediate/TP_tutorial.html). Currently, it only accepts
                `tp_plan="auto"` to use predefined plan based on the model. Note that if you use it, you should launch your script accordingly with
                `torchrun [args] script.py`. This will be much faster than using a `device_map`, but has limitations.
            tp_size (`str`, *optional*):
                A torch tensor parallel degree. If not provided would default to world size.
            device_mesh (`torch.distributed.DeviceMesh`, *optional*):
                A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.
                If provided, it has to contain dimension named `"tp"` in case it's > 1 dimensional, this dimension will be used for tensor parallelism
            offload_folder (`str` or `os.PathLike`, *optional*):
                If the `device_map` contains any value `"disk"`, the folder where we will offload weights.
            offload_buffers (`bool`, *optional*):
                Whether or not to offload the buffers with the model parameters.
            quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):
                A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g
                bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and
                `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes
                quantizations and not preferred. consider inserting all such arguments into quantization_config
                instead.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            variant (`str`, *optional*):
                If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is
                ignored when using `from_tf` or `from_flax`.
            use_safetensors (`bool`, *optional*, defaults to `None`):
                Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`
                is not installed, it will be set to `False`.
            weights_only (`bool`, *optional*, defaults to `True`):
                Indicates whether unpickler should be restricted to loading only tensors, primitive types,
                dictionaries and any types added via torch.serialization.add_safe_globals().
                When set to False, we can load wrapper tensor subclass weights.
            key_mapping (`dict[str, str], *optional*):
                A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers
                architecture, but was not converted accordingly.
            kwargs (remaining dictionary of keyword arguments, *optional*):
                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or
                automatically loaded:

                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the
                      underlying model's `__init__` method (we assume all relevant updates to the configuration have
                      already been done)
                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class
                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that
                      corresponds to a configuration attribute will be used to override said attribute with the
                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute
                      will be passed to the underlying model's `__init__` function.

        <Tip>

        Activate the special ["offline-mode"](https://huggingface.co/transformers/installation.html#offline-mode) to
        use this method in a firewalled environment.

        </Tip>

        Examples:

        ```python
        >>> from transformers import BertConfig, BertModel

        >>> # Download model and configuration from huggingface.co and cache.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased")
        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
        >>> model = BertModel.from_pretrained("./test/saved_model/")
        >>> # Update configuration during loading.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", output_attentions=True)
        >>> assert model.config.output_attentions == True
        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).
        >>> config = BertConfig.from_json_file("./tf_model/my_tf_model_config.json")
        >>> model = BertModel.from_pretrained("./tf_model/my_tf_checkpoint.ckpt.index", from_tf=True, config=config)
        >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", from_flax=True)
        ```
        """
        state_dict = kwargs.pop("state_dict", None)
        from_tf = kwargs.pop("from_tf", False)
        from_flax = kwargs.pop("from_flax", False)
        proxies = kwargs.pop("proxies", None)
        output_loading_info = kwargs.pop("output_loading_info", False)
        use_auth_token = kwargs.pop("use_auth_token", None)
        from_pipeline = kwargs.pop("_from_pipeline", None)
        from_auto_class = kwargs.pop("_from_auto", False)
        dtype = kwargs.pop("dtype", None)
        torch_dtype = kwargs.pop("torch_dtype", None)  # kept for BC
        device_map = kwargs.pop("device_map", None)
        max_memory = kwargs.pop("max_memory", None)
        offload_folder = kwargs.pop("offload_folder", None)
        offload_buffers = kwargs.pop("offload_buffers", False)
        load_in_8bit = kwargs.pop("load_in_8bit", False)
        load_in_4bit = kwargs.pop("load_in_4bit", False)
        quantization_config = kwargs.pop("quantization_config", None)
        subfolder = kwargs.pop("subfolder", "")
        commit_hash = kwargs.pop("_commit_hash", None)
        variant = kwargs.pop("variant", None)
        adapter_kwargs = kwargs.pop("adapter_kwargs", {})
        adapter_name = kwargs.pop("adapter_name", "default")
        generation_config = kwargs.pop("generation_config", None)
        gguf_file = kwargs.pop("gguf_file", None)
        tp_plan = kwargs.pop("tp_plan", None)
        tp_size = kwargs.pop("tp_size", None)
        distributed_config: DistributedConfig = kwargs.pop("distributed_config", None)
        device_mesh = kwargs.pop("device_mesh", None)
        trust_remote_code = kwargs.pop("trust_remote_code", None)
        use_kernels = kwargs.pop("use_kernels", False)

        key_mapping = kwargs.pop("key_mapping", None)
        # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model
        if key_mapping is None and any(
            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS
        ):
            key_mapping = cls._checkpoint_conversion_mapping

        if distributed_config is not None:
            tp_plan = "auto"

        # Not used anymore -- remove them from the kwargs
        _ = kwargs.pop("resume_download", None)
        _ = kwargs.pop("mirror", None)
        _ = kwargs.pop("_fast_init", True)
        _ = kwargs.pop("low_cpu_mem_usage", None)

        # For BC on torch_dtype argument
        if torch_dtype is not None:
            logger.warning_once("`torch_dtype` is deprecated! Use `dtype` instead!")
            # If both kwargs are provided, use `dtype`
            dtype = dtype if dtype is not None else torch_dtype

        if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):
            raise ValueError(
                "`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies."
            )
        if tp_size is not None and tp_plan is None:
            raise ValueError("tp_plan has to be set when tp_size is passed.")
        if tp_plan is not None and tp_plan != "auto":
            # TODO: we can relax this check when we support taking tp_plan from a json file, for example.
            raise ValueError(f"tp_plan supports 'auto' only for now but got {tp_plan}.")
        if tp_plan is not None and device_map is not None:
            raise ValueError(
                "`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization."
            )

        if device_map == "auto" and int(os.environ.get("WORLD_SIZE", "0")):
            logger.info(
                "You've set device_map=`auto` while triggering a distributed run with torchrun. This might lead to unexpected behavior. "
                "If your plan is to load the model on each device, you should set device_map={"
                ": PartialState().process_index} where PartialState comes from accelerate library"
            )

        # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple
        # `device_map` pointing to the correct device
        if tp_plan is not None:
            if device_mesh is None:
                tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(tp_plan, tp_size=tp_size)
            else:
                if device_mesh.ndim > 1:
                    if "tp" not in device_mesh.mesh_dim_names:
                        raise ValueError(
                            "When using `tp_plan` and n-d `device_mesh`, it must contain a 'tp' dimension. "
                            "Please provide a valid `device_mesh`."
                        )
                    device_mesh = device_mesh["tp"]
                tp_size = device_mesh.size()
                device_map = torch.device(f"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}")

            if tp_size is None:
                tp_size = torch.distributed.get_world_size()

        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError(
                    "`token` and `use_auth_token` are both specified. Please set only the argument `token`."
                )
            token = use_auth_token

        if token is not None and adapter_kwargs is not None and "token" not in adapter_kwargs:
            adapter_kwargs["token"] = token

        if gguf_file is not None and not is_accelerate_available():
            raise ValueError("accelerate is required when loading a GGUF file `pip install accelerate`.")

        if commit_hash is None:
            if not isinstance(config, PretrainedConfig):
                # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible
                resolved_config_file = cached_file(
                    pretrained_model_name_or_path,
                    CONFIG_NAME,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    token=token,
                    revision=revision,
                    subfolder=subfolder,
                    _raise_exceptions_for_gated_repo=False,
                    _raise_exceptions_for_missing_entries=False,
                    _raise_exceptions_for_connection_errors=False,
                )
                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)
            else:
                commit_hash = getattr(config, "_commit_hash", None)

        if is_peft_available():
            _adapter_model_path = adapter_kwargs.pop("_adapter_model_path", None)

            if _adapter_model_path is None:
                _adapter_model_path = find_adapter_config_file(
                    pretrained_model_name_or_path,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    _commit_hash=commit_hash,
                    **adapter_kwargs,
                )
            if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):
                with open(_adapter_model_path, "r", encoding="utf-8") as f:
                    _adapter_model_path = pretrained_model_name_or_path
                    pretrained_model_name_or_path = json.load(f)["base_model_name_or_path"]
        else:
            _adapter_model_path = None

        # Potentially detect context manager or global device, and use it (only if no device_map was provided)
        if device_map is None and not is_deepspeed_zero3_enabled():
            device_in_context = get_torch_context_manager_or_global_device()
            if device_in_context == torch.device("meta"):
                raise RuntimeError(
                    "You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\n"
                    "This is an anti-pattern as `from_pretrained` wants to load existing weights.\nIf you want to initialize an "
                    "empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`"
                )
            device_map = device_in_context

        # change device_map into a map if we passed an int, a str or a torch.device
        if isinstance(device_map, torch.device):
            device_map = {"": device_map}
        elif isinstance(device_map, str) and device_map not in ["auto", "balanced", "balanced_low_0", "sequential"]:
            try:
                device_map = {"": torch.device(device_map)}
            except RuntimeError:
                raise ValueError(
                    "When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or "
                    f"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}."
                )
        elif isinstance(device_map, int):
            if device_map < 0:
                raise ValueError(
                    "You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' "
                )
            else:
                device_map = {"": device_map}

        if device_map is not None:
            if is_deepspeed_zero3_enabled():
                raise ValueError("DeepSpeed Zero-3 is not compatible with passing a `device_map`.")
            if not is_accelerate_available():
                raise ValueError(
                    "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` "
                    "requires `accelerate`. You can install it with `pip install accelerate`"
                )

        # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.
        if load_in_4bit or load_in_8bit:
            if quantization_config is not None:
                raise ValueError(
                    "You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing "
                    "`quantization_config` argument at the same time."
                )

            # preparing BitsAndBytesConfig from kwargs
            config_dict = {k: v for k, v in kwargs.items() if k in inspect.signature(BitsAndBytesConfig).parameters}
            config_dict = {**config_dict, "load_in_4bit": load_in_4bit, "load_in_8bit": load_in_8bit}
            quantization_config, kwargs = BitsAndBytesConfig.from_dict(
                config_dict=config_dict, return_unused_kwargs=True, **kwargs
            )
            logger.warning(
                "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. "
                "Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead."
            )

        from_pt = not (from_tf | from_flax)

        user_agent = {"file_type": "model", "framework": "pytorch", "from_auto_class": from_auto_class}
        if from_pipeline is not None:
            user_agent["using_pipeline"] = from_pipeline

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True

        # Load config if we don't provide a configuration
        if not isinstance(config, PretrainedConfig):
            config_path = config if config is not None else pretrained_model_name_or_path
            config, model_kwargs = cls.config_class.from_pretrained(
                config_path,
                cache_dir=cache_dir,
                return_unused_kwargs=True,
                force_download=force_download,
                proxies=proxies,
                local_files_only=local_files_only,
                token=token,
                revision=revision,
                subfolder=subfolder,
                gguf_file=gguf_file,
                _from_auto=from_auto_class,
                _from_pipeline=from_pipeline,
                **kwargs,
            )
            if "gguf_file" in model_kwargs:
                model_kwargs.pop("gguf_file")
        else:
            config = copy.deepcopy(config)
            model_kwargs = kwargs

        # Because some composite configs call super().__init__ before instantiating the sub-configs, we need this call
        # to correctly redispatch recursively if the kwarg is provided
        if "attn_implementation" in kwargs:
            config._attn_implementation = kwargs.pop("attn_implementation")

        transformers_explicit_filename = getattr(config, "transformers_weights", None)

        if transformers_explicit_filename is not None:
            if not transformers_explicit_filename.endswith(
                ".safetensors"
            ) and not transformers_explicit_filename.endswith(".safetensors.index.json"):
                raise ValueError(
                    "The transformers file in the config seems to be incorrect: it is neither a safetensors file "
                    "(*.safetensors) nor a safetensors index file (*.safetensors.index.json): "
                    f"{transformers_explicit_filename}"
                )

        hf_quantizer, config, dtype, device_map = get_hf_quantizer(
            config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent
        )

        if gguf_file is not None and hf_quantizer is not None:
            raise ValueError(
                "You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub."
            )

        if (
            gguf_file
            and device_map is not None
            and ((isinstance(device_map, dict) and "disk" in device_map.values()) or "disk" in device_map)
        ):
            raise RuntimeError(
                "One or more modules is configured to be mapped to disk. Disk offload is not supported for models "
                "loaded from GGUF files."
            )

        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
            pretrained_model_name_or_path=pretrained_model_name_or_path,
            subfolder=subfolder,
            variant=variant,
            gguf_file=gguf_file,
            from_tf=from_tf,
            from_flax=from_flax,
            use_safetensors=use_safetensors,
            cache_dir=cache_dir,
            force_download=force_download,
            proxies=proxies,
            local_files_only=local_files_only,
            token=token,
            user_agent=user_agent,
            revision=revision,
            commit_hash=commit_hash,
            is_remote_code=cls._auto_class is not None,
            transformers_explicit_filename=transformers_explicit_filename,
        )

        is_sharded = sharded_metadata is not None
        is_quantized = hf_quantizer is not None
        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None

        if is_from_file and not is_sharded and checkpoint_files[0].endswith(".safetensors"):
            with safe_open(checkpoint_files[0], framework="pt") as f:
                metadata = f.metadata()

            if metadata is None:
                # Assume it's a pytorch checkpoint (introduced for timm checkpoints)
                pass
            elif metadata.get("format") == "pt":
                pass
            elif metadata.get("format") == "tf":
                from_tf = True
                logger.info("A TensorFlow safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "flax":
                from_flax = True
                logger.info("A Flax safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "mlx":
                # This is a mlx file, we assume weights are compatible with pt
                pass
            else:
                raise ValueError(
                    f"Incompatible safetensors file. File metadata is not ['pt', 'tf', 'flax', 'mlx'] but {metadata.get('format')}"
                )

        from_pt = not (from_tf | from_flax)

        if from_pt:
            if gguf_file:
                from .modeling_gguf_pytorch_utils import load_gguf_checkpoint

                # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was
                # passed directly as a kwarg from now on
                with torch.device("meta"):
                    dummy_model = cls(config)
                state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[
                    "tensors"
                ]

            # Find the correct dtype based on current state
            config, dtype, dtype_orig = _get_dtype(
                cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only
            )

        config.name_or_path = pretrained_model_name_or_path
        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)
        config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.
        with ContextManagers(model_init_context):
            # Let's make sure we don't run the init function of buffer modules
>           model = cls(config, *model_args, **model_kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'

venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: TypeError
----------------------------- Captured stderr call -----------------------------
Couldn't connect to the Hub: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/hf-internal-testing/tiny-sd-pipe (Request ID: Root=1-68e20f74-7fca1ce976f078ed18a3fda9;6c786237-40b4-4099-933d-13b4497f5242)

We had to rate limit your IP (169.237.118.139). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API..
Will try to load from local cache.
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  40%|████▍      | 2/5 [00:00<00:00, 216.02it/s]
=============================== warnings summary ===============================
src/peft/tuners/bone/config.py:126: 4 warnings
tests/test_custom_models.py: 241 warnings
tests/test_decoder_models.py: 166 warnings
tests/test_config.py: 16 warnings
tests/test_encoder_decoder_models.py: 38 warnings
tests/test_feature_extraction_models.py: 54 warnings
tests/test_seq_classifier.py: 27 warnings
  /app/src/peft/tuners/bone/config.py:126: UserWarning: Bone will be removed in v0.19.0 of PEFT, use `MissConfig` instead. If you already have a Bone checkpoint, you can use `/scripts/convert-bone-to-miss.py` to convert it into
    warnings.warn(

tests/test_custom_models.py: 52 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2dGroups' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 104 warnings
tests/test_initialization.py: 6 warnings
  /app/src/peft/tuners/lora/layer.py:1138: UserWarning: LoRA adapter added to ConvNd layer with groups > 1. Merging is not supported.
    warnings.warn("LoRA adapter added to ConvNd layer with groups > 1. Merging is not supported.")

tests/test_custom_models.py: 52 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2dGroups2' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 300 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv3d' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 3114 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MLP' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 811 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2d' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 63 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MHA' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 26 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MlpUsingParameters' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 26 warnings
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'tests.test_custom_models._LinearUsingParameter'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_custom_models.py: 385 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'EmbConv1D' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 160 warnings
tests/test_decoder_models.py: 100 warnings
tests/test_initialization.py: 4 warnings
tests/test_tuners_utils.py: 2 warnings
tests/test_helpers.py: 2 warnings
  /app/src/peft/tuners/lora/layer.py:2264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py: 87 warnings
tests/test_decoder_models.py: 20 warnings
  /app/src/peft/tuners/ia3/model.py:130: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py: 29 warnings
  /app/src/peft/tuners/ia3/model.py:122: UserWarning: fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. Setting fan_in_fan_out to False.
    warnings.warn(

tests/test_custom_models.py: 207 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv1d' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 116 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv1dBigger' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 58 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2d1x1' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 129 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MLP_LayerNorm' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 79 warnings
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'torch.nn.modules.normalization.LayerNorm'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_custom_models.py::TestPeftCustomModel::test_training_custom_models[Vanilla MLP 1 BOFT-MLP-BOFTConfig-config_kwargs137]
tests/test_custom_models.py::TestPeftCustomModel::test_save_pretrained_pickle[Vanilla MLP 1 BOFT-MLP-BOFTConfig-config_kwargs137]
tests/test_boft.py::TestBoft::test_boft_state_dict
tests/test_seq_classifier.py::TestSequenceClassificationModels::test_save_pretrained[BOFTConfig-config_kwargs1-hf-internal-testing/tiny-random-BertForSequenceClassification]
  /app/venv/lib/python3.11/site-packages/pkg_resources/_vendor/pyparsing.py:87: DeprecationWarning: module 'sre_constants' is deprecated
    import sre_constants

tests/test_custom_models.py::TestPeftCustomModel::test_training_custom_models[Vanilla MLP 1 BOFT-MLP-BOFTConfig-config_kwargs137]
tests/test_custom_models.py::TestPeftCustomModel::test_save_pretrained_pickle[Vanilla MLP 1 BOFT-MLP-BOFTConfig-config_kwargs137]
tests/test_boft.py::TestBoft::test_boft_state_dict
tests/test_seq_classifier.py::TestSequenceClassificationModels::test_save_pretrained[BOFTConfig-config_kwargs1-hf-internal-testing/tiny-random-BertForSequenceClassification]
  /app/src/peft/tuners/boft/layer.py:95: UserWarning: Failed to load the CUDA extension: Ninja is required to load C++ extensions (pip install ninja to get it), check if ninja is available.
    warnings.warn(f"Failed to load the CUDA extension: {e}, check if ninja is available.")

tests/test_custom_models.py: 353 warnings
tests/test_decoder_models.py: 160 warnings
tests/test_boft.py: 2 warnings
tests/test_encoder_decoder_models.py: 36 warnings
tests/test_feature_extraction_models.py: 46 warnings
tests/test_seq_classifier.py: 27 warnings
  /app/src/peft/tuners/boft/layer.py:96: UserWarning: Setting boft_n_butterfly_factor to 1 to speed up the finetuning process.
    warnings.warn("Setting boft_n_butterfly_factor to 1 to speed up the finetuning process.")

tests/test_custom_models.py: 351 warnings
tests/test_decoder_models.py: 160 warnings
tests/test_boft.py: 2 warnings
tests/test_encoder_decoder_models.py: 36 warnings
tests/test_feature_extraction_models.py: 46 warnings
tests/test_seq_classifier.py: 27 warnings
  /app/src/peft/tuners/boft/layer.py:95: UserWarning: Failed to load the CUDA extension: /root/.cache/torch_extensions/py311_cu128/fbd_cuda/fbd_cuda.so: cannot open shared object file: No such file or directory, check if ninja is available.
    warnings.warn(f"Failed to load the CUDA extension: {e}, check if ninja is available.")

tests/test_custom_models.py: 56 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MLP2' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 56 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2d2' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 27 warnings
tests/test_decoder_models.py: 17 warnings
  /app/src/peft/tuners/vera/model.py:275: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py: 30 warnings
tests/test_decoder_models.py: 19 warnings
  /app/src/peft/tuners/vblora/model.py:141: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py: 36 warnings
tests/test_config.py: 1 warning
tests/test_decoder_models.py: 24 warnings
tests/test_encoder_decoder_models.py: 6 warnings
tests/test_feature_extraction_models.py: 8 warnings
tests/test_seq_classifier.py: 6 warnings
tests/test_vision_models.py: 1 warning
  /app/src/peft/tuners/oft/config.py:206: UserWarning: The cayley-neumann parameterization has been slightly changed to be more numerically stable in PEFT 0.18.0. Please retrain your adapter weights with newer PEFT versions. Alternatively, downgrade PEFT to version 0.17.0 to use the old parameterization.
    warnings.warn(msg)

tests/test_custom_models.py: 14 warnings
  /app/src/peft/tuners/tuners_utils.py:1683: UserWarning: All adapters are already merged, nothing to do.
    warnings.warn("All adapters are already merged, nothing to do.")

tests/test_custom_models.py: 10 warnings
tests/test_tuners_utils.py: 2 warnings
  /app/src/peft/tuners/lora/layer.py:728: UserWarning: Already unmerged. Nothing to do.
    warnings.warn("Already unmerged. Nothing to do.")

tests/test_custom_models.py: 110 warnings
tests/test_decoder_models.py: 7 warnings
tests/test_encoder_decoder_models.py: 2 warnings
tests/test_feature_extraction_models.py: 4 warnings
  /app/src/peft/tuners/ia3/layer.py:142: UserWarning: Unmerge result can be inaccurate for (IA)^3.
    warnings.warn("Unmerge result can be inaccurate for (IA)^3.")

tests/test_custom_models.py: 60 warnings
  /app/src/peft/tuners/ia3/layer.py:268: UserWarning: Unmerge result can be inaccurate for (IA)^3.
    warnings.warn("Unmerge result can be inaccurate for (IA)^3.")

tests/test_custom_models.py: 160 warnings
tests/test_decoder_models.py: 112 warnings
tests/test_encoder_decoder_models.py: 22 warnings
tests/test_feature_extraction_models.py: 44 warnings
  /app/src/peft/tuners/tuners_utils.py:1350: UserWarning: Adapter delete_me was active which is now deleted. Setting active adapter to default.
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter[MHA 1 LoRA-MHA-LoraConfig-config_kwargs33]
tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter[MHA 2 LoRA-MHA-LoraConfig-config_kwargs34]
tests/test_custom_models.py::TestPeftCustomModel::test_delete_inactive_adapter[MHA 1 LoRA-MHA-LoraConfig-config_kwargs33]
tests/test_custom_models.py::TestPeftCustomModel::test_delete_inactive_adapter[MHA 2 LoRA-MHA-LoraConfig-config_kwargs34]
  /app/src/peft/tuners/lora/layer.py:1679: UserWarning: Already unmerged. Nothing to do.
    warnings.warn("Already unmerged. Nothing to do.")

tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_with_multiple_adapters_works
tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_modules_to_save
tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_trainable_token_indices
tests/test_mixed.py::TestMixedAdapterTypes::test_delete_adapter
  /app/src/peft/tuners/tuners_utils.py:1350: UserWarning: Adapter adapter0 was active which is now deleted. Setting active adapter to adapter1.
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_modules_to_save
tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_trainable_token_indices
  /app/src/peft/tuners/tuners_utils.py:1350: UserWarning: Adapter adapter1 was active which is now deleted. Setting active adapter to adapter2.
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_trainable_token_indices
  /app/src/peft/tuners/tuners_utils.py:1350: UserWarning: Adapter adapter0 was active which is now deleted. Setting active adapter to adapter2.
    warnings.warn(

tests/test_custom_models.py: 1 warning
tests/test_tuners_utils.py: 9 warnings
tests/test_mapping.py: 2 warnings
  /app/src/peft/tuners/tuners_utils.py:279: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_load_resized_embedding_ignore_mismatched_sizes
  /app/src/peft/utils/save_and_load.py:561: UserWarning: Some weights of PeftModel were not initialized from the model checkpoint and are being ignored because you passed `ignore_mismatched_sizes=True`: - base_model.model.emb.lora_embedding_A.default: found shape torch.Size([8, 100]) in the checkpoint and torch.Size([8, 105]) in the model instantiated.
    warnings.warn(msg)

tests/test_custom_models.py::TestPeftCustomModel::test_load_resized_embedding_ignore_mismatched_sizes
  /app/src/peft/peft_model.py:587: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.emb.lora_embedding_A.default'].
    warnings.warn(warn_message)

tests/test_decoder_models.py: 14 warnings
  /app/src/peft/tuners/adalora/model.py:211: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_decoder_models.py: 18 warnings
  /app/src/peft/tuners/fourierft/model.py:116: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py::TestDynamicDispatch::test_custom_lora_layer_used
tests/test_custom_models.py::TestDynamicDispatch::test_training_works
tests/test_custom_models.py::TestDynamicDispatch::test_saving_and_loading
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'tests.test_custom_models.TestDynamicDispatch.custom_module_cls.<locals>.MyModule'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_decoder_models.py: 57 warnings
tests/test_encoder_decoder_models.py: 17 warnings
tests/test_seq_classifier.py: 9 warnings
  /app/src/peft/tuners/oft/layer.py:452: UserWarning: Invalid `oft_block_size` (32)! Adjusted `oft_block_size` to (16).
    warnings.warn(

tests/test_decoder_models.py: 19 warnings
  /app/src/peft/tuners/oft/layer.py:452: UserWarning: Invalid `oft_block_size` (32)! Adjusted `oft_block_size` to (8).
    warnings.warn(

tests/test_config.py::TestPeftConfig::test_from_peft_type
  /app/src/peft/tuners/xlora/config.py:83: UserWarning: No value was provided for `hidden_size`. This will be set to 4096 by default, please ensure that this is correct.
    warnings.warn(

tests/test_config.py::TestPeftConfig::test_from_peft_type
  /app/src/peft/tuners/xlora/config.py:88: UserWarning: No value was provided for for `adapters`. This will be set to empty, please ensure that this is correct.
    warnings.warn(

tests/test_config.py::TestPeftConfig::test_from_peft_type
tests/test_cpt.py::test_model_initialization_text
tests/test_cpt.py::test_model_initialization_random
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-hf-internal-testing/tiny-random-gpt2]
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-hf-internal-testing/tiny-random-OPTForCausalLM]
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-hf-internal-testing/tiny-random-MistralForCausalLM]
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-peft-internal-testing/tiny-dummy-qwen2]
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-trl-internal-testing/tiny-random-LlamaForCausalLM]
  /app/src/peft/tuners/cpt/config.py:85: FutureWarning: CPTConfig only supports task_type = CAUSAL_LM, setting it automatically. This will raise an error starting from PEFT v0.18.0.
    warnings.warn(

tests/test_config.py: 24 warnings
  /app/src/peft/config.py:225: UserWarning: The configuration file contains a `runtime_config` key. This is ignored. Runtime configurations are only valid at runtime.
    warnings.warn(

tests/test_decoder_models.py: 135 warnings
tests/test_hub_features.py: 7 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-Gemma3ForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 135 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-OPTForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 135 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-BloomForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 135 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-gpt_neo - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 135 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-GPTJForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 135 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-GPTBigCodeForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 135 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in trl-internal-testing/tiny-random-LlamaForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 135 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in peft-internal-testing/tiny-dummy-qwen2 - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 87 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-GPT2LMHeadModel - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py::TestDecoderModels::test_prompt_tuning_text_prepare_for_training[PromptTuningConfig-config_kwargs15-trl-internal-testing/tiny-random-LlamaForCausalLM]
tests/test_decoder_models.py::TestDecoderModels::test_prefix_tuning_mistral
tests/test_vision_models.py::TestPastKV::test_past_kv
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute

tests/test_decoder_models.py::TestDecoderModels::test_prompt_tuning_text_prepare_for_training[PromptTuningConfig-config_kwargs15-trl-internal-testing/tiny-random-LlamaForCausalLM]
tests/test_decoder_models.py::TestDecoderModels::test_prefix_tuning_mistral
tests/test_vision_models.py::TestPastKV::test_past_kv
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute

tests/test_cpt.py: 2 warnings
tests/test_decoder_models.py: 16 warnings
  /app/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
    warnings.warn(warn_msg)

tests/test_decoder_models.py: 104 warnings
tests/test_multitask_prompt_tuning.py: 7 warnings
  /app/src/peft/peft_model.py:2104: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.
    warnings.warn("Position ids are not supported for parameter efficient tuning. Ignoring position ids.")

tests/test_decoder_models.py: 9 warnings
tests/test_encoder_decoder_models.py: 2 warnings
  /app/src/peft/tuners/adalora/config.py:96: UserWarning: Note that `r` is not used in AdaLora and will be ignored.If you intended to set the initial rank, use `init_r` instead.
    warnings.warn(

tests/test_decoder_models.py::TestDecoderModels::test_lora_layer_replication
  /app/tests/test_decoder_models.py:599: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
    layers[0].mlp.up_proj.base_layer.weight.data.storage().data_ptr()

tests/test_encoder_decoder_models.py: 112 warnings
tests/test_hub_features.py: 2 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-BartForConditionalGeneration - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_encoder_decoder_models.py: 113 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in ybelkada/tiny-random-T5ForConditionalGeneration-calibrated - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_feature_extraction_models.py: 78 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-BertModel - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_feature_extraction_models.py: 77 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-RobertaModel - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_feature_extraction_models.py: 76 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-DebertaModel - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_feature_extraction_models.py: 76 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-DebertaV2Model - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 18 warnings
  /app/src/peft/tuners/lora/variants.py:714: UserWarning: Cannot calculate aLoRA offsets when only inputs_embeds are provided. Disabling aLoRA for this forward pass.
    warnings.warn(

tests/test_initialization.py::TestLoraInitialization::test_lora_pissa_conversion_same_output_after_loading
tests/test_initialization.py::TestLoraInitialization::test_lora_pissa_conversion_same_output_after_loading_with_rank_pattern
tests/test_initialization.py::TestLoraInitialization::test_lora_pissa_conversion_same_output_after_loading_with_alpha_pattern
tests/test_initialization.py::TestLoraInitialization::test_lora_pissa_conversion_same_output_after_loading_with_rslora
  /app/src/peft/peft_model.py:1257: UserWarning: PiSSA changes the base weights of the model and should thus not be used with other adapters. Consider converting the PiSSA adapter into a normal LoRA adapter: https://github.com/huggingface/peft/tree/main/examples/pissa_finetuning#convert-pissa-to-lora
    warnings.warn(msg)

tests/test_initialization.py::TestLoraInitialization::test_pissa_rank_pattern_and_rslora_raises
tests/test_initialization.py::TestLoraInitialization::test_pissa_alpha_pattern_and_rslora_raises
tests/test_initialization.py::TestLoraInitialization::test_olora_rank_pattern_and_rslora_raises
tests/test_initialization.py::TestLoraInitialization::test_olora_alpha_pattern_and_rslora_raises
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_rank_pattern_and_rslora_raises[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_rank_pattern_and_rslora_raises[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_alpha_pattern_and_rslora_raises[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_alpha_pattern_and_rslora_raises[kpm]
  /app/src/peft/tuners/lora/config.py:762: UserWarning: Using Rank-Stabilized LoRA with rank_pattern/alpha_pattern and post-training conversion of modified base weights PiSSA/CorDA/OLoRA means that you won't be able to pass `path_initial_model_for_weight_conversion` to `save_pretrained` to restore the initial values of the base weights; if you intend to do this, please ensure not to use rslora or rank_pattern/alpha_pattern.
    warnings.warn(msg)

tests/test_initialization.py::TestLoraInitialization::test_olora_conversion_same_output_after_loading
tests/test_initialization.py::TestLoraInitialization::test_olora_conversion_same_output_after_loading_with_rank_pattern
tests/test_initialization.py::TestLoraInitialization::test_olora_conversion_same_output_after_loading_with_alpha_pattern
tests/test_initialization.py::TestLoraInitialization::test_olora_conversion_same_output_after_loading_with_rslora
  /app/src/peft/peft_model.py:1271: UserWarning: OLoRA changes the base weights of the model and should thus not be used with other adapters. Consider converting the OLoRA adapter into a normal LoRA adapter: https://github.com/huggingface/peft/tree/main/examples/olora_finetuning#olora-and-lora
    warnings.warn(msg)

tests/test_initialization.py::TestLoraInitialization::test_lora_with_bias_incompatible_arguments[extra_kwargs1]
  /app/src/peft/tuners/lora/config.py:718: UserWarning: `init_lora_weights` is 'eva' but `eva_config` is not specified. Using default EVA config.
    warnings.warn("`init_lora_weights` is 'eva' but `eva_config` is not specified. Using default EVA config.")

tests/test_initialization.py::TestVeraInitialization::test_vera_mixing_save_projection_raises
tests/test_vera.py::TestVera::test_multiple_adapters_save_load_save_projection_false
tests/test_vera.py::TestVera::test_multiple_adapters_save_projection_false_contains_no_vera_A_vera_B
  /app/src/peft/tuners/vera/config.py:158: UserWarning: Specified to not save vera_A and vera_B within the state dictionary, instead they will be restored using the PRNG key store in `config.projection_prng_key`. Consider setting `config.save_projection` to `True` to guarantee restoring the checkpoint correctly on all system configurations.
    warnings.warn(

tests/test_other.py::TestTargetingAuxiliaryTrainingWrapper::test_targeting_trainable_tokens_raises
tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/tuners/tuners_utils.py:899: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
    warnings.warn(

tests/test_seq_classifier.py: 90 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-BertForSequenceClassification - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_seq_classifier.py: 90 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-RobertaForSequenceClassification - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_seq_classifier.py: 90 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in trl-internal-testing/tiny-LlamaForSequenceClassification-3.2 - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rank_pattern[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rank_pattern[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_alpha_pattern[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_alpha_pattern[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rslora[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rslora[kpm]
  /app/src/peft/tuners/lora/config.py:729: UserWarning: `corda_config` specified but will be ignored when `init_lora_weights` is not 'corda'.
    warnings.warn("`corda_config` specified but will be ignored when `init_lora_weights` is not 'corda'.")

tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rank_pattern[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rank_pattern[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_alpha_pattern[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_alpha_pattern[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rslora[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rslora[kpm]
  /app/src/peft/peft_model.py:1264: UserWarning: CorDA changes the base weights of the model and should thus not be used with other adapters. Consider converting the CorDA adapter into a normal LoRA adapter: https://github.com/huggingface/peft/tree/main/examples/corda_finetuning#convert-corda-to-lora
    warnings.warn(msg)

tests/test_randlora.py::TestRandLora::test_multiple_adapters_save_load_save_projection_false
tests/test_randlora.py::TestRandLora::test_multiple_adapters_save_projection_false_contains_no_randlora_A_randlora_B
  /app/src/peft/tuners/randlora/config.py:195: UserWarning: Specified to not save basis_A and basis_B within the state dictionary, instead they will be restored using the PRNG key store in `config.projection_prng_key`. Consider setting `config.save_projection` to `True` to guarantee restoring the checkpoint correctly on all system configurations.
    warnings.warn(

tests/test_initialization.py::TestEvaInitialization::test_eva_state_dict_adjust_scaling_factors[eva_config0]
tests/test_initialization.py::TestEvaInitialization::test_load_eva_state_dict[True]
tests/test_initialization.py::TestEvaInitialization::test_load_eva_state_dict[False]
tests/test_initialization.py::TestEvaInitialization::test_missing_eva_inits
tests/test_initialization.py::TestEvaInitialization::test_load_eva_model
  /app/src/peft/mapping_func.py:96: UserWarning: lora with eva initialization used with low_cpu_mem_usage=False. Setting low_cpu_mem_usage=True can improve the maximum batch size possible for eva initialization.
    warnings.warn(

tests/test_target_parameters.py: 81 warnings
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'transformers.models.llama4.modeling_llama4.Llama4TextExperts'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_target_parameters.py: 19 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in trl-internal-testing/tiny-Llama4ForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_target_parameters.py: 80 warnings
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssExperts'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_target_parameters.py: 19 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in trl-internal-testing/tiny-GptOssForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_shira.py::TestShira::test_mlp_single_adapter_shapes
  /app/venv/lib/python3.11/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_shira.py::TestShira::test_mlp_single_adapter_shapes returned <class 'peft.peft_model.PeftModel'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_shira.py::TestShira::test_multiple_adapters_save_load
  /app/venv/lib/python3.11/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_shira.py::TestShira::test_multiple_adapters_save_load returned <class 'peft.peft_model.PeftModel'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_shira.py::TestShira::test_save_load_custom_mask_function
  /app/src/peft/tuners/shira/config.py:126: UserWarning: Argument self.mask_type='custom' is not recognized, please supply your own masking function by calling `config.mask_fn = my_mask_fn`.
    warnings.warn(

tests/test_shira.py::TestShira::test_save_load_custom_mask_function
  /app/venv/lib/python3.11/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_shira.py::TestShira::test_save_load_custom_mask_function returned <class 'peft.peft_model.PeftModel'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_shira.py::TestShira::test_save_load_default_random_mask_with_seed_function
  /app/venv/lib/python3.11/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_shira.py::TestShira::test_save_load_default_random_mask_with_seed_function returned <class 'peft.peft_model.PeftModel'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_initialization.py::TestHotSwapping::test_prepare_model_for_compiled_hotswap_lora_bias
  /app/src/peft/tuners/lora/layer.py:170: PeftWarning: `lora_bias=True` was passed but the targeted layer of type Linear has no bias. This means that merging LoRA weights won't be possible.
    warnings.warn(

tests/test_trainable_tokens.py::TestTrainableTokens::test_save_pretrained_auto[peft_config0-True]
tests/test_trainable_tokens.py::TestTrainableTokens::test_save_pretrained_auto[peft_config1-True]
  /app/src/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
    warnings.warn(

tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_0_HuggingFaceH4_tiny_random_LlamaForCausalLM
tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_1_HuggingFaceH4_tiny_random_LlamaForCausalLM
tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_2_hf_internal_testing_tiny_random_gpt2
tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_3_hf_internal_testing_tiny_random_t5
tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_4_hf_internal_testing_tiny_random_GPTNeoXForCausalLM
  /app/src/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
    warnings.warn(

tests/test_tuners_utils.py::TestModelAndLayerStatus::test_model_merged_adapters_large
  /app/src/peft/tuners/lora/layer.py:984: UserWarning: Already unmerged. Nothing to do.
    warnings.warn("Already unmerged. Nothing to do.")

tests/test_tuners_utils.py::TestModelAndLayerStatus::test_model_merged_adapters_large
  /app/src/peft/tuners/lora/layer.py:1317: UserWarning: Already unmerged. Nothing to do.
    warnings.warn("Already unmerged. Nothing to do.")

tests/test_vera.py::TestVera::test_multiple_adapters_save_load_save_projection_false
  /app/src/peft/utils/save_and_load.py:505: UserWarning: Specified to not load vera_A and vera_B from state dictionary. This means we will be relying on PRNG initialisation to restore these projections using `config.projection_prng_key`, which may not be accurate on all system configurations.
    warnings.warn(

tests/test_tuners_utils.py::TestModelAndLayerStatus::test_merge_adapters_large
  /app/src/peft/tuners/tuners_utils.py:1678: UserWarning: Already following adapters were merged other. You are now additionally merging default.
    warnings.warn(

tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/utils/save_and_load.py:279: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.
    warnings.warn("Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.")

tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/tuners/tuners_utils.py:899: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
    warnings.warn(

tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/tuners/tuners_utils.py:899: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
    warnings.warn(

tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/tuners/tuners_utils.py:899: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens', 'model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
    warnings.warn(

tests/test_lora_variants.py::TestLoraVariants::test_variant_is_applied_to_layers[alora-LoraConfig-config_kwargs1]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets[input_ids0-alora_invocation_tokens0-expected_offsets0]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets[input_ids1-alora_invocation_tokens1-expected_offsets1]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets[input_ids2-alora_invocation_tokens2-expected_offsets2]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets_with_adapter_names[input_ids0-alora_invocations0-expected_offsets0]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets_with_adapter_names[input_ids1-alora_invocations1-expected_offsets1]
tests/test_lora_variants.py::TestActivatedLora::test_alora_activation_matches_base_until_invocation
tests/test_lora_variants.py::TestActivatedLora::test_input_embeds_warning
tests/test_lora_variants.py::TestActivatedLora::test_num_beams_error
  /app/src/peft/tuners/lora/config.py:741: UserWarning: aLoRA is currently only supported for CAUSAL_LM task.
    warnings.warn("aLoRA is currently only supported for CAUSAL_LM task.")

tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_transformers
tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_transformers_irregular_targets
tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_transformers_target_parameters_fails
tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_transformers_target_parameters_raises
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in facebook/opt-125m - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_extra_keys_warning
  /app/src/peft/tuners/tuners_utils.py:877: UserWarning: You have passed exclude_modules=['model.decoder.layers.5.self_attn.q_proj'] but no modules were excluded. Please check that exclude_modules was set correctly.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
_____________ coverage: platform linux, python 3.11.0-candidate-1 ______________

___________________________ coverage: failed workers ___________________________

The following workers failed to return coverage data, ensure that pytest-cov is installed on these workers.
gw1
Name                                                  Stmts   Miss  Cover   Missing
-----------------------------------------------------------------------------------
src/peft/__init__.py                                     10      0   100%
src/peft/auto.py                                         71      4    94%   61, 92, 99, 111
src/peft/config.py                                      133      5    96%   89, 156, 242, 267-268
src/peft/functional.py                                    4      4     0%   21-26
src/peft/helpers.py                                      72     21    71%   48-58, 84-98, 124-132, 235
src/peft/import_utils.py                                 94     41    56%   31, 41-46, 55-73, 87-98, 131-147, 158, 161-166
src/peft/mapping.py                                      18      3    83%   44, 78, 81
src/peft/mapping_func.py                                 36      0   100%
src/peft/mixed_model.py                                 151     24    84%   48, 51-54, 116, 124, 141, 147, 167-169, 239-242, 280, 297, 378, 387, 442-445, 449, 454, 458
src/peft/optimizers/__init__.py                           3      0   100%
src/peft/optimizers/lorafa.py                           105     25    76%   67, 69, 71, 73, 93, 113, 127-131, 157, 176, 182-210
src/peft/optimizers/loraplus.py                          36      3    92%   72, 76, 79
src/peft/peft_model.py                                 1342    430    68%   167-170, 176, 223, 232, 249, 264, 304, 328-331, 454, 474, 477-504, 509, 512, 516-538, 582, 635, 659, 684, 703-704, 751-752, 760, 770-772, 796, 811, 845, 851-857, 876-878, 913-915, 923, 960, 1008, 1055, 1076, 1174-1241, 1246, 1376-1423, 1455, 1462, 1479, 1532, 1537-1539, 1543-1546, 1552, 1678-1730, 1743-1796, 1869-1871, 1882, 1904-1905, 1907-1908, 1925, 1951, 2044, 2052, 2062-2067, 2076-2098, 2108-2111, 2122, 2239-2243, 2246-2247, 2249-2250, 2293-2323, 2340, 2342-2345, 2347-2350, 2362-2363, 2384-2390, 2402, 2480-2481, 2513-2520, 2534-2587, 2600-2636, 2701-2702, 2734-2741, 2758-2814, 2828-2879, 2945, 2966-2967, 2969-2970, 3071, 3216
src/peft/tuners/__init__.py                              29      0   100%
src/peft/tuners/_buffer_dict.py                          62      9    85%   76, 83, 92-94, 123, 137, 141, 159
src/peft/tuners/adalora/__init__.py                      16      7    56%   33-43
src/peft/tuners/adalora/bnb.py                           74     60    19%   38-44, 48-75, 78-79, 96-102, 106-139, 142-143
src/peft/tuners/adalora/config.py                        36      2    94%   88, 92
src/peft/tuners/adalora/gptq.py                          31     26    16%   30-37, 40-67
src/peft/tuners/adalora/layer.py                        219     94    57%   31, 53, 58, 126, 142, 152-153, 169, 217, 236-252, 256-273, 278, 281-283, 286-335, 339-347, 351-360
src/peft/tuners/adalora/model.py                        152     76    50%   78, 104, 129, 136, 168, 181-188, 190-198, 200, 204-208, 217, 230-252, 256-284, 287-300, 323-342, 346
src/peft/tuners/adaption_prompt/__init__.py               6      0   100%
src/peft/tuners/adaption_prompt/config.py                25      1    96%   81
src/peft/tuners/adaption_prompt/layer.py                 86      5    94%   39, 54, 171, 185, 192
src/peft/tuners/adaption_prompt/model.py                 85      5    94%   64, 72, 98, 100, 168
src/peft/tuners/adaption_prompt/utils.py                 68     34    50%   47-58, 73, 84-89, 100-128, 141-146
src/peft/tuners/boft/__init__.py                          6      0   100%
src/peft/tuners/boft/config.py                           30      3    90%   152, 154, 158
src/peft/tuners/boft/fbd/__init__.py                      0      0   100%
src/peft/tuners/boft/layer.py                           495    100    80%   65-71, 78, 130-132, 136-138, 168, 234, 240-244, 247-254, 257-261, 283, 288, 301, 306-311, 319, 324-329, 336, 342-346, 388, 422-446, 511, 528, 550-551, 583, 595, 614, 625, 639, 713, 718, 724, 736, 741-746, 754, 759-764, 771, 777-781, 824, 872-873, 916, 928, 951, 962, 975
src/peft/tuners/boft/model.py                            35      5    86%   78, 111, 117-121, 126
src/peft/tuners/bone/__init__.py                          6      0   100%
src/peft/tuners/bone/config.py                           26      2    92%   120, 124
src/peft/tuners/bone/layer.py                           186     31    83%   46, 64, 74, 79, 99-106, 109-113, 151, 169, 188-189, 273-295, 327, 338, 342-343
src/peft/tuners/bone/model.py                            29      3    90%   89, 115, 122
src/peft/tuners/c3a/__init__.py                           6      0   100%
src/peft/tuners/c3a/config.py                            23      2    91%   133, 137
src/peft/tuners/c3a/layer.py                            112     14    88%   47, 51, 61, 97, 103-108, 142, 155, 171-172, 192
src/peft/tuners/c3a/model.py                             32      2    94%   63, 90
src/peft/tuners/c3a/utils.py                             30      0   100%
src/peft/tuners/cpt/__init__.py                           5      0   100%
src/peft/tuners/cpt/config.py                            35      1    97%   106
src/peft/tuners/cpt/model.py                             84      0   100%
src/peft/tuners/fourierft/__init__.py                     6      0   100%
src/peft/tuners/fourierft/config.py                      30      3    90%   199, 203, 206
src/peft/tuners/fourierft/layer.py                      104      7    93%   52, 58, 60, 133, 159-160, 182
src/peft/tuners/fourierft/model.py                       47      5    89%   67, 102, 108-112, 121
src/peft/tuners/hra/__init__.py                           6      0   100%
src/peft/tuners/hra/config.py                            27      3    89%   125, 129, 133
src/peft/tuners/hra/layer.py                            248     43    83%   50, 70, 85, 99-100, 111-118, 121-125, 163, 177, 193-194, 213-220, 250, 262, 306, 330, 358-359, 385-392, 424, 447
src/peft/tuners/hra/model.py                             31      3    90%   89, 117, 126
src/peft/tuners/ia3/__init__.py                          15      1    93%   39
src/peft/tuners/ia3/bnb.py                               67     53    21%   36-42, 46-69, 72-73, 88-94, 98-125, 128-129
src/peft/tuners/ia3/config.py                            22      0   100%
src/peft/tuners/ia3/layer.py                            194     13    93%   44, 50, 108, 139-140, 169, 232, 246, 265-266, 298, 322, 330
src/peft/tuners/ia3/model.py                            120     20    83%   92, 97-105, 107-115, 138, 192, 198, 219, 222, 234, 241, 247, 251, 256, 284, 307, 314
src/peft/tuners/ln_tuning/__init__.py                     5      0   100%
src/peft/tuners/ln_tuning/config.py                      13      0   100%
src/peft/tuners/ln_tuning/layer.py                       64      8    88%   73, 76, 82-83, 93-94, 106, 112
src/peft/tuners/ln_tuning/model.py                       44      1    98%   102
src/peft/tuners/loha/__init__.py                          6      0   100%
src/peft/tuners/loha/config.py                           25      1    96%   143
src/peft/tuners/loha/layer.py                           207      2    99%   126, 166
src/peft/tuners/loha/model.py                            22      0   100%
src/peft/tuners/lokr/__init__.py                          6      0   100%
src/peft/tuners/lokr/config.py                           28      1    96%   155
src/peft/tuners/lokr/layer.py                           230     15    93%   82-88, 146-147, 156, 186, 239, 286, 479-481
src/peft/tuners/lokr/model.py                            23      0   100%
src/peft/tuners/lora/__init__.py                         21      2    90%   60-62
src/peft/tuners/lora/aqlm.py                             48     31    35%   25, 42-49, 62-85, 88-89, 111-112
src/peft/tuners/lora/arrow.py                           210     16    92%   177, 212, 217, 316, 318, 340-341, 346-347, 389, 400, 411, 420, 434, 444, 450
src/peft/tuners/lora/awq.py                              55     37    33%   39-50, 62-85, 88-89, 105-119
src/peft/tuners/lora/bnb.py                             316    257    19%   74-76, 81-86, 101-146, 152-182, 185, 198-246, 249-293, 296-297, 340-345, 360-373, 388-432, 438-467, 470, 483-531, 534-585, 588-589, 601-609
src/peft/tuners/lora/config.py                          134     11    92%   117, 119, 707, 709, 714-715, 724-727, 781-783
src/peft/tuners/lora/corda.py                           173     17    90%   51, 111, 168, 181, 186, 246, 262, 284, 293, 298, 303, 333, 337, 342, 347, 352, 357
src/peft/tuners/lora/dora.py                            107      4    96%   48, 63, 97, 164
src/peft/tuners/lora/eetq.py                             55     42    24%   24-96, 112-116
src/peft/tuners/lora/eva.py                             333     55    83%   63, 67, 82-102, 132-135, 156-157, 164-165, 221-222, 234-238, 250, 253, 273, 277, 310-313, 322, 332-334, 383-385, 392, 434-436, 454, 461, 470, 544, 632, 713, 718, 722, 727
src/peft/tuners/lora/gptq.py                             65     43    34%   42-52, 66-80, 84-114, 117-118, 142-146, 151-152
src/peft/tuners/lora/hqq.py                             132    115    13%   30-237, 249
src/peft/tuners/lora/inc.py                              23     10    57%   31-59, 71-76
src/peft/tuners/lora/layer.py                          1182    139    88%   58, 63, 91, 167, 220-221, 250, 260, 271, 281, 285, 299-306, 308-313, 322, 341, 358, 368, 380, 384, 388, 394, 400, 406, 428-446, 479, 492, 505, 522-526, 531-532, 539-540, 696, 701, 715, 892, 901, 923, 951, 967, 1036-1059, 1083, 1091, 1136, 1181, 1197, 1225, 1256, 1265, 1278, 1286, 1291, 1305, 1360, 1367, 1389, 1399, 1431, 1448, 1465, 1516, 1534, 1619, 1635, 1643, 1679-1680, 1771, 1789-1790, 1870-1873, 1913, 1920, 1922, 1924, 1926, 1928, 1965, 1971, 1978, 1991, 2001-2002, 2004-2005, 2007-2008, 2010-2011, 2013, 2015-2016, 2023, 2042, 2047, 2075-2076, 2081, 2085, 2104, 2126, 2136, 2166-2167, 2179, 2203, 2255-2259
src/peft/tuners/lora/model.py                           346     60    83%   170, 221, 243, 270, 272, 291-306, 341, 372, 385, 393, 428, 430, 437, 449, 453, 467, 497, 501, 503, 509, 515, 573, 601-605, 615-619, 624, 680, 694, 698-702, 713-717, 719-720, 740-744, 763, 779
src/peft/tuners/lora/torchao.py                          80     57    29%   35-40, 44-54, 57-91, 94-124, 127-128, 150-156
src/peft/tuners/lora/tp_layer.py                        169    140    17%   56-99, 117-186, 189-216, 231-256, 262-270, 280-304, 307-308, 325, 333-346
src/peft/tuners/lora/variants.py                        396     67    83%   55, 121, 129, 144-152, 229-230, 248, 338, 407-408, 454-486, 490, 494, 498, 502, 512-541, 551, 555, 559, 580, 585, 621, 637-639, 694-695, 726, 750
src/peft/tuners/lycoris_utils.py                        104     27    74%   98-101, 130, 140, 153-156, 159-166, 173-174, 181-188, 241-242, 257-258
src/peft/tuners/miss/__init__.py                          6      0   100%
src/peft/tuners/miss/config.py                           26      2    92%   136, 140
src/peft/tuners/miss/layer.py                           214     43    80%   50, 70, 75, 86, 91, 94-100, 116, 122-129, 132-136, 176, 190-191, 197, 207-208, 219-220, 231, 307, 310-332, 364, 375, 378, 383-384
src/peft/tuners/miss/model.py                            29      3    90%   89, 119, 126
src/peft/tuners/mixed/__init__.py                         2      0   100%
src/peft/tuners/mixed/model.py                          166     40    76%   82, 102-107, 119, 127-131, 150-160, 167, 172, 182-187, 195-196, 204, 220, 249-255, 260, 270, 276
src/peft/tuners/multitask_prompt_tuning/__init__.py       5      0   100%
src/peft/tuners/multitask_prompt_tuning/config.py        21      0   100%
src/peft/tuners/multitask_prompt_tuning/model.py         48      4    92%   38, 65, 71-73
src/peft/tuners/oft/__init__.py                          19     10    47%   37-52
src/peft/tuners/oft/aqlm.py                              41     25    39%   25, 45-49, 64-82, 85-86, 97, 102-103
src/peft/tuners/oft/awq.py                               49     32    35%   42-50, 64-83, 86-87, 98, 103-117
src/peft/tuners/oft/bnb.py                              188    149    21%   48-53, 79-115, 121-150, 153, 156-179, 182-183, 189, 195-203, 227-232, 258-293, 299-326, 329, 332-362, 365-366, 372, 378-386
src/peft/tuners/oft/config.py                            48      4    92%   174, 176, 180, 194
src/peft/tuners/oft/eetq.py                              48     36    25%   24-94, 105, 110-114
src/peft/tuners/oft/gptq.py                              49     30    39%   41-48, 63-84, 87-88, 99, 106-110, 115-116
src/peft/tuners/oft/hqq.py                               94     79    16%   29-172, 179, 184
src/peft/tuners/oft/inc.py                               23     11    52%   31-59, 66, 71-76
src/peft/tuners/oft/layer.py                            430     82    81%   52-69, 199, 224, 255, 350-374, 386-390, 393-400, 403-407, 458-460, 463, 506, 513-515, 519-526, 585, 600, 621-622, 661, 736, 744, 749-755, 758-760, 763, 810, 856-857, 906, 931, 939-943
src/peft/tuners/oft/model.py                             54      5    91%   101, 122, 183, 197, 199
src/peft/tuners/p_tuning/__init__.py                      5      0   100%
src/peft/tuners/p_tuning/config.py                       17      0   100%
src/peft/tuners/p_tuning/model.py                        34      7    79%   84-96, 119, 124, 128
src/peft/tuners/poly/__init__.py                          6      0   100%
src/peft/tuners/poly/config.py                           21      0   100%
src/peft/tuners/poly/layer.py                            89      9    90%   49, 56, 103-108, 137
src/peft/tuners/poly/model.py                            52      5    90%   43, 52, 58, 65, 73
src/peft/tuners/poly/router.py                           34      3    91%   31, 64, 66
src/peft/tuners/prefix_tuning/__init__.py                 5      0   100%
src/peft/tuners/prefix_tuning/config.py                  10      0   100%
src/peft/tuners/prefix_tuning/model.py                   19      4    79%   65-66, 76-77
src/peft/tuners/prompt_tuning/__init__.py                 5      0   100%
src/peft/tuners/prompt_tuning/config.py                  23      0   100%
src/peft/tuners/prompt_tuning/model.py                   30      0   100%
src/peft/tuners/randlora/__init__.py                     15      1    93%   40
src/peft/tuners/randlora/bnb.py                         209    181    13%   46-51, 75-105, 111-135, 147-183, 194-201, 218-251, 254-255, 274-278, 302-327, 333-350, 364-398, 408-415, 418-452, 455-456
src/peft/tuners/randlora/config.py                       26      0   100%
src/peft/tuners/randlora/layer.py                       176     14    92%   80-81, 106, 109, 133, 147, 150, 159, 162, 220, 235, 251-252, 339
src/peft/tuners/randlora/model.py                       144     24    83%   60, 119-123, 133-134, 239, 255, 304, 309-317, 319-327, 330-343
src/peft/tuners/road/__init__.py                         15      1    93%   47
src/peft/tuners/road/bnb.py                             195    157    19%   43-47, 67-113, 119-158, 161-190, 193-194, 200, 206-214, 232-236, 256-301, 307-344, 347-381, 384-385, 391, 397-405
src/peft/tuners/road/config.py                           21      1    95%   124
src/peft/tuners/road/layer.py                           199     31    84%   70, 102, 151-162, 175, 182, 202-228, 246, 265, 276, 298-299, 380, 411
src/peft/tuners/road/model.py                            77     22    71%   34-35, 55, 136-163
src/peft/tuners/shira/__init__.py                         6      0   100%
src/peft/tuners/shira/config.py                          25      0   100%
src/peft/tuners/shira/layer.py                          102     13    87%   48, 64, 72, 95, 103, 106-109, 128, 150, 163, 174-175
src/peft/tuners/shira/mask_functions.py                  14      0   100%
src/peft/tuners/shira/model.py                           42      6    86%   72, 81, 109, 115-121
src/peft/tuners/trainable_tokens/__init__.py              6      0   100%
src/peft/tuners/trainable_tokens/config.py               13      0   100%
src/peft/tuners/trainable_tokens/layer.py               115     14    88%   88-105, 127, 168, 180, 187-188, 242
src/peft/tuners/trainable_tokens/model.py                41      0   100%
src/peft/tuners/tuners_utils.py                         775     81    90%   78-79, 88-105, 110, 114-123, 160, 177, 181, 184, 187, 190, 193, 196, 199, 284, 346, 434-439, 456-460, 478, 499, 648, 857, 859, 870, 872, 987, 1035-1039, 1046, 1048, 1051-1054, 1214, 1226, 1229, 1251, 1373, 1497, 1530, 1575, 1603, 1671, 1718, 1725-1730, 1732, 1749-1754, 1789-1790
src/peft/tuners/vblora/__init__.py                        6      0   100%
src/peft/tuners/vblora/config.py                         29      1    97%   196
src/peft/tuners/vblora/layer.py                         130      7    95%   75, 77, 154, 165, 175-176, 245
src/peft/tuners/vblora/model.py                          77     12    84%   90, 127, 133-137, 146, 184-189, 205-206
src/peft/tuners/vera/__init__.py                         15      1    93%   40
src/peft/tuners/vera/bnb.py                             208    182    12%   46-51, 62-98, 101-125, 143-178, 195-237, 240-241, 260-265, 276-309, 312-329, 334-361, 364-407, 410-411
src/peft/tuners/vera/config.py                           28      1    96%   156
src/peft/tuners/vera/layer.py                           149      8    95%   81, 99, 115, 125, 184, 208-209, 268
src/peft/tuners/vera/model.py                           119     15    87%   57, 123, 133-134, 194, 241, 246-254, 256-264, 267-271, 280
src/peft/tuners/xlora/__init__.py                         5      0   100%
src/peft/tuners/xlora/classifier.py                      88     11    88%   74-77, 80, 118-120, 142-143, 185-186
src/peft/tuners/xlora/config.py                          36      3    92%   94, 97, 102
src/peft/tuners/xlora/layer.py                          110     29    74%   114, 158, 161, 170-171, 186, 194-223
src/peft/tuners/xlora/model.py                          209     17    92%   75-85, 148, 241, 257, 261, 266-267, 350, 399, 405, 437, 442, 445
src/peft/utils/__init__.py                                7      0   100%
src/peft/utils/constants.py                              63     16    75%   23-32, 37-43, 50, 59
src/peft/utils/hotswap.py                               201     28    86%   46, 50, 75-76, 130, 136, 172, 175, 210, 216, 356, 360, 423, 457, 474-500, 542
src/peft/utils/incremental_pca.py                       148     10    93%   74, 76-77, 103, 121, 142, 146, 148, 199-200
src/peft/utils/integrations.py                          159     84    47%   33, 45-49, 61-62, 65-66, 70-73, 79-86, 94-124, 130, 132, 142-163, 175-191, 214, 228-230, 234, 242-246, 251, 253, 258, 260
src/peft/utils/loftq_utils.py                           234    202    14%   37-49, 53-61, 65-87, 90-103, 106-113, 116-154, 158-170, 177-187, 192-237, 242-258, 270-308, 311-327, 366-409
src/peft/utils/merge_utils.py                            79      7    91%   91-92, 94, 100, 120-123
src/peft/utils/other.py                                 652    177    73%   113, 115, 117, 119, 121, 187-190, 198, 223-232, 263, 267, 274, 316, 322, 333, 360-368, 377-381, 384, 389, 393, 406, 456, 472-475, 479, 483, 492, 500, 521, 525, 544-547, 554-557, 591, 622, 632, 647-676, 691, 711-713, 770-773, 833, 839, 866-867, 880-881, 898, 910, 957, 1019, 1030, 1043, 1070, 1085-1123, 1140-1144, 1154, 1172, 1183-1209, 1216-1246, 1265-1267, 1289-1293, 1309, 1328-1329, 1437-1479
src/peft/utils/peft_types.py                             60      6    90%   141, 144, 147, 160, 163, 167
src/peft/utils/save_and_load.py                         337     62    82%   54, 103-112, 117-119, 138-149, 171, 193, 206-211, 228, 236, 325, 338-357, 424, 476, 479, 495, 499, 523-527, 551-561, 600, 625, 657-658, 665, 683
src/peft/utils/warning.py                                 1      0   100%
-----------------------------------------------------------------------------------
TOTAL                                                 16919   4359    74%
============================= slowest 10 durations =============================
37.38s call     tests/test_auto.py::TestPeftAutoModel::test_embedding_size_not_reduced_if_greater_vocab_size
30.18s call     tests/test_poly.py::TestPoly::test_poly
14.29s call     tests/test_xlora.py::TestXlora::test_save_load_functional_pt
13.82s call     tests/test_xlora.py::TestXlora::test_save_load_functional
11.76s call     tests/test_custom_models.py::TestPeftCustomModel::test_gpt2_dora_merge_and_unload
10.76s call     tests/test_decoder_models.py::TestDecoderModels::test_generate_with_mixed_adapter_batches[LoraConfig-config_kwargs8-hf-internal-testing/tiny-random-Gemma3ForCausalLM]
10.72s call     tests/test_xlora.py::TestXlora::test_scalings_logging_methods
8.06s call     tests/test_auto.py::TestPeftAutoModel::test_peft_whisper
7.51s call     tests/test_gpu_examples.py::TestFSDPWrap::test_bnb_4bit_wrap_fsdp
7.48s call     tests/test_custom_models.py::TestPeftCustomModel::test_dora_save_and_load_remapping
=========================== short test summary info ============================
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 1 OFT-MLP-OFTConfig-config_kwargs96]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 2 OFT-MLP-OFTConfig-config_kwargs97]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 5 OFT-MLP-OFTConfig-config_kwargs98]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 6 OFT-MLP-OFTConfig-config_kwargs99]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 7 OFT-MLP-OFTConfig-config_kwargs100]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 8 OFT-MLP-OFTConfig-config_kwargs101]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 9 OFT-MLP-OFTConfig-config_kwargs102]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 10 OFT-MLP-OFTConfig-config_kwargs103]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 11 OFT-MLP-OFTConfig-config_kwargs104]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 12 OFT-MLP-OFTConfig-config_kwargs105]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 13 OFT-MLP-OFTConfig-config_kwargs106]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Conv2d 1 OFT-Conv2d-OFTConfig-config_kwargs107]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Conv2d 3 OFT-Conv2d-OFTConfig-config_kwargs108]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Conv2d 4 OFT-Conv2d-OFTConfig-config_kwargs109]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Conv2d 5 OFT-Conv2d-OFTConfig-config_kwargs110]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_disable_adapters_with_merging[Conv2d 1 OFT-Conv2d-OFTConfig-config_kwargs107]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_disable_adapters_with_merging[Conv2d 4 OFT-Conv2d-OFTConfig-config_kwargs109]
FAILED tests/test_gpu_examples.py::TestFSDPWrap::test_bnb_4bit_wrap_fsdp - Ru...
FAILED tests/test_gpu_examples.py::TestLowCpuMemUsageDifferentDevices::test_low_cpu_mem_usage_with_quantization[bnb-4bit]
FAILED tests/test_initialization.py::TestOft::test_load_outdated_oft_checkpoint_warns[0.17.0]
FAILED tests/test_initialization.py::TestOft::test_load_outdated_oft_checkpoint_warns[None]
FAILED tests/test_initialization.py::TestHotSwapping::test_hotswap_works[True-config0]
FAILED tests/test_initialization.py::TestHotSwapping::test_hotswap_works[True-config1]
FAILED tests/test_tuners_utils.py::PeftCustomKwargsTester::test_maybe_include_all_linear_layers_diffusion
FAILED tests/test_helpers.py::TestScalingAdapters::test_diffusers_pipeline - ...
FAILED tests/test_hub_features.py::TestBaseModelRevision::test_save_and_load_base_model_revision
FAILED tests/test_loraplus.py::test_lora_plus_optimizer_sucess - AttributeErr...
FAILED tests/test_hub_features.py::TestBaseModelRevision::test_load_different_peft_and_base_model_revision
FAILED tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_stable_diffusion
ERROR tests/test_stablediffusion.py - TypeError: CLIPTextModel.__init__() got...
ERROR tests/test_stablediffusion.py - TypeError: CLIPTextModel.__init__() got...
ERROR tests/test_stablediffusion.py - TypeError: CLIPTextModel.__init__() got...
ERROR tests/test_stablediffusion.py - TypeError: CLIPTextModel.__init__() got...
= 29 failed, 13876 passed, 4337 skipped, 10 xfailed, 2 xpassed, 11184 warnings, 4 errors in 963.47s (0:16:03) =
make: *** [Makefile:20: test] Error 1
