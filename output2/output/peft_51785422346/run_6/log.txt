Requirement already satisfied: pip in ./venv/lib/python3.11/site-packages (22.0.2)
Collecting pip
  Using cached pip-25.2-py3-none-any.whl (1.8 MB)
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 22.0.2
    Uninstalling pip-22.0.2:
      Successfully uninstalled pip-22.0.2
Successfully installed pip-25.2
Requirement already satisfied: setuptools in ./venv/lib/python3.11/site-packages (59.6.0)
Obtaining file:///app
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Collecting numpy>=1.17 (from peft==0.17.2.dev0)
  Using cached numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)
Collecting packaging>=20.0 (from peft==0.17.2.dev0)
  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)
Collecting psutil (from peft==0.17.2.dev0)
  Using cached psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)
Collecting pyyaml (from peft==0.17.2.dev0)
  Using cached pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)
Collecting torch>=1.13.0 (from peft==0.17.2.dev0)
  Using cached torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)
Collecting transformers (from peft==0.17.2.dev0)
  Using cached transformers-4.57.0-py3-none-any.whl.metadata (41 kB)
Collecting tqdm (from peft==0.17.2.dev0)
  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)
Collecting accelerate>=0.21.0 (from peft==0.17.2.dev0)
  Using cached accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)
Collecting safetensors (from peft==0.17.2.dev0)
  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
Collecting huggingface_hub>=0.25.0 (from peft==0.17.2.dev0)
  Using cached huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)
Collecting black (from peft==0.17.2.dev0)
  Using cached black-25.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (83 kB)
Collecting hf-doc-builder (from peft==0.17.2.dev0)
  Using cached hf_doc_builder-0.5.0-py3-none-any.whl.metadata (28 kB)
Collecting ruff~=0.12.8 (from peft==0.17.2.dev0)
  Using cached ruff-0.12.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)
Collecting pytest (from peft==0.17.2.dev0)
  Using cached pytest-8.4.2-py3-none-any.whl.metadata (7.7 kB)
Collecting pytest-cov (from peft==0.17.2.dev0)
  Using cached pytest_cov-7.0.0-py3-none-any.whl.metadata (31 kB)
Collecting pytest-xdist (from peft==0.17.2.dev0)
  Using cached pytest_xdist-3.8.0-py3-none-any.whl.metadata (3.0 kB)
Collecting parameterized (from peft==0.17.2.dev0)
  Using cached parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)
Collecting datasets (from peft==0.17.2.dev0)
  Using cached datasets-4.1.1-py3-none-any.whl.metadata (18 kB)
Collecting diffusers (from peft==0.17.2.dev0)
  Using cached diffusers-0.35.1-py3-none-any.whl.metadata (20 kB)
Collecting scipy (from peft==0.17.2.dev0)
  Using cached scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)
Collecting protobuf (from peft==0.17.2.dev0)
  Using cached protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)
Collecting sentencepiece (from peft==0.17.2.dev0)
  Using cached sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)
Collecting filelock (from huggingface_hub>=0.25.0->peft==0.17.2.dev0)
  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)
Collecting fsspec>=2023.5.0 (from huggingface_hub>=0.25.0->peft==0.17.2.dev0)
  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)
Collecting requests (from huggingface_hub>=0.25.0->peft==0.17.2.dev0)
  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)
Collecting typing-extensions>=3.7.4.3 (from huggingface_hub>=0.25.0->peft==0.17.2.dev0)
  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)
Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub>=0.25.0->peft==0.17.2.dev0)
  Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)
Collecting sympy>=1.13.3 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)
Collecting networkx (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)
Collecting jinja2 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)
Collecting nvidia-nccl-cu12==2.27.3 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)
Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting triton==3.4.0 (from torch>=1.13.0->peft==0.17.2.dev0)
  Using cached triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)
Requirement already satisfied: setuptools>=40.8.0 in ./venv/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.13.0->peft==0.17.2.dev0) (59.6.0)
Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.13.0->peft==0.17.2.dev0)
  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)
Collecting click>=8.0.0 (from black->peft==0.17.2.dev0)
  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)
Collecting mypy-extensions>=0.4.3 (from black->peft==0.17.2.dev0)
  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)
Collecting pathspec>=0.9.0 (from black->peft==0.17.2.dev0)
  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)
Collecting platformdirs>=2 (from black->peft==0.17.2.dev0)
  Using cached platformdirs-4.4.0-py3-none-any.whl.metadata (12 kB)
Collecting pytokens>=0.1.10 (from black->peft==0.17.2.dev0)
  Using cached pytokens-0.1.10-py3-none-any.whl.metadata (2.0 kB)
Collecting pyarrow>=21.0.0 (from datasets->peft==0.17.2.dev0)
  Using cached pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)
Collecting dill<0.4.1,>=0.3.0 (from datasets->peft==0.17.2.dev0)
  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)
Collecting pandas (from datasets->peft==0.17.2.dev0)
  Using cached pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)
Collecting xxhash (from datasets->peft==0.17.2.dev0)
  Using cached xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)
Collecting multiprocess<0.70.17 (from datasets->peft==0.17.2.dev0)
  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)
Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)
Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)
Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)
Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)
Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)
Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached propcache-0.4.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)
Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)
Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0)
  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)
Collecting charset_normalizer<4,>=2 (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0)
  Using cached charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)
Collecting urllib3<3,>=1.21.1 (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0)
  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)
Collecting certifi>=2017.4.17 (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0)
  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)
Collecting importlib_metadata (from diffusers->peft==0.17.2.dev0)
  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)
Collecting regex!=2019.12.17 (from diffusers->peft==0.17.2.dev0)
  Using cached regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)
Collecting Pillow (from diffusers->peft==0.17.2.dev0)
  Using cached pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)
Collecting GitPython (from hf-doc-builder->peft==0.17.2.dev0)
  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)
Collecting nbformat (from hf-doc-builder->peft==0.17.2.dev0)
  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)
Collecting gitdb<5,>=4.0.1 (from GitPython->hf-doc-builder->peft==0.17.2.dev0)
  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)
Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython->hf-doc-builder->peft==0.17.2.dev0)
  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)
Collecting zipp>=3.20 (from importlib_metadata->diffusers->peft==0.17.2.dev0)
  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)
Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.13.0->peft==0.17.2.dev0)
  Using cached markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)
Collecting fastjsonschema>=2.15 (from nbformat->hf-doc-builder->peft==0.17.2.dev0)
  Using cached fastjsonschema-2.21.2-py3-none-any.whl.metadata (2.3 kB)
Collecting jsonschema>=2.6 (from nbformat->hf-doc-builder->peft==0.17.2.dev0)
  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)
Collecting jupyter-core!=5.0.*,>=4.12 (from nbformat->hf-doc-builder->peft==0.17.2.dev0)
  Using cached jupyter_core-5.8.1-py3-none-any.whl.metadata (1.6 kB)
Collecting traitlets>=5.1 (from nbformat->hf-doc-builder->peft==0.17.2.dev0)
  Using cached traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)
Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.6->nbformat->hf-doc-builder->peft==0.17.2.dev0)
  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)
Collecting referencing>=0.28.4 (from jsonschema>=2.6->nbformat->hf-doc-builder->peft==0.17.2.dev0)
  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)
Collecting rpds-py>=0.7.1 (from jsonschema>=2.6->nbformat->hf-doc-builder->peft==0.17.2.dev0)
  Using cached rpds_py-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
Collecting python-dateutil>=2.8.2 (from pandas->datasets->peft==0.17.2.dev0)
  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
Collecting pytz>=2020.1 (from pandas->datasets->peft==0.17.2.dev0)
  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)
Collecting tzdata>=2022.7 (from pandas->datasets->peft==0.17.2.dev0)
  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)
Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets->peft==0.17.2.dev0)
  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)
Collecting iniconfig>=1 (from pytest->peft==0.17.2.dev0)
  Using cached iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)
Collecting pluggy<2,>=1.5 (from pytest->peft==0.17.2.dev0)
  Using cached pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)
Collecting pygments>=2.7.2 (from pytest->peft==0.17.2.dev0)
  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)
Collecting coverage>=7.10.6 (from coverage[toml]>=7.10.6->pytest-cov->peft==0.17.2.dev0)
  Using cached coverage-7.10.7-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (8.9 kB)
Collecting execnet>=2.1 (from pytest-xdist->peft==0.17.2.dev0)
  Using cached execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)
Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers->peft==0.17.2.dev0)
  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)
Using cached ruff-0.12.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)
Using cached accelerate-1.10.1-py3-none-any.whl (374 kB)
Using cached numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)
Using cached huggingface_hub-0.35.3-py3-none-any.whl (564 kB)
Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)
Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)
Using cached packaging-25.0-py3-none-any.whl (66 kB)
Using cached pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)
Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)
Using cached torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)
Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)
Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)
Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)
Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)
Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)
Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)
Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)
Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)
Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)
Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)
Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)
Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)
Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)
Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)
Using cached triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)
Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)
Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)
Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)
Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)
Using cached black-25.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.6 MB)
Using cached click-8.3.0-py3-none-any.whl (107 kB)
Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)
Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)
Using cached platformdirs-4.4.0-py3-none-any.whl (18 kB)
Using cached pytokens-0.1.10-py3-none-any.whl (12 kB)
Using cached datasets-4.1.1-py3-none-any.whl (503 kB)
Using cached dill-0.4.0-py3-none-any.whl (119 kB)
Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)
Using cached aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)
Using cached multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)
Using cached yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)
Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)
Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)
Using cached attrs-25.3.0-py3-none-any.whl (63 kB)
Using cached frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)
Using cached idna-3.10-py3-none-any.whl (70 kB)
Using cached propcache-0.4.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (209 kB)
Using cached pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)
Using cached requests-2.32.5-py3-none-any.whl (64 kB)
Using cached charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)
Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)
Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)
Using cached diffusers-0.35.1-py3-none-any.whl (4.1 MB)
Using cached regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)
Using cached filelock-3.19.1-py3-none-any.whl (15 kB)
Using cached hf_doc_builder-0.5.0-py3-none-any.whl (67 kB)
Using cached gitpython-3.1.45-py3-none-any.whl (208 kB)
Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)
Using cached smmap-5.0.2-py3-none-any.whl (24 kB)
Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)
Using cached zipp-3.23.0-py3-none-any.whl (10 kB)
Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)
Using cached markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)
Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)
Using cached fastjsonschema-2.21.2-py3-none-any.whl (24 kB)
Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)
Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)
Using cached jupyter_core-5.8.1-py3-none-any.whl (28 kB)
Using cached referencing-0.36.2-py3-none-any.whl (26 kB)
Using cached rpds_py-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)
Using cached traitlets-5.14.3-py3-none-any.whl (85 kB)
Using cached networkx-3.5-py3-none-any.whl (2.0 MB)
Using cached pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)
Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)
Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)
Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)
Using cached parameterized-0.9.0-py2.py3-none-any.whl (20 kB)
Using cached pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)
Using cached protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl (322 kB)
Using cached psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)
Using cached pytest-8.4.2-py3-none-any.whl (365 kB)
Using cached pluggy-1.6.0-py3-none-any.whl (20 kB)
Using cached iniconfig-2.1.0-py3-none-any.whl (6.0 kB)
Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)
Using cached pytest_cov-7.0.0-py3-none-any.whl (22 kB)
Using cached coverage-7.10.7-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (250 kB)
Using cached pytest_xdist-3.8.0-py3-none-any.whl (46 kB)
Using cached execnet-2.1.1-py3-none-any.whl (40 kB)
Using cached scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)
Using cached sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)
Using cached transformers-4.57.0-py3-none-any.whl (12.0 MB)
Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)
Using cached xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)
Building wheels for collected packages: peft
  Building editable for peft (pyproject.toml): started
  Building editable for peft (pyproject.toml): finished with status 'done'
  Created wheel for peft: filename=peft-0.17.2.dev0-0.editable-py3-none-any.whl size=10766 sha256=f0e386568b19d25ef13c20838d45f2ba1284f78fe68a738ed77f8adf4f717ad6
  Stored in directory: /tmp/pip-ephem-wheel-cache-yxe7u3l5/wheels/57/0f/98/bb57b2b57b95807699b822a35c022f139d38a02c27922f27ce
Successfully built peft
Installing collected packages: pytz, nvidia-cusparselt-cu12, mpmath, fastjsonschema, zipp, xxhash, urllib3, tzdata, typing-extensions, triton, traitlets, tqdm, sympy, smmap, six, sentencepiece, safetensors, ruff, rpds-py, regex, pyyaml, pytokens, pygments, pyarrow, psutil, protobuf, propcache, pluggy, platformdirs, Pillow, pathspec, parameterized, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, mypy-extensions, multidict, MarkupSafe, iniconfig, idna, hf-xet, fsspec, frozenlist, filelock, execnet, dill, coverage, click, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, scipy, requests, referencing, python-dateutil, pytest, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, jupyter-core, jinja2, importlib_metadata, gitdb, black, aiosignal, pytest-xdist, pytest-cov, pandas, nvidia-cusolver-cu12, jsonschema-specifications, huggingface_hub, GitPython, aiohttp, torch, tokenizers, jsonschema, diffusers, transformers, nbformat, datasets, accelerate, peft, hf-doc-builder

Successfully installed GitPython-3.1.45 MarkupSafe-3.0.3 Pillow-11.3.0 accelerate-1.10.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 black-25.9.0 certifi-2025.10.5 charset_normalizer-3.4.3 click-8.3.0 coverage-7.10.7 datasets-4.1.1 diffusers-0.35.1 dill-0.4.0 execnet-2.1.1 fastjsonschema-2.21.2 filelock-3.19.1 frozenlist-1.7.0 fsspec-2025.9.0 gitdb-4.0.12 hf-doc-builder-0.5.0 hf-xet-1.1.10 huggingface_hub-0.35.3 idna-3.10 importlib_metadata-8.7.0 iniconfig-2.1.0 jinja2-3.1.6 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 jupyter-core-5.8.1 mpmath-1.3.0 multidict-6.6.4 multiprocess-0.70.16 mypy-extensions-1.1.0 nbformat-5.10.4 networkx-3.5 numpy-2.3.3 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 packaging-25.0 pandas-2.3.3 parameterized-0.9.0 pathspec-0.12.1 peft-0.17.2.dev0 platformdirs-4.4.0 pluggy-1.6.0 propcache-0.4.0 protobuf-6.32.1 psutil-7.1.0 pyarrow-21.0.0 pygments-2.19.2 pytest-8.4.2 pytest-cov-7.0.0 pytest-xdist-3.8.0 python-dateutil-2.9.0.post0 pytokens-0.1.10 pytz-2025.2 pyyaml-6.0.3 referencing-0.36.2 regex-2025.9.18 requests-2.32.5 rpds-py-0.27.1 ruff-0.12.12 safetensors-0.6.2 scipy-1.16.2 sentencepiece-0.2.1 six-1.17.0 smmap-5.0.2 sympy-1.14.0 tokenizers-0.22.1 torch-2.8.0 tqdm-4.67.1 traitlets-5.14.3 transformers-4.57.0 triton-3.4.0 typing-extensions-4.15.0 tzdata-2025.2 urllib3-2.5.0 xxhash-3.6.0 yarl-1.20.1 zipp-3.23.0
python -m pytest -n 3 tests/
============================= test session starts ==============================
platform linux -- Python 3.11.0rc1, pytest-8.4.2, pluggy-1.6.0
rootdir: /app
configfile: pyproject.toml
plugins: xdist-3.8.0, cov-7.0.0
created: 3/3 workers
3 workers [18244 items]

sssssssssssss.ss.ss.s.s.ss.s.ss.s....................................... [  0%]
........................................................................ [  0%]
........................................................................ [  1%]
........................................................................ [  1%]
........................................................................ [  1%]
........................................................................ [  2%]
........................................................................ [  2%]
........................................................................ [  3%]
........................................................................ [  3%]
........................................................................ [  3%]
........................................................................ [  4%]
........................................................................ [  4%]
...................................sssssssssssssssssssssssssssssssssssss [  5%]
ssssssssssssssssssssssssss.sssssssss.ssssssss.sssssssss.ssssssss.sssssss [  5%]
ss.ssssssss.sssssssss.sssssssss.ssssssss.sssssssss.sssssssss.ssssssss.ss [  5%]
ssssss.ssssssss.sssssssss.s............................................. [  6%]
........................................................................ [  6%]
............ss.......................................................... [  7%]
.........................................................F.............. [  7%]
.....F...............F..........F.................F.........F........... [  7%]
....F.........F........................F...........sssssssssssssssssssss [  8%]
sssssssssssssssssssssssssFssssssssssssssssssssssssssssssssssssssssssssss [  8%]
sssssssssssssssssssssssssssssssssssss......ssssssssssssssssssss......... [  9%]
.......................................................F................ [  9%]
..................................................................F..... [  9%]
........................................................................ [ 10%]
.................F.........................................F............ [ 10%]
.....................................F.................................. [ 11%]
........................................................................ [ 11%]
........................................................................ [ 11%]
........................................................................ [ 12%]
.........................x...................x.......................... [ 12%]
..............x......................................................... [ 13%]
........................................................................ [ 13%]
........................................................................ [ 13%]
........................................................................ [ 14%]
........................................................................ [ 14%]
........................................................................ [ 14%]
........................................................................ [ 15%]
........................................................................ [ 15%]
........................................................................ [ 16%]
..........ss............................................................ [ 16%]
........................................................................ [ 16%]
........................................................................ [ 17%]
........................................................................ [ 17%]
........................................................................ [ 18%]
........................................................................ [ 18%]
...........................sss.s........................................ [ 18%]
........................................................................ [ 19%]
................................................................ssssssss [ 19%]
s.ssssss..................................sssssss.sssss................. [ 20%]
.............................................................ssss....... [ 20%]
.......................................................................s [ 20%]
sssssssssssssssssssssssssss.sssssssssssssssssssss.sssssssssssssssssssss. [ 21%]
.......sssssssssssssss.sssss.................................ssss....... [ 21%]
........................................................................ [ 22%]
........................................................................ [ 22%]
........................................................................ [ 22%]
...........ss.ss........................................................ [ 23%]
........................................................................ [ 23%]
........................................................................ [ 24%]
.......................................s...........................s.... [ 24%]
........................................................................ [ 24%]
........................................................................ [ 25%]
........................................................................ [ 25%]
........................................................................ [ 26%]
........................................................................ [ 26%]
........................................................................ [ 26%]
.....................................................ss................. [ 27%]
........................................................................ [ 27%]
...........................................ssss.s..............sssss.... [ 28%]
...................s.ssssssss.ssssss.sssss.............................. [ 28%]
...............ss....................................................... [ 28%]
........................................................................ [ 29%]
........sssss.......................s.sssssss.sss...............s.ssssss [ 29%]
sssssssssssss........................................................... [ 29%]
........................................................................ [ 30%]
........................................................................ [ 30%]
...............................ssss..................................... [ 31%]
.............................................F.F........................ [ 31%]
...........................................................sssssssssssss [ 31%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 32%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 32%]
sssssssssssssssssssssssssss.ssssssss....s..........s...................s [ 33%]
s........s..s...............s................s.................s........ [ 33%]
.......s....s...s...s...s..s...s...................s....sss....s........ [ 33%]
s...sss.....................s..................s..................s..... [ 34%]
........................................................................ [ 34%]
.....sssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 35%]
sssssssssssssssssssssssssssssssss............ssssssssssssssssssssssssss. [ 35%]
sssssssssssssssss.........s..........................................ss. [ 35%]
sssssssssssssssssssss.ssssssssssssssssssssssssssss.ssssssssssssssssss... [ 36%]
........................................................................ [ 36%]
..........................................s............................. [ 37%]
........................................................................ [ 37%]
.........................................s..............s............... [ 37%]
.....................s.................................................. [ 38%]
........................................................................ [ 38%]
.............................ssssss.sss..s.............................. [ 39%]
........................................................................ [ 39%]
........................................................................ [ 39%]
........................................................................ [ 40%]
..............ss............s..................ss....................... [ 40%]
........ss....................s...................x........x............ [ 41%]
......................................................s................s [ 41%]
sssssssssssssssss.ssssssssssssssssssssssssss.sssssssssssssssssssssssssss [ 41%]
sssss.sssssssssssssssssssssss.sssssssssssssssssssssssssssss.sssssss..... [ 42%]
...........s...................s....xx.................................. [ 42%]
...............................s.s...................................... [ 43%]
........................................................................ [ 43%]
........................................................................ [ 43%]
........................................................................ [ 44%]
........................................................................ [ 44%]
........................................................................ [ 44%]
........................................................................ [ 45%]
........................................................................ [ 45%]
........................................................................ [ 46%]
......s.....................s.................s......................... [ 46%]
.....................................s.................................. [ 46%]
........................................................................ [ 47%]
.........................s...........................................sss [ 47%]
ss.ssssssssssssssssssssssss.ssssssssssssssssssssssss.ssssss.sssssssss.s. [ 48%]
..........s.........s.............................s...................s. [ 48%]
.......s.........s............................s..........s.............. [ 48%]
..............ssssss.ssssssssssss.sssssssssssss.ssssssssssssss.......... [ 49%]
...s...............s..............................s.........s........... [ 49%]
.......s.............s......s.......................s..s................ [ 50%]
..................................s................s..................s. [ 50%]
........................................................................ [ 50%]
................s....................................................... [ 51%]
..................s.........s.........s...................s............. [ 51%]
..........................s.........................................s... [ 52%]
......s.............s................s.s......s......................... [ 52%]
..s......................s.............................................s [ 52%]
...................s...................sssssssss....s................... [ 53%]
......s......s....s.................s..........s............s........... [ 53%]
....................................s...............s................... [ 54%]
.s........s...............................s..............sss.ssssss..... [ 54%]
s...........................s....................s...................... [ 54%]
.....s..............s............s...................................... [ 55%]
...s......................s...s........................s................ [ 55%]
...s...............................sssssssss....s....................... [ 56%]
..............................s........s..........................s..... [ 56%]
.......s.............s...s........................................s..... [ 56%]
..s....s............s......s....s................s...................... [ 57%]
.......s...........s....s.s.s.s.s.s.s.s.s.s.s..s.s.s.s.s.s.s.s.s.s.s..s. [ 57%]
s.s.s.s.s.s.s.s.ssss.....s.......s.sssssssssss.....ss.s...sss.......s..s [ 58%]
........................s....s..s.s.s.s.ssssssssssssssssssssssssssssssss [ 58%]
ssssssssssssssssssssssssssssssssssssss.ssssss.sssssssss.........ssssssss [ 58%]
ssssss.sssssssssssssssssssssssssssssssssssssss.s.s.........sssssssssssss [ 59%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss.... [ 59%]
.....s..............................s............s............s......... [ 59%]
...........................s............................................ [ 60%]
........sssssssss..........................s...........................s [ 60%]
ss.ssssss......................s............s.ssssssssssssssssssssssssss [ 61%]
sssssssssssssssssssssssss.sssssssssssssssssssssssssssss.ssssssssssssssss [ 61%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 61%]
sssssssssssssssssssssssssss.ssssssssssssssssssssssssssssssssssssssssssss [ 62%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 62%]
sssssssssssssssssssss.s............ssssssssss.ssssssssssssssssssssssssss [ 63%]
sssssssssssssssssssssssssss.................s..............s............ [ 63%]
....s..s..........sssssssss........s..........s..............ssssssssss. [ 63%]
...s.......................s...s.......................ssssssssss....s.. [ 64%]
..s..........sssssssssssssssssssssssssss.s.........s..s....s............ [ 64%]
.............s...s..........ssss.s.ssssssssssssssssssssssssss.ssssssss.. [ 65%]
......s....s............ssssssssssssssssssss.ssssss.ssssssssssssssssssss [ 65%]
sssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss. [ 65%]
...............ssssssssssssssssss..sssssssssssssssssss...............s.. [ 66%]
........ssssssssssssssssssssss.ssssssssssssssssssssssssssssssss......... [ 66%]
.....................sssssssssss.ssssssssss.ssssssssssssssss......s..... [ 67%]
.................sssssssssssssss.sssssssssssssssssssssssssssssssssssssss [ 67%]
sss.ssssssssssssssssssssssss....s.s......s........s.........ssss.sssssss [ 67%]
ssss.ssssssssssssss.sssssssssssssssssssssssss....s.s.......s...........s [ 68%]
...ssss.sssssssssssssssssssssss...s.........s.......s.....s............. [ 68%]
.....s..............s.ssssss.sss..s....s...............s................ [ 69%]
.........s............................s..................sssssss.sssssss [ 69%]
ssssssss.ssssssssssssssssssssssssssssss.ssssssssssssssssss.ss........... [ 69%]
ss.sssssss..........................s.........................s......... [ 70%]
..........ssssss.sssssssssssss.sssssssssssss.sssssssssssss.............. [ 70%]
....s...................s.............sssssssssssssss.sss...s........... [ 71%]
.....s................s..............ssssssss.s........s............s... [ 71%]
.....ss.ssssssssssssssssssssssssssssssssssss.sssssssssssssssssssssssss.s [ 71%]
sssssssssssssssssssssssss.sssssssssssssssssssssssssssssssssssssss.ssssss [ 72%]
ssssssssss........................ss...s..............s.............s... [ 72%]
............sssssssssssss.sssss.....................s................... [ 73%]
...ss.............................................s.......sssssssss..... [ 73%]
.............s....s..........ss.s..............................s........ [ 73%]
................s..ss..s..........s............s..................ssssss [ 74%]
sssssssssssssssssssssssssssssssssssssssssssssssss.ssssssssssssssssssssss [ 74%]
ssssssssssssss.sssssssssss.sssssssssssssssss.....s.........ssssssssssss. [ 74%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss............ [ 75%]
.......................s...sssssssssssssssssssssssssss.................. [ 75%]
............s........................................................... [ 76%]
..........s...............................................s...........s. [ 76%]
...................s................s................................... [ 76%]
......ss................................................................ [ 77%]
.....................ssssssss...................................ss.sssss [ 77%]
sssssssss......................ssssssssssssssssssssssssssssssss........s [ 78%]
sssssssssssssssssss....................................sssssssss........ [ 78%]
........................................................................ [ 78%]
.s..............ssssssssssss................ss...ssssss.ssssssssssssssss [ 79%]
ssssssssssss...ssssssssssssss..................................ssssss... [ 79%]
..........ss.....sssssss...ssssssssssss................................. [ 80%]
.ssssssssssssssssssssssssssssssssssssss.........ssssssssss.............. [ 80%]
..................ss................ssss.......................s........ [ 80%]
....................s................................................... [ 81%]
........................................................................ [ 81%]
.................................................................sssssss [ 82%]
...................ssssssssssssssssss...........................sss.s... [ 82%]
..................ss.................................................... [ 82%]
.................s...................................................... [ 83%]
.............................ssssssssssssss...........ssssssssssss...... [ 83%]
..ssss.......s....ssssss........s...........s........................... [ 84%]
...............s.................sss..sssssss........................sss [ 84%]
s....................................s.................................. [ 84%]
.............ssssssssss......s.........ssss..s..........ssssssssss...... [ 85%]
.ssssssssssssssssss..................................................... [ 85%]
.........................ssssssssss........ss.....................ssssss [ 86%]
sss.sssssssssssssssssssssssssssssssssssssss............................. [ 86%]
ssssssssssssssssssssssssssssssssssss..sss...ss....ss........ss..ss.ssss. [ 86%]
.ss....ss...ss....ssssss...............................ss....ss......... [ 87%]
........................................................ss.....sssssssss [ 87%]
sssssss................................................................. [ 88%]
.......................s...ssssssssssssssssss.ssssssssss................ [ 88%]
...........................ssssssss.s.sssssssssssssss................... [ 88%]
...ss.ssssssssss...................ssssssssssss.ssssss.sssssssssssssssss [ 89%]
s.............................................sssssssssssssss.ssssssssss [ 89%]
sssssssssssssssssssssssssssssssss..............sssssssssssssssssssssssss [ 89%]
sssssssssssssssssssssssssssssssssssssssssss.ssssssssssssssssssssssssssss [ 90%]
sssssssssssssssss.................ssssssssssssssssssss.........ssssssss. [ 90%]
...............................ssssssssssssssssssss.ssssssssssssssssssss [ 91%]
ssssssssssssssssssssssssssssssssssssssss.ssssssss.ssssssssssssssssssssss [ 91%]
ssss............ssssssssssssss.ssssss....ssssssss....................... [ 91%]
...............................................................F......F. [ 92%]
......................XF.X.........x.F....................s.....X....... [ 92%]
........................................................................ [ 93%]
........................................................................ [ 93%]
........................................................................ [ 93%]
.........sss............................................................ [ 94%]
...............s.ssssssssssssssssssssssssss.sssssssss................... [ 94%]
........................sssssssssssssss................................. [ 95%]
...........................................sss.................s........ [ 95%]
........................................................................ [ 95%]
....................ssF...............F................................. [ 96%]
......................ssssss.................................x.......x.. [ 96%]
.......................ss............................................... [ 97%]
........................................................ssssssss........ [ 97%]
....ssssssssss.....ssssssssssssssss..........ssssssssssssssss........... [ 97%]
...................................................................sssss [ 98%]
sssssssssss..............................................ssssssssss..... [ 98%]
............................................ss.......................... [ 99%]
............ss.....s...F................................................ [ 99%]
........................................................................ [ 99%]
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
............................                                             [100%]
==================================== ERRORS ====================================
________________ ERROR collecting tests/test_stablediffusion.py ________________
tests/test_stablediffusion.py:222: in <module>
    class TestStableDiffusionModel(PeftCommonTester):
tests/test_stablediffusion.py:228: in TestStableDiffusionModel
    sd_model = StableDiffusionPipeline.from_pretrained("hf-internal-testing/tiny-sd-pipe")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'
------------------------------- Captured stderr --------------------------------
Fetching 12 files:   0%|                                | 0/12 [00:00<?, ?it/s]Fetching 12 files:  17%|████                    | 2/12 [00:00<00:02,  4.74it/s]Fetching 12 files:  33%|████████                | 4/12 [00:01<00:02,  3.32it/s]Fetching 12 files: 100%|███████████████████████| 12/12 [00:01<00:00, 10.42it/s]
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]
________________ ERROR collecting tests/test_stablediffusion.py ________________
tests/test_stablediffusion.py:222: in <module>
    class TestStableDiffusionModel(PeftCommonTester):
tests/test_stablediffusion.py:228: in TestStableDiffusionModel
    sd_model = StableDiffusionPipeline.from_pretrained("hf-internal-testing/tiny-sd-pipe")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'
------------------------------- Captured stderr --------------------------------
Fetching 12 files:   0%|                                | 0/12 [00:00<?, ?it/s]Fetching 12 files:  17%|████                    | 2/12 [00:00<00:02,  3.94it/s]Fetching 12 files:  33%|████████                | 4/12 [00:01<00:02,  3.17it/s]Fetching 12 files: 100%|███████████████████████| 12/12 [00:01<00:00,  9.78it/s]
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  40%|████▊       | 2/5 [00:00<00:00, 70.06it/s]
________________ ERROR collecting tests/test_stablediffusion.py ________________
tests/test_stablediffusion.py:222: in <module>
    class TestStableDiffusionModel(PeftCommonTester):
tests/test_stablediffusion.py:228: in TestStableDiffusionModel
    sd_model = StableDiffusionPipeline.from_pretrained("hf-internal-testing/tiny-sd-pipe")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'
------------------------------- Captured stderr --------------------------------
Fetching 12 files:   0%|                                | 0/12 [00:00<?, ?it/s]Fetching 12 files:  17%|████                    | 2/12 [00:00<00:02,  4.15it/s]Fetching 12 files:  33%|████████                | 4/12 [00:01<00:02,  3.44it/s]Fetching 12 files: 100%|███████████████████████| 12/12 [00:01<00:00, 10.57it/s]
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  40%|████▊       | 2/5 [00:00<00:00, 16.59it/s]Loading pipeline components...:  80%|█████████▌  | 4/5 [00:00<00:00, 28.70it/s]
=================================== FAILURES ===================================
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 1 OFT-MLP-OFTConfig-config_kwargs96] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f00633c0690>
test_name = 'Vanilla MLP 1 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'oft_block_size': 0, 'r': 2, 'target_modules': 'lin0', 'use_cayley_neumann': False}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 2, n_elements = 10, block_size = 5
in_features = 10, coft = False, eps = 6e-05, block_share = False
kernel_size = (0, 0), use_cayley_neumann = False, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 2 OFT-MLP-OFTConfig-config_kwargs97] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f00633c0990>
test_name = 'Vanilla MLP 2 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'oft_block_size': 0, 'r': 2, 'target_modules': ['lin0'], 'use_cayley_neumann': False}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 2, n_elements = 10, block_size = 5
in_features = 10, coft = False, eps = 6e-05, block_share = False
kernel_size = (0, 0), use_cayley_neumann = False, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 5 OFT-MLP-OFTConfig-config_kwargs98] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f00633c0c90>
test_name = 'Vanilla MLP 5 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'modules_to_save': ['lin1'], 'oft_block_size': 0, 'r': 2, 'target_modules': ['lin0'], ...}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 2, n_elements = 10, block_size = 5
in_features = 10, coft = False, eps = 6e-05, block_share = False
kernel_size = (0, 0), use_cayley_neumann = False, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 6 OFT-MLP-OFTConfig-config_kwargs99] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f00633c0f90>
test_name = 'Vanilla MLP 6 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'module_dropout': 0.1, 'oft_block_size': 0, 'r': 2, 'target_modules': ['lin0'], ...}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 2, n_elements = 10, block_size = 5
in_features = 10, coft = False, eps = 6e-05, block_share = False
kernel_size = (0, 0), use_cayley_neumann = False, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 7 OFT-MLP-OFTConfig-config_kwargs100] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f00633c1290>
test_name = 'Vanilla MLP 7 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'coft': True, 'eps': 0.01, 'oft_block_size': 0, 'r': 2, ...}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 2, n_elements = 10, block_size = 5
in_features = 10, coft = True, eps = 0.01, block_share = False
kernel_size = (0, 0), use_cayley_neumann = True, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 8 OFT-MLP-OFTConfig-config_kwargs101] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f00633c1590>
test_name = 'Vanilla MLP 8 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'block_share': True, 'oft_block_size': 0, 'r': 2, 'target_modules': ['lin0'], ...}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 1, n_elements = 10, block_size = 5
in_features = 10, coft = False, eps = 6e-05, block_share = True
kernel_size = (0, 0), use_cayley_neumann = False, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 9 OFT-MLP-OFTConfig-config_kwargs102] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f00633c1890>
test_name = 'Vanilla MLP 9 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'block_share': True, 'coft': True, 'eps': 0.01, 'oft_block_size': 0, ...}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 1, n_elements = 10, block_size = 5
in_features = 10, coft = True, eps = 0.01, block_share = True
kernel_size = (0, 0), use_cayley_neumann = True, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 10 OFT-MLP-OFTConfig-config_kwargs103] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f00633c1b90>
test_name = 'Vanilla MLP 10 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'oft_block_size': 2, 'r': 0, 'target_modules': ['lin0'], 'use_cayley_neumann': True}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 5, n_elements = 1, block_size = 2
in_features = 10, coft = False, eps = 6e-05, block_share = False
kernel_size = (0, 0), use_cayley_neumann = True, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 11 OFT-MLP-OFTConfig-config_kwargs104] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f00633c1e90>
test_name = 'Vanilla MLP 11 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'oft_block_size': 2, 'r': 0, 'target_modules': ['lin0'], 'use_cayley_neumann': False}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 5, n_elements = 1, block_size = 2
in_features = 10, coft = False, eps = 6e-05, block_share = False
kernel_size = (0, 0), use_cayley_neumann = False, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 12 OFT-MLP-OFTConfig-config_kwargs105] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f00633c2190>
test_name = 'Vanilla MLP 12 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'block_share': True, 'coft': True, 'eps': 0.01, 'oft_block_size': 2, ...}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 1, n_elements = 1, block_size = 2
in_features = 10, coft = True, eps = 0.01, block_share = True
kernel_size = (0, 0), use_cayley_neumann = True, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Vanilla MLP 13 OFT-MLP-OFTConfig-config_kwargs106] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f00633c2490>
test_name = 'Vanilla MLP 13 OFT', model_id = 'MLP'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'block_share': True, 'coft': True, 'eps': 0.01, 'oft_block_size': 2, ...}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:944: in dispatch_default
    new_module = Linear(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:555: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:469: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 1, n_elements = 1, block_size = 2
in_features = 10, coft = True, eps = 0.01, block_share = True
kernel_size = (0, 0), use_cayley_neumann = False, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Conv2d 1 OFT-Conv2d-OFTConfig-config_kwargs107] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f00633c2790>
test_name = 'Conv2d 1 OFT', model_id = 'Conv2d'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'oft_block_size': 0, 'r': 5, 'target_modules': ['conv2d']}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:936: in dispatch_default
    new_module = Conv2d(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:703: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:769: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 5, n_elements = 36, block_size = 9
in_features = 45, coft = False, eps = 6e-05, block_share = False
kernel_size = (3, 3), use_cayley_neumann = True, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Conv2d 3 OFT-Conv2d-OFTConfig-config_kwargs108] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f00633c2a90>
test_name = 'Conv2d 3 OFT', model_id = 'Conv2d'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'coft': True, 'oft_block_size': 0, 'r': 5, 'target_modules': ['conv2d']}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:936: in dispatch_default
    new_module = Conv2d(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:703: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:769: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 5, n_elements = 36, block_size = 9
in_features = 45, coft = True, eps = 6e-05, block_share = False
kernel_size = (3, 3), use_cayley_neumann = True, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Conv2d 4 OFT-Conv2d-OFTConfig-config_kwargs109] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f00633c2d90>
test_name = 'Conv2d 4 OFT', model_id = 'Conv2d'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'block_share': True, 'oft_block_size': 0, 'r': 5, 'target_modules': ['conv2d']}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:936: in dispatch_default
    new_module = Conv2d(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:703: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:769: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 1, n_elements = 36, block_size = 9
in_features = 45, coft = False, eps = 6e-05, block_share = True
kernel_size = (3, 3), use_cayley_neumann = True, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_load_model_low_cpu_mem_usage[Conv2d 5 OFT-Conv2d-OFTConfig-config_kwargs110] _
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7f00633c3090>
test_name = 'Conv2d 5 OFT', model_id = 'Conv2d'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'block_share': True, 'coft': True, 'oft_block_size': 0, 'r': 5, ...}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_load_model_low_cpu_mem_usage(self, test_name, model_id, config_cls, config_kwargs):
        _skip_tests_with_multiple_adapters_with_target_parameters(config_cls, config_kwargs)
>       self._test_load_model_low_cpu_mem_usage(model_id, config_cls, config_kwargs)

tests/test_custom_models.py:1560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/testing_common.py:361: in _test_load_model_low_cpu_mem_usage
    model = PeftModel.from_pretrained(
src/peft/peft_model.py:541: in from_pretrained
    model = cls(
src/peft/peft_model.py:132: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/oft/model.py:127: in _create_and_replace
    new_module = self._create_new_module(oft_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/model.py:177: in _create_new_module
    new_module = dispatcher(target, adapter_name, oft_config=oft_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:936: in dispatch_default
    new_module = Conv2d(target, adapter_name, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/oft/layer.py:703: in __init__
    self.update_layer(
src/peft/tuners/oft/layer.py:769: in update_layer
    self.oft_R[adapter_name] = OFTRotationModule(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OFTRotationModule(), r = 1, n_elements = 36, block_size = 9
in_features = 45, coft = True, eps = 6e-05, block_share = True
kernel_size = (3, 3), use_cayley_neumann = True, num_cayley_neumann_terms = 5
device = device(type='cpu')

    def __init__(
        self,
        r,
        n_elements,
        block_size,
        in_features,
        coft=False,
        eps=6e-5,
        block_share=False,
        kernel_size=(0, 0),
        use_cayley_neumann=True,
        num_cayley_neumann_terms=5,
        device=None,
    ):
        super().__init__()
        self.r = r
        self.n_elements = n_elements
        self.block_size = block_size
        self.in_features = in_features
        self.weight = nn.Parameter(torch.empty(r, n_elements))
>       self.weight = self.weight.to(device)
                      ^^^^^^^^^^^^^^^^^^^^^^
E       NotImplementedError: Cannot copy out of meta tensor; no data!

src/peft/tuners/oft/layer.py:93: NotImplementedError
_ TestPeftCustomModel.test_disable_adapters_with_merging[Conv2d 1 OFT-Conv2d-OFTConfig-config_kwargs107] _
[gw2] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7fa21d407110>
test_name = 'Conv2d 1 OFT', model_id = 'Conv2d'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'oft_block_size': 0, 'r': 5, 'target_modules': ['conv2d']}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_disable_adapters_with_merging(self, test_name, model_id, config_cls, config_kwargs):
        # Same test as test_disable_adapters, but additionally merge the trained adapter.

        # https://github.com/huggingface/peft/pull/2403
        if model_id in ["Conv2dGroups", "Conv2dGroups2"]:
            pytest.skip(
                f"Skipping test for {model_id} as merging is not supported. (See https://github.com/huggingface/peft/pull/2403 for details)"
            )

        # same as test_disable_adapters, but with merging
        X = self.prepare_inputs_for_testing()
        model = self.transformers_class.from_pretrained(model_id).to(self.torch_device)
        config = config_cls(
            base_model_name_or_path=model_id,
            **config_kwargs,
        )
        model = get_peft_model(model, config)
        if issubclass(config_cls, VBLoRAConfig):
            # Manually set the `vblora_vector_bank` to zero so that VB-LoRA functions as an identity operation.
            torch.nn.init.zeros_(model.vblora_vector_bank["default"])
        model.eval()
        outputs_before = model(**X)

        if issubclass(config_cls, VBLoRAConfig):
            # initialize `vblora_vector_bank` so it can be trained
            model._init_vblora_vector_bank(config, "default")
        model.train()
        if isinstance(config_cls, LNTuningConfig):
            # LayerNorm tuning is slow to learn
            lr = 1.0
            optimizer = torch.optim.SGD(model.parameters(), lr=lr)
        else:
            # Adam optimizer since SGD isn't great for small models with IA3 + Conv1D
            lr = 0.01
            optimizer = torch.optim.Adam(model.parameters(), lr=lr)

        # train at least 3 steps for all parameters to be updated (probably this is required because of symmetry
        # breaking of some LoRA layers that are initialized with constants)
        for _ in range(3):
            optimizer.zero_grad()
            y_pred = model(**X)
            y = torch.arange(len(y_pred)).to(self.torch_device) % 2
            loss = nn.functional.nll_loss(y_pred, y)
            loss.backward()
            optimizer.step()

        model.eval()
        outputs_unmerged = model(**X)
        model.merge_adapter()
        outputs_after = model(**X)

        with model.disable_adapter():
            outputs_disabled = model(**X)

        # check that after leaving the disable_adapter context, everything is enabled again
        outputs_enabled_after_disable = model(**X)

        atol, rtol = 1e-5, 1e-5  # tolerances higher than defaults since merging introduces some numerical instability

        conv_ids = ["Conv2d", "Conv3d", "Conv2d2"]
        if issubclass(config_cls, (IA3Config, LoraConfig)) and model_id in conv_ids:  # more instability with Conv
            atol, rtol = 1e-3, 1e-3

        if issubclass(config_cls, OFTConfig):
            atol, rtol = 1e-4, 1e-4

        if config_kwargs.get("use_dora") and model_id == "EmbConv1D":
            atol, rtol = 1e-4, 1e-4

        # check that there is a difference in results after training
        assert not torch.allclose(outputs_before, outputs_after, atol=atol, rtol=rtol)

        if self.torch_device in ["mlu"] and model_id in conv_ids:
            atol, rtol = 1e-3, 1e-2  # MLU

        # unmerged or merged should make no difference
        assert torch.allclose(outputs_after, outputs_unmerged, atol=atol, rtol=rtol)

        # check that disabling adapters gives the same results as before training
>       assert torch.allclose(outputs_before, outputs_disabled, atol=atol, rtol=rtol)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x7fa472ac94c0>(tensor([[-1.1555e+01, -9.5367e-06],\n        [-4.0945e+01,  0.0000e+00]], grad_fn=<LogSoftmaxBackward0>), tensor([[-1.1542e+01, -9.7751e-06],\n        [-4.0898e+01,  0.0000e+00]]), atol=0.0001, rtol=0.0001)
E        +    where <built-in method allclose of type object at 0x7fa472ac94c0> = torch.allclose

tests/test_custom_models.py:2101: AssertionError
_ TestPeftCustomModel.test_disable_adapters_with_merging[Conv2d 4 OFT-Conv2d-OFTConfig-config_kwargs109] _
[gw2] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_custom_models.TestPeftCustomModel object at 0x7fa21d407710>
test_name = 'Conv2d 4 OFT', model_id = 'Conv2d'
config_cls = <class 'peft.tuners.oft.config.OFTConfig'>
config_kwargs = {'block_share': True, 'oft_block_size': 0, 'r': 5, 'target_modules': ['conv2d']}

    @pytest.mark.parametrize("test_name, model_id, config_cls, config_kwargs", TEST_CASES)
    def test_disable_adapters_with_merging(self, test_name, model_id, config_cls, config_kwargs):
        # Same test as test_disable_adapters, but additionally merge the trained adapter.

        # https://github.com/huggingface/peft/pull/2403
        if model_id in ["Conv2dGroups", "Conv2dGroups2"]:
            pytest.skip(
                f"Skipping test for {model_id} as merging is not supported. (See https://github.com/huggingface/peft/pull/2403 for details)"
            )

        # same as test_disable_adapters, but with merging
        X = self.prepare_inputs_for_testing()
        model = self.transformers_class.from_pretrained(model_id).to(self.torch_device)
        config = config_cls(
            base_model_name_or_path=model_id,
            **config_kwargs,
        )
        model = get_peft_model(model, config)
        if issubclass(config_cls, VBLoRAConfig):
            # Manually set the `vblora_vector_bank` to zero so that VB-LoRA functions as an identity operation.
            torch.nn.init.zeros_(model.vblora_vector_bank["default"])
        model.eval()
        outputs_before = model(**X)

        if issubclass(config_cls, VBLoRAConfig):
            # initialize `vblora_vector_bank` so it can be trained
            model._init_vblora_vector_bank(config, "default")
        model.train()
        if isinstance(config_cls, LNTuningConfig):
            # LayerNorm tuning is slow to learn
            lr = 1.0
            optimizer = torch.optim.SGD(model.parameters(), lr=lr)
        else:
            # Adam optimizer since SGD isn't great for small models with IA3 + Conv1D
            lr = 0.01
            optimizer = torch.optim.Adam(model.parameters(), lr=lr)

        # train at least 3 steps for all parameters to be updated (probably this is required because of symmetry
        # breaking of some LoRA layers that are initialized with constants)
        for _ in range(3):
            optimizer.zero_grad()
            y_pred = model(**X)
            y = torch.arange(len(y_pred)).to(self.torch_device) % 2
            loss = nn.functional.nll_loss(y_pred, y)
            loss.backward()
            optimizer.step()

        model.eval()
        outputs_unmerged = model(**X)
        model.merge_adapter()
        outputs_after = model(**X)

        with model.disable_adapter():
            outputs_disabled = model(**X)

        # check that after leaving the disable_adapter context, everything is enabled again
        outputs_enabled_after_disable = model(**X)

        atol, rtol = 1e-5, 1e-5  # tolerances higher than defaults since merging introduces some numerical instability

        conv_ids = ["Conv2d", "Conv3d", "Conv2d2"]
        if issubclass(config_cls, (IA3Config, LoraConfig)) and model_id in conv_ids:  # more instability with Conv
            atol, rtol = 1e-3, 1e-3

        if issubclass(config_cls, OFTConfig):
            atol, rtol = 1e-4, 1e-4

        if config_kwargs.get("use_dora") and model_id == "EmbConv1D":
            atol, rtol = 1e-4, 1e-4

        # check that there is a difference in results after training
        assert not torch.allclose(outputs_before, outputs_after, atol=atol, rtol=rtol)

        if self.torch_device in ["mlu"] and model_id in conv_ids:
            atol, rtol = 1e-3, 1e-2  # MLU

        # unmerged or merged should make no difference
        assert torch.allclose(outputs_after, outputs_unmerged, atol=atol, rtol=rtol)

        # check that disabling adapters gives the same results as before training
>       assert torch.allclose(outputs_before, outputs_disabled, atol=atol, rtol=rtol)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x7fa472ac94c0>(tensor([[-1.1555e+01, -9.5367e-06],\n        [-4.0945e+01,  0.0000e+00]], grad_fn=<LogSoftmaxBackward0>), tensor([[-1.1547e+01, -9.6559e-06],\n        [-4.0910e+01,  0.0000e+00]]), atol=0.0001, rtol=0.0001)
E        +    where <built-in method allclose of type object at 0x7fa472ac94c0> = torch.allclose

tests/test_custom_models.py:2101: AssertionError
___________ TestOft.test_load_outdated_oft_checkpoint_warns[0.17.0] ____________
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_initialization.TestOft object at 0x7f005f66ff90>
peft_version = '0.17.0'
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/popen-gw1/test_load_outdated_oft_checkpo0')
recwarn = WarningsRecorder(record=True)

    @pytest.mark.parametrize("peft_version", ["0.17.0", "0.18.0", None])
    def test_load_outdated_oft_checkpoint_warns(self, peft_version, tmp_path, recwarn):
        # In PEFT v0.18.0, there was a small change in the OFT implementation with Cayley-Neumann enabled. As the
        # outputs change slightly, users need to be warned about it if the checkpoint stems from a PEFT version below
        # 0.18.0. When the 'peft_version' key is not in the config, it means that the version is below 0.18.0.
        config = OFTConfig(target_modules=["lin"], use_cayley_neumann=True)  # only relevant when using Cayley-Neumann
        model = get_peft_model(self.get_model(), config)
        model.save_pretrained(tmp_path)
        del model

        # overwrite the peft_version
        with open(tmp_path / "adapter_config.json") as f:
            config_json = json.load(f)

        if peft_version is None:
            del config_json["peft_version"]
        else:
            config_json["peft_version"] = peft_version

        with open(tmp_path / "adapter_config.json", "w") as f:
            json.dump(config_json, f)

        msg = "TODO"  # <= replace with final warning message
        PeftModel.from_pretrained(self.get_model(), tmp_path)

        warn_messages = [str(w.message) for w in recwarn.list]
        if peft_version == "0.18.0":
            assert not any(w.startswith(msg) for w in warn_messages)
        else:
>           assert any(w.startswith(msg) for w in warn_messages)
E           assert False
E            +  where False = any(<generator object TestOft.test_load_outdated_oft_checkpoint_warns.<locals>.<genexpr> at 0x7f0047b827a0>)

tests/test_initialization.py:1801: AssertionError
____________ TestOft.test_load_outdated_oft_checkpoint_warns[None] _____________
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_initialization.TestOft object at 0x7f005f678a10>
peft_version = None
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/popen-gw1/test_load_outdated_oft_checkpo2')
recwarn = WarningsRecorder(record=True)

    @pytest.mark.parametrize("peft_version", ["0.17.0", "0.18.0", None])
    def test_load_outdated_oft_checkpoint_warns(self, peft_version, tmp_path, recwarn):
        # In PEFT v0.18.0, there was a small change in the OFT implementation with Cayley-Neumann enabled. As the
        # outputs change slightly, users need to be warned about it if the checkpoint stems from a PEFT version below
        # 0.18.0. When the 'peft_version' key is not in the config, it means that the version is below 0.18.0.
        config = OFTConfig(target_modules=["lin"], use_cayley_neumann=True)  # only relevant when using Cayley-Neumann
        model = get_peft_model(self.get_model(), config)
        model.save_pretrained(tmp_path)
        del model

        # overwrite the peft_version
        with open(tmp_path / "adapter_config.json") as f:
            config_json = json.load(f)

        if peft_version is None:
>           del config_json["peft_version"]
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           KeyError: 'peft_version'

tests/test_initialization.py:1787: KeyError
_________________ TestScalingAdapters.test_diffusers_pipeline __________________
[gw0] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_helpers.TestScalingAdapters object at 0x7f6f3bbea8d0>

    def test_diffusers_pipeline(self):
        model_id = "hf-internal-testing/tiny-sd-pipe"
>       pipeline = StableDiffusionPipeline.from_pretrained(model_id)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_helpers.py:185:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'transformers.models.clip.modeling_clip.CLIPTextModel'>
pretrained_model_name_or_path = '/root/.cache/huggingface/hub/models--hf-internal-testing--tiny-sd-pipe/snapshots/39f7e1819d772fbb8dd7f2aa3da8e2fc5a9bd917/text_encoder'
config = CLIPTextConfig {
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dty...en_layers": 5,
  "pad_token_id": 1,
  "projection_dim": 32,
  "transformers_version": "4.57.0",
  "vocab_size": 1000
}

cache_dir = None, ignore_mismatched_sizes = False, force_download = False
local_files_only = False, token = None, revision = 'main'
use_safetensors = None, weights_only = True, model_args = ()
kwargs = {'offload_state_dict': None}, state_dict = None

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        **kwargs,
    ) -> SpecificPreTrainedModelType:
        r"""
        Instantiate a pretrained pytorch model from a pre-trained model configuration.

        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you should first set it back in training mode with `model.train()`.

        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come
        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
        task.

        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those
        weights are discarded.

        Parameters:
            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):
                Can be either:

                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
                    - A path to a *directory* containing model weights saved using
                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In
                      this case, `from_tf` should be set to `True` and a configuration object should be provided as
                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a
                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.
                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,
                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to
                      `True`.
                    - `None` if you are both providing the configuration and state dictionary (resp. with keyword
                      arguments `config` and `state_dict`).
            model_args (sequence of positional arguments, *optional*):
                All remaining positional arguments will be passed to the underlying model's `__init__` method.
            config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):
                Can be either:

                    - an instance of a class derived from [`PretrainedConfig`],
                    - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].

                Configuration for the model to use instead of an automatically loaded configuration. Configuration can
                be automatically loaded when:

                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained
                      model).
                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the
                      save directory.
                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a
                      configuration JSON file named *config.json* is found in the directory.
            state_dict (`dict[str, torch.Tensor]`, *optional*):
                A state dictionary to use instead of a state dictionary loaded from saved weights file.

                This option can be used if you want to create a model from a pretrained configuration but load your own
                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and
                [`~PreTrainedModel.from_pretrained`] is not a simpler option.
            cache_dir (`Union[str, os.PathLike]`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the
                standard cache should not be used.
            from_tf (`bool`, *optional*, defaults to `False`):
                Load the model weights from a TensorFlow checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            from_flax (`bool`, *optional*, defaults to `False`):
                Load the model weights from a Flax checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):
                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size
                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a
                checkpoint with 3 labels).
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
                cached versions if they exist.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
            output_loading_info(`bool`, *optional*, defaults to `False`):
                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.
            local_files_only(`bool`, *optional*, defaults to `False`):
                Whether or not to only look at local files (i.e., do not try to download the model).
            token (`str` or `bool`, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use
                the token generated when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.

                <Tip>

                To test a pull request you made on the Hub, you can pass `revision="refs/pr/<pr_number>"`.

                </Tip>
            attn_implementation (`str`, *optional*):
                The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)), or `"flash_attention_3"` (using [Dao-AILab/flash-attention/hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.

                Accept HF kernel references in the form:
                  <namespace>/<repo_name>[@<revision>][:<kernel_name>]

                - <namespace> and <repo_name> are any non-"/" and non-":" sequences.
                - "@<revision>" is optional (branch, tag, or commit-ish), e.g. "@main", "@v1.2.0", "@abc123".
                - ":<kernel_name>" is optional and selects a function inside the kernel repo.
                - Both options can appear together and in this order only: @revision first, then :kernel_name.
                - We intentionally allow a leading "<wrapper>|" prefix (e.g., "flash|...") because the code
                  strips it before loading; '|' is not excluded in the character classes here.

                Examples that match:
                  "org/model"
                  "org/model@main"
                  "org/model:custom_kernel"
                  "org/model@v1.2.3:custom_kernel"

            > Parameters for big model inference

            dtype (`str` or `torch.dtype`, *optional*):
                Override the default `torch_dtype` and load the model under a specific `dtype`. The different options
                are:

                1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified
                  `dtype`, ignoring the model's `config.dtype` if one exists. If not specified
                  - the model will get loaded in `torch.float` (fp32).

                2. `"auto"` - A `dtype` or `torch_dtype` entry in the `config.json` file of the model will be
                  attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in
                  the checkpoint that's of a floating point type and use that as `dtype`. This will load the model
                  using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how
                  the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.

                3. A string that is a valid `torch.dtype`. E.g. "float32" loads the model in `torch.float32`, "float16" loads in `torch.float16` etc.

                <Tip>

                For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or
                reach out to the authors and ask them to add this information to the model's card and to insert the
                `dtype` or `torch_dtype` entry in `config.json` on the hub.

                </Tip>

            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):
                A map that specifies where each submodule should go. It doesn't need to be refined to each
                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
                same device. If we only pass the device (*e.g.*, `"cpu"`, `"cuda:1"`, `"mps"`, or a GPU ordinal rank
                like `1`) on which the model will be allocated, the device map will map the entire model to this
                device. Passing `device_map = 0` means put the whole model on GPU 0.

                To have Accelerate compute the most optimized `device_map` automatically, set `device_map="auto"`. For
                more information about each option see [designing a device
                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).
            max_memory (`Dict`, *optional*):
                A dictionary device identifier to maximum memory if using `device_map`. Will default to the maximum memory available for each
                GPU and the available CPU RAM if unset.
            tp_plan (`str`, *optional*):
                A torch tensor parallel plan, see [here](https://pytorch.org/tutorials/intermediate/TP_tutorial.html). Currently, it only accepts
                `tp_plan="auto"` to use predefined plan based on the model. Note that if you use it, you should launch your script accordingly with
                `torchrun [args] script.py`. This will be much faster than using a `device_map`, but has limitations.
            tp_size (`str`, *optional*):
                A torch tensor parallel degree. If not provided would default to world size.
            device_mesh (`torch.distributed.DeviceMesh`, *optional*):
                A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.
                If provided, it has to contain dimension named `"tp"` in case it's > 1 dimensional, this dimension will be used for tensor parallelism
            offload_folder (`str` or `os.PathLike`, *optional*):
                If the `device_map` contains any value `"disk"`, the folder where we will offload weights.
            offload_buffers (`bool`, *optional*):
                Whether or not to offload the buffers with the model parameters.
            quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):
                A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g
                bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and
                `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes
                quantizations and not preferred. consider inserting all such arguments into quantization_config
                instead.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            variant (`str`, *optional*):
                If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is
                ignored when using `from_tf` or `from_flax`.
            use_safetensors (`bool`, *optional*, defaults to `None`):
                Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`
                is not installed, it will be set to `False`.
            weights_only (`bool`, *optional*, defaults to `True`):
                Indicates whether unpickler should be restricted to loading only tensors, primitive types,
                dictionaries and any types added via torch.serialization.add_safe_globals().
                When set to False, we can load wrapper tensor subclass weights.
            key_mapping (`dict[str, str], *optional*):
                A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers
                architecture, but was not converted accordingly.
            kwargs (remaining dictionary of keyword arguments, *optional*):
                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or
                automatically loaded:

                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the
                      underlying model's `__init__` method (we assume all relevant updates to the configuration have
                      already been done)
                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class
                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that
                      corresponds to a configuration attribute will be used to override said attribute with the
                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute
                      will be passed to the underlying model's `__init__` function.

        <Tip>

        Activate the special ["offline-mode"](https://huggingface.co/transformers/installation.html#offline-mode) to
        use this method in a firewalled environment.

        </Tip>

        Examples:

        ```python
        >>> from transformers import BertConfig, BertModel

        >>> # Download model and configuration from huggingface.co and cache.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased")
        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
        >>> model = BertModel.from_pretrained("./test/saved_model/")
        >>> # Update configuration during loading.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", output_attentions=True)
        >>> assert model.config.output_attentions == True
        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).
        >>> config = BertConfig.from_json_file("./tf_model/my_tf_model_config.json")
        >>> model = BertModel.from_pretrained("./tf_model/my_tf_checkpoint.ckpt.index", from_tf=True, config=config)
        >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", from_flax=True)
        ```
        """
        state_dict = kwargs.pop("state_dict", None)
        from_tf = kwargs.pop("from_tf", False)
        from_flax = kwargs.pop("from_flax", False)
        proxies = kwargs.pop("proxies", None)
        output_loading_info = kwargs.pop("output_loading_info", False)
        use_auth_token = kwargs.pop("use_auth_token", None)
        from_pipeline = kwargs.pop("_from_pipeline", None)
        from_auto_class = kwargs.pop("_from_auto", False)
        dtype = kwargs.pop("dtype", None)
        torch_dtype = kwargs.pop("torch_dtype", None)  # kept for BC
        device_map = kwargs.pop("device_map", None)
        max_memory = kwargs.pop("max_memory", None)
        offload_folder = kwargs.pop("offload_folder", None)
        offload_buffers = kwargs.pop("offload_buffers", False)
        load_in_8bit = kwargs.pop("load_in_8bit", False)
        load_in_4bit = kwargs.pop("load_in_4bit", False)
        quantization_config = kwargs.pop("quantization_config", None)
        subfolder = kwargs.pop("subfolder", "")
        commit_hash = kwargs.pop("_commit_hash", None)
        variant = kwargs.pop("variant", None)
        adapter_kwargs = kwargs.pop("adapter_kwargs", {})
        adapter_name = kwargs.pop("adapter_name", "default")
        generation_config = kwargs.pop("generation_config", None)
        gguf_file = kwargs.pop("gguf_file", None)
        tp_plan = kwargs.pop("tp_plan", None)
        tp_size = kwargs.pop("tp_size", None)
        distributed_config: DistributedConfig = kwargs.pop("distributed_config", None)
        device_mesh = kwargs.pop("device_mesh", None)
        trust_remote_code = kwargs.pop("trust_remote_code", None)
        use_kernels = kwargs.pop("use_kernels", False)

        key_mapping = kwargs.pop("key_mapping", None)
        # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model
        if key_mapping is None and any(
            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS
        ):
            key_mapping = cls._checkpoint_conversion_mapping

        if distributed_config is not None:
            tp_plan = "auto"

        # Not used anymore -- remove them from the kwargs
        _ = kwargs.pop("resume_download", None)
        _ = kwargs.pop("mirror", None)
        _ = kwargs.pop("_fast_init", True)
        _ = kwargs.pop("low_cpu_mem_usage", None)

        # For BC on torch_dtype argument
        if torch_dtype is not None:
            logger.warning_once("`torch_dtype` is deprecated! Use `dtype` instead!")
            # If both kwargs are provided, use `dtype`
            dtype = dtype if dtype is not None else torch_dtype

        if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):
            raise ValueError(
                "`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies."
            )
        if tp_size is not None and tp_plan is None:
            raise ValueError("tp_plan has to be set when tp_size is passed.")
        if tp_plan is not None and tp_plan != "auto":
            # TODO: we can relax this check when we support taking tp_plan from a json file, for example.
            raise ValueError(f"tp_plan supports 'auto' only for now but got {tp_plan}.")
        if tp_plan is not None and device_map is not None:
            raise ValueError(
                "`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization."
            )

        if device_map == "auto" and int(os.environ.get("WORLD_SIZE", "0")):
            logger.info(
                "You've set device_map=`auto` while triggering a distributed run with torchrun. This might lead to unexpected behavior. "
                "If your plan is to load the model on each device, you should set device_map={"
                ": PartialState().process_index} where PartialState comes from accelerate library"
            )

        # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple
        # `device_map` pointing to the correct device
        if tp_plan is not None:
            if device_mesh is None:
                tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(tp_plan, tp_size=tp_size)
            else:
                if device_mesh.ndim > 1:
                    if "tp" not in device_mesh.mesh_dim_names:
                        raise ValueError(
                            "When using `tp_plan` and n-d `device_mesh`, it must contain a 'tp' dimension. "
                            "Please provide a valid `device_mesh`."
                        )
                    device_mesh = device_mesh["tp"]
                tp_size = device_mesh.size()
                device_map = torch.device(f"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}")

            if tp_size is None:
                tp_size = torch.distributed.get_world_size()

        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError(
                    "`token` and `use_auth_token` are both specified. Please set only the argument `token`."
                )
            token = use_auth_token

        if token is not None and adapter_kwargs is not None and "token" not in adapter_kwargs:
            adapter_kwargs["token"] = token

        if gguf_file is not None and not is_accelerate_available():
            raise ValueError("accelerate is required when loading a GGUF file `pip install accelerate`.")

        if commit_hash is None:
            if not isinstance(config, PretrainedConfig):
                # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible
                resolved_config_file = cached_file(
                    pretrained_model_name_or_path,
                    CONFIG_NAME,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    token=token,
                    revision=revision,
                    subfolder=subfolder,
                    _raise_exceptions_for_gated_repo=False,
                    _raise_exceptions_for_missing_entries=False,
                    _raise_exceptions_for_connection_errors=False,
                )
                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)
            else:
                commit_hash = getattr(config, "_commit_hash", None)

        if is_peft_available():
            _adapter_model_path = adapter_kwargs.pop("_adapter_model_path", None)

            if _adapter_model_path is None:
                _adapter_model_path = find_adapter_config_file(
                    pretrained_model_name_or_path,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    _commit_hash=commit_hash,
                    **adapter_kwargs,
                )
            if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):
                with open(_adapter_model_path, "r", encoding="utf-8") as f:
                    _adapter_model_path = pretrained_model_name_or_path
                    pretrained_model_name_or_path = json.load(f)["base_model_name_or_path"]
        else:
            _adapter_model_path = None

        # Potentially detect context manager or global device, and use it (only if no device_map was provided)
        if device_map is None and not is_deepspeed_zero3_enabled():
            device_in_context = get_torch_context_manager_or_global_device()
            if device_in_context == torch.device("meta"):
                raise RuntimeError(
                    "You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\n"
                    "This is an anti-pattern as `from_pretrained` wants to load existing weights.\nIf you want to initialize an "
                    "empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`"
                )
            device_map = device_in_context

        # change device_map into a map if we passed an int, a str or a torch.device
        if isinstance(device_map, torch.device):
            device_map = {"": device_map}
        elif isinstance(device_map, str) and device_map not in ["auto", "balanced", "balanced_low_0", "sequential"]:
            try:
                device_map = {"": torch.device(device_map)}
            except RuntimeError:
                raise ValueError(
                    "When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or "
                    f"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}."
                )
        elif isinstance(device_map, int):
            if device_map < 0:
                raise ValueError(
                    "You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' "
                )
            else:
                device_map = {"": device_map}

        if device_map is not None:
            if is_deepspeed_zero3_enabled():
                raise ValueError("DeepSpeed Zero-3 is not compatible with passing a `device_map`.")
            if not is_accelerate_available():
                raise ValueError(
                    "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` "
                    "requires `accelerate`. You can install it with `pip install accelerate`"
                )

        # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.
        if load_in_4bit or load_in_8bit:
            if quantization_config is not None:
                raise ValueError(
                    "You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing "
                    "`quantization_config` argument at the same time."
                )

            # preparing BitsAndBytesConfig from kwargs
            config_dict = {k: v for k, v in kwargs.items() if k in inspect.signature(BitsAndBytesConfig).parameters}
            config_dict = {**config_dict, "load_in_4bit": load_in_4bit, "load_in_8bit": load_in_8bit}
            quantization_config, kwargs = BitsAndBytesConfig.from_dict(
                config_dict=config_dict, return_unused_kwargs=True, **kwargs
            )
            logger.warning(
                "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. "
                "Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead."
            )

        from_pt = not (from_tf | from_flax)

        user_agent = {"file_type": "model", "framework": "pytorch", "from_auto_class": from_auto_class}
        if from_pipeline is not None:
            user_agent["using_pipeline"] = from_pipeline

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True

        # Load config if we don't provide a configuration
        if not isinstance(config, PretrainedConfig):
            config_path = config if config is not None else pretrained_model_name_or_path
            config, model_kwargs = cls.config_class.from_pretrained(
                config_path,
                cache_dir=cache_dir,
                return_unused_kwargs=True,
                force_download=force_download,
                proxies=proxies,
                local_files_only=local_files_only,
                token=token,
                revision=revision,
                subfolder=subfolder,
                gguf_file=gguf_file,
                _from_auto=from_auto_class,
                _from_pipeline=from_pipeline,
                **kwargs,
            )
            if "gguf_file" in model_kwargs:
                model_kwargs.pop("gguf_file")
        else:
            config = copy.deepcopy(config)
            model_kwargs = kwargs

        # Because some composite configs call super().__init__ before instantiating the sub-configs, we need this call
        # to correctly redispatch recursively if the kwarg is provided
        if "attn_implementation" in kwargs:
            config._attn_implementation = kwargs.pop("attn_implementation")

        transformers_explicit_filename = getattr(config, "transformers_weights", None)

        if transformers_explicit_filename is not None:
            if not transformers_explicit_filename.endswith(
                ".safetensors"
            ) and not transformers_explicit_filename.endswith(".safetensors.index.json"):
                raise ValueError(
                    "The transformers file in the config seems to be incorrect: it is neither a safetensors file "
                    "(*.safetensors) nor a safetensors index file (*.safetensors.index.json): "
                    f"{transformers_explicit_filename}"
                )

        hf_quantizer, config, dtype, device_map = get_hf_quantizer(
            config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent
        )

        if gguf_file is not None and hf_quantizer is not None:
            raise ValueError(
                "You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub."
            )

        if (
            gguf_file
            and device_map is not None
            and ((isinstance(device_map, dict) and "disk" in device_map.values()) or "disk" in device_map)
        ):
            raise RuntimeError(
                "One or more modules is configured to be mapped to disk. Disk offload is not supported for models "
                "loaded from GGUF files."
            )

        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
            pretrained_model_name_or_path=pretrained_model_name_or_path,
            subfolder=subfolder,
            variant=variant,
            gguf_file=gguf_file,
            from_tf=from_tf,
            from_flax=from_flax,
            use_safetensors=use_safetensors,
            cache_dir=cache_dir,
            force_download=force_download,
            proxies=proxies,
            local_files_only=local_files_only,
            token=token,
            user_agent=user_agent,
            revision=revision,
            commit_hash=commit_hash,
            is_remote_code=cls._auto_class is not None,
            transformers_explicit_filename=transformers_explicit_filename,
        )

        is_sharded = sharded_metadata is not None
        is_quantized = hf_quantizer is not None
        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None

        if is_from_file and not is_sharded and checkpoint_files[0].endswith(".safetensors"):
            with safe_open(checkpoint_files[0], framework="pt") as f:
                metadata = f.metadata()

            if metadata is None:
                # Assume it's a pytorch checkpoint (introduced for timm checkpoints)
                pass
            elif metadata.get("format") == "pt":
                pass
            elif metadata.get("format") == "tf":
                from_tf = True
                logger.info("A TensorFlow safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "flax":
                from_flax = True
                logger.info("A Flax safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "mlx":
                # This is a mlx file, we assume weights are compatible with pt
                pass
            else:
                raise ValueError(
                    f"Incompatible safetensors file. File metadata is not ['pt', 'tf', 'flax', 'mlx'] but {metadata.get('format')}"
                )

        from_pt = not (from_tf | from_flax)

        if from_pt:
            if gguf_file:
                from .modeling_gguf_pytorch_utils import load_gguf_checkpoint

                # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was
                # passed directly as a kwarg from now on
                with torch.device("meta"):
                    dummy_model = cls(config)
                state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[
                    "tensors"
                ]

            # Find the correct dtype based on current state
            config, dtype, dtype_orig = _get_dtype(
                cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only
            )

        config.name_or_path = pretrained_model_name_or_path
        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)
        config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.
        with ContextManagers(model_init_context):
            # Let's make sure we don't run the init function of buffer modules
>           model = cls(config, *model_args, **model_kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'

venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: TypeError
----------------------------- Captured stderr call -----------------------------
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  40%|████▍      | 2/5 [00:00<00:00, 144.90it/s]
_ TestInjectAdapterFromStateDict.test_inject_from_state_dict_stable_diffusion __
[gw2] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_low_level_api.TestInjectAdapterFromStateDict object at 0x7fa21a57ff50>

    def test_inject_from_state_dict_stable_diffusion(self):
        # same test as above, but with stable diffusion model and only testing LoRA
        model_id = "hf-internal-testing/tiny-sd-pipe"
        config_text_encoder = LoraConfig(target_modules=["k_proj", "q_proj", "v_proj", "out_proj", "fc1", "fc2"])
        config_unet = LoraConfig(
            target_modules=[
                "proj_in",
                "proj_out",
                "to_k",
                "to_q",
                "to_v",
                "to_out.0",
                "ff.net.0.proj",
                "ff.net.2",
            ]
        )
        with hub_online_once(model_id):
>           pipe = StableDiffusionPipeline.from_pretrained(model_id)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_low_level_api.py:283:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'transformers.models.clip.modeling_clip.CLIPTextModel'>
pretrained_model_name_or_path = '/root/.cache/huggingface/hub/models--hf-internal-testing--tiny-sd-pipe/snapshots/39f7e1819d772fbb8dd7f2aa3da8e2fc5a9bd917/text_encoder'
config = CLIPTextConfig {
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dty...en_layers": 5,
  "pad_token_id": 1,
  "projection_dim": 32,
  "transformers_version": "4.57.0",
  "vocab_size": 1000
}

cache_dir = None, ignore_mismatched_sizes = False, force_download = False
local_files_only = False, token = None, revision = 'main'
use_safetensors = None, weights_only = True, model_args = ()
kwargs = {'offload_state_dict': None}, state_dict = None

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        **kwargs,
    ) -> SpecificPreTrainedModelType:
        r"""
        Instantiate a pretrained pytorch model from a pre-trained model configuration.

        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you should first set it back in training mode with `model.train()`.

        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come
        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
        task.

        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those
        weights are discarded.

        Parameters:
            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):
                Can be either:

                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
                    - A path to a *directory* containing model weights saved using
                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In
                      this case, `from_tf` should be set to `True` and a configuration object should be provided as
                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a
                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.
                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,
                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to
                      `True`.
                    - `None` if you are both providing the configuration and state dictionary (resp. with keyword
                      arguments `config` and `state_dict`).
            model_args (sequence of positional arguments, *optional*):
                All remaining positional arguments will be passed to the underlying model's `__init__` method.
            config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):
                Can be either:

                    - an instance of a class derived from [`PretrainedConfig`],
                    - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].

                Configuration for the model to use instead of an automatically loaded configuration. Configuration can
                be automatically loaded when:

                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained
                      model).
                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the
                      save directory.
                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a
                      configuration JSON file named *config.json* is found in the directory.
            state_dict (`dict[str, torch.Tensor]`, *optional*):
                A state dictionary to use instead of a state dictionary loaded from saved weights file.

                This option can be used if you want to create a model from a pretrained configuration but load your own
                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and
                [`~PreTrainedModel.from_pretrained`] is not a simpler option.
            cache_dir (`Union[str, os.PathLike]`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the
                standard cache should not be used.
            from_tf (`bool`, *optional*, defaults to `False`):
                Load the model weights from a TensorFlow checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            from_flax (`bool`, *optional*, defaults to `False`):
                Load the model weights from a Flax checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):
                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size
                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a
                checkpoint with 3 labels).
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
                cached versions if they exist.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
            output_loading_info(`bool`, *optional*, defaults to `False`):
                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.
            local_files_only(`bool`, *optional*, defaults to `False`):
                Whether or not to only look at local files (i.e., do not try to download the model).
            token (`str` or `bool`, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use
                the token generated when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.

                <Tip>

                To test a pull request you made on the Hub, you can pass `revision="refs/pr/<pr_number>"`.

                </Tip>
            attn_implementation (`str`, *optional*):
                The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)), or `"flash_attention_3"` (using [Dao-AILab/flash-attention/hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.

                Accept HF kernel references in the form:
                  <namespace>/<repo_name>[@<revision>][:<kernel_name>]

                - <namespace> and <repo_name> are any non-"/" and non-":" sequences.
                - "@<revision>" is optional (branch, tag, or commit-ish), e.g. "@main", "@v1.2.0", "@abc123".
                - ":<kernel_name>" is optional and selects a function inside the kernel repo.
                - Both options can appear together and in this order only: @revision first, then :kernel_name.
                - We intentionally allow a leading "<wrapper>|" prefix (e.g., "flash|...") because the code
                  strips it before loading; '|' is not excluded in the character classes here.

                Examples that match:
                  "org/model"
                  "org/model@main"
                  "org/model:custom_kernel"
                  "org/model@v1.2.3:custom_kernel"

            > Parameters for big model inference

            dtype (`str` or `torch.dtype`, *optional*):
                Override the default `torch_dtype` and load the model under a specific `dtype`. The different options
                are:

                1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified
                  `dtype`, ignoring the model's `config.dtype` if one exists. If not specified
                  - the model will get loaded in `torch.float` (fp32).

                2. `"auto"` - A `dtype` or `torch_dtype` entry in the `config.json` file of the model will be
                  attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in
                  the checkpoint that's of a floating point type and use that as `dtype`. This will load the model
                  using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how
                  the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.

                3. A string that is a valid `torch.dtype`. E.g. "float32" loads the model in `torch.float32`, "float16" loads in `torch.float16` etc.

                <Tip>

                For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or
                reach out to the authors and ask them to add this information to the model's card and to insert the
                `dtype` or `torch_dtype` entry in `config.json` on the hub.

                </Tip>

            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):
                A map that specifies where each submodule should go. It doesn't need to be refined to each
                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
                same device. If we only pass the device (*e.g.*, `"cpu"`, `"cuda:1"`, `"mps"`, or a GPU ordinal rank
                like `1`) on which the model will be allocated, the device map will map the entire model to this
                device. Passing `device_map = 0` means put the whole model on GPU 0.

                To have Accelerate compute the most optimized `device_map` automatically, set `device_map="auto"`. For
                more information about each option see [designing a device
                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).
            max_memory (`Dict`, *optional*):
                A dictionary device identifier to maximum memory if using `device_map`. Will default to the maximum memory available for each
                GPU and the available CPU RAM if unset.
            tp_plan (`str`, *optional*):
                A torch tensor parallel plan, see [here](https://pytorch.org/tutorials/intermediate/TP_tutorial.html). Currently, it only accepts
                `tp_plan="auto"` to use predefined plan based on the model. Note that if you use it, you should launch your script accordingly with
                `torchrun [args] script.py`. This will be much faster than using a `device_map`, but has limitations.
            tp_size (`str`, *optional*):
                A torch tensor parallel degree. If not provided would default to world size.
            device_mesh (`torch.distributed.DeviceMesh`, *optional*):
                A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.
                If provided, it has to contain dimension named `"tp"` in case it's > 1 dimensional, this dimension will be used for tensor parallelism
            offload_folder (`str` or `os.PathLike`, *optional*):
                If the `device_map` contains any value `"disk"`, the folder where we will offload weights.
            offload_buffers (`bool`, *optional*):
                Whether or not to offload the buffers with the model parameters.
            quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):
                A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g
                bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and
                `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes
                quantizations and not preferred. consider inserting all such arguments into quantization_config
                instead.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            variant (`str`, *optional*):
                If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is
                ignored when using `from_tf` or `from_flax`.
            use_safetensors (`bool`, *optional*, defaults to `None`):
                Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`
                is not installed, it will be set to `False`.
            weights_only (`bool`, *optional*, defaults to `True`):
                Indicates whether unpickler should be restricted to loading only tensors, primitive types,
                dictionaries and any types added via torch.serialization.add_safe_globals().
                When set to False, we can load wrapper tensor subclass weights.
            key_mapping (`dict[str, str], *optional*):
                A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers
                architecture, but was not converted accordingly.
            kwargs (remaining dictionary of keyword arguments, *optional*):
                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or
                automatically loaded:

                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the
                      underlying model's `__init__` method (we assume all relevant updates to the configuration have
                      already been done)
                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class
                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that
                      corresponds to a configuration attribute will be used to override said attribute with the
                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute
                      will be passed to the underlying model's `__init__` function.

        <Tip>

        Activate the special ["offline-mode"](https://huggingface.co/transformers/installation.html#offline-mode) to
        use this method in a firewalled environment.

        </Tip>

        Examples:

        ```python
        >>> from transformers import BertConfig, BertModel

        >>> # Download model and configuration from huggingface.co and cache.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased")
        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
        >>> model = BertModel.from_pretrained("./test/saved_model/")
        >>> # Update configuration during loading.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", output_attentions=True)
        >>> assert model.config.output_attentions == True
        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).
        >>> config = BertConfig.from_json_file("./tf_model/my_tf_model_config.json")
        >>> model = BertModel.from_pretrained("./tf_model/my_tf_checkpoint.ckpt.index", from_tf=True, config=config)
        >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", from_flax=True)
        ```
        """
        state_dict = kwargs.pop("state_dict", None)
        from_tf = kwargs.pop("from_tf", False)
        from_flax = kwargs.pop("from_flax", False)
        proxies = kwargs.pop("proxies", None)
        output_loading_info = kwargs.pop("output_loading_info", False)
        use_auth_token = kwargs.pop("use_auth_token", None)
        from_pipeline = kwargs.pop("_from_pipeline", None)
        from_auto_class = kwargs.pop("_from_auto", False)
        dtype = kwargs.pop("dtype", None)
        torch_dtype = kwargs.pop("torch_dtype", None)  # kept for BC
        device_map = kwargs.pop("device_map", None)
        max_memory = kwargs.pop("max_memory", None)
        offload_folder = kwargs.pop("offload_folder", None)
        offload_buffers = kwargs.pop("offload_buffers", False)
        load_in_8bit = kwargs.pop("load_in_8bit", False)
        load_in_4bit = kwargs.pop("load_in_4bit", False)
        quantization_config = kwargs.pop("quantization_config", None)
        subfolder = kwargs.pop("subfolder", "")
        commit_hash = kwargs.pop("_commit_hash", None)
        variant = kwargs.pop("variant", None)
        adapter_kwargs = kwargs.pop("adapter_kwargs", {})
        adapter_name = kwargs.pop("adapter_name", "default")
        generation_config = kwargs.pop("generation_config", None)
        gguf_file = kwargs.pop("gguf_file", None)
        tp_plan = kwargs.pop("tp_plan", None)
        tp_size = kwargs.pop("tp_size", None)
        distributed_config: DistributedConfig = kwargs.pop("distributed_config", None)
        device_mesh = kwargs.pop("device_mesh", None)
        trust_remote_code = kwargs.pop("trust_remote_code", None)
        use_kernels = kwargs.pop("use_kernels", False)

        key_mapping = kwargs.pop("key_mapping", None)
        # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model
        if key_mapping is None and any(
            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS
        ):
            key_mapping = cls._checkpoint_conversion_mapping

        if distributed_config is not None:
            tp_plan = "auto"

        # Not used anymore -- remove them from the kwargs
        _ = kwargs.pop("resume_download", None)
        _ = kwargs.pop("mirror", None)
        _ = kwargs.pop("_fast_init", True)
        _ = kwargs.pop("low_cpu_mem_usage", None)

        # For BC on torch_dtype argument
        if torch_dtype is not None:
            logger.warning_once("`torch_dtype` is deprecated! Use `dtype` instead!")
            # If both kwargs are provided, use `dtype`
            dtype = dtype if dtype is not None else torch_dtype

        if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):
            raise ValueError(
                "`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies."
            )
        if tp_size is not None and tp_plan is None:
            raise ValueError("tp_plan has to be set when tp_size is passed.")
        if tp_plan is not None and tp_plan != "auto":
            # TODO: we can relax this check when we support taking tp_plan from a json file, for example.
            raise ValueError(f"tp_plan supports 'auto' only for now but got {tp_plan}.")
        if tp_plan is not None and device_map is not None:
            raise ValueError(
                "`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization."
            )

        if device_map == "auto" and int(os.environ.get("WORLD_SIZE", "0")):
            logger.info(
                "You've set device_map=`auto` while triggering a distributed run with torchrun. This might lead to unexpected behavior. "
                "If your plan is to load the model on each device, you should set device_map={"
                ": PartialState().process_index} where PartialState comes from accelerate library"
            )

        # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple
        # `device_map` pointing to the correct device
        if tp_plan is not None:
            if device_mesh is None:
                tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(tp_plan, tp_size=tp_size)
            else:
                if device_mesh.ndim > 1:
                    if "tp" not in device_mesh.mesh_dim_names:
                        raise ValueError(
                            "When using `tp_plan` and n-d `device_mesh`, it must contain a 'tp' dimension. "
                            "Please provide a valid `device_mesh`."
                        )
                    device_mesh = device_mesh["tp"]
                tp_size = device_mesh.size()
                device_map = torch.device(f"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}")

            if tp_size is None:
                tp_size = torch.distributed.get_world_size()

        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError(
                    "`token` and `use_auth_token` are both specified. Please set only the argument `token`."
                )
            token = use_auth_token

        if token is not None and adapter_kwargs is not None and "token" not in adapter_kwargs:
            adapter_kwargs["token"] = token

        if gguf_file is not None and not is_accelerate_available():
            raise ValueError("accelerate is required when loading a GGUF file `pip install accelerate`.")

        if commit_hash is None:
            if not isinstance(config, PretrainedConfig):
                # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible
                resolved_config_file = cached_file(
                    pretrained_model_name_or_path,
                    CONFIG_NAME,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    token=token,
                    revision=revision,
                    subfolder=subfolder,
                    _raise_exceptions_for_gated_repo=False,
                    _raise_exceptions_for_missing_entries=False,
                    _raise_exceptions_for_connection_errors=False,
                )
                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)
            else:
                commit_hash = getattr(config, "_commit_hash", None)

        if is_peft_available():
            _adapter_model_path = adapter_kwargs.pop("_adapter_model_path", None)

            if _adapter_model_path is None:
                _adapter_model_path = find_adapter_config_file(
                    pretrained_model_name_or_path,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    _commit_hash=commit_hash,
                    **adapter_kwargs,
                )
            if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):
                with open(_adapter_model_path, "r", encoding="utf-8") as f:
                    _adapter_model_path = pretrained_model_name_or_path
                    pretrained_model_name_or_path = json.load(f)["base_model_name_or_path"]
        else:
            _adapter_model_path = None

        # Potentially detect context manager or global device, and use it (only if no device_map was provided)
        if device_map is None and not is_deepspeed_zero3_enabled():
            device_in_context = get_torch_context_manager_or_global_device()
            if device_in_context == torch.device("meta"):
                raise RuntimeError(
                    "You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\n"
                    "This is an anti-pattern as `from_pretrained` wants to load existing weights.\nIf you want to initialize an "
                    "empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`"
                )
            device_map = device_in_context

        # change device_map into a map if we passed an int, a str or a torch.device
        if isinstance(device_map, torch.device):
            device_map = {"": device_map}
        elif isinstance(device_map, str) and device_map not in ["auto", "balanced", "balanced_low_0", "sequential"]:
            try:
                device_map = {"": torch.device(device_map)}
            except RuntimeError:
                raise ValueError(
                    "When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or "
                    f"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}."
                )
        elif isinstance(device_map, int):
            if device_map < 0:
                raise ValueError(
                    "You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' "
                )
            else:
                device_map = {"": device_map}

        if device_map is not None:
            if is_deepspeed_zero3_enabled():
                raise ValueError("DeepSpeed Zero-3 is not compatible with passing a `device_map`.")
            if not is_accelerate_available():
                raise ValueError(
                    "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` "
                    "requires `accelerate`. You can install it with `pip install accelerate`"
                )

        # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.
        if load_in_4bit or load_in_8bit:
            if quantization_config is not None:
                raise ValueError(
                    "You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing "
                    "`quantization_config` argument at the same time."
                )

            # preparing BitsAndBytesConfig from kwargs
            config_dict = {k: v for k, v in kwargs.items() if k in inspect.signature(BitsAndBytesConfig).parameters}
            config_dict = {**config_dict, "load_in_4bit": load_in_4bit, "load_in_8bit": load_in_8bit}
            quantization_config, kwargs = BitsAndBytesConfig.from_dict(
                config_dict=config_dict, return_unused_kwargs=True, **kwargs
            )
            logger.warning(
                "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. "
                "Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead."
            )

        from_pt = not (from_tf | from_flax)

        user_agent = {"file_type": "model", "framework": "pytorch", "from_auto_class": from_auto_class}
        if from_pipeline is not None:
            user_agent["using_pipeline"] = from_pipeline

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True

        # Load config if we don't provide a configuration
        if not isinstance(config, PretrainedConfig):
            config_path = config if config is not None else pretrained_model_name_or_path
            config, model_kwargs = cls.config_class.from_pretrained(
                config_path,
                cache_dir=cache_dir,
                return_unused_kwargs=True,
                force_download=force_download,
                proxies=proxies,
                local_files_only=local_files_only,
                token=token,
                revision=revision,
                subfolder=subfolder,
                gguf_file=gguf_file,
                _from_auto=from_auto_class,
                _from_pipeline=from_pipeline,
                **kwargs,
            )
            if "gguf_file" in model_kwargs:
                model_kwargs.pop("gguf_file")
        else:
            config = copy.deepcopy(config)
            model_kwargs = kwargs

        # Because some composite configs call super().__init__ before instantiating the sub-configs, we need this call
        # to correctly redispatch recursively if the kwarg is provided
        if "attn_implementation" in kwargs:
            config._attn_implementation = kwargs.pop("attn_implementation")

        transformers_explicit_filename = getattr(config, "transformers_weights", None)

        if transformers_explicit_filename is not None:
            if not transformers_explicit_filename.endswith(
                ".safetensors"
            ) and not transformers_explicit_filename.endswith(".safetensors.index.json"):
                raise ValueError(
                    "The transformers file in the config seems to be incorrect: it is neither a safetensors file "
                    "(*.safetensors) nor a safetensors index file (*.safetensors.index.json): "
                    f"{transformers_explicit_filename}"
                )

        hf_quantizer, config, dtype, device_map = get_hf_quantizer(
            config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent
        )

        if gguf_file is not None and hf_quantizer is not None:
            raise ValueError(
                "You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub."
            )

        if (
            gguf_file
            and device_map is not None
            and ((isinstance(device_map, dict) and "disk" in device_map.values()) or "disk" in device_map)
        ):
            raise RuntimeError(
                "One or more modules is configured to be mapped to disk. Disk offload is not supported for models "
                "loaded from GGUF files."
            )

        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
            pretrained_model_name_or_path=pretrained_model_name_or_path,
            subfolder=subfolder,
            variant=variant,
            gguf_file=gguf_file,
            from_tf=from_tf,
            from_flax=from_flax,
            use_safetensors=use_safetensors,
            cache_dir=cache_dir,
            force_download=force_download,
            proxies=proxies,
            local_files_only=local_files_only,
            token=token,
            user_agent=user_agent,
            revision=revision,
            commit_hash=commit_hash,
            is_remote_code=cls._auto_class is not None,
            transformers_explicit_filename=transformers_explicit_filename,
        )

        is_sharded = sharded_metadata is not None
        is_quantized = hf_quantizer is not None
        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None

        if is_from_file and not is_sharded and checkpoint_files[0].endswith(".safetensors"):
            with safe_open(checkpoint_files[0], framework="pt") as f:
                metadata = f.metadata()

            if metadata is None:
                # Assume it's a pytorch checkpoint (introduced for timm checkpoints)
                pass
            elif metadata.get("format") == "pt":
                pass
            elif metadata.get("format") == "tf":
                from_tf = True
                logger.info("A TensorFlow safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "flax":
                from_flax = True
                logger.info("A Flax safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "mlx":
                # This is a mlx file, we assume weights are compatible with pt
                pass
            else:
                raise ValueError(
                    f"Incompatible safetensors file. File metadata is not ['pt', 'tf', 'flax', 'mlx'] but {metadata.get('format')}"
                )

        from_pt = not (from_tf | from_flax)

        if from_pt:
            if gguf_file:
                from .modeling_gguf_pytorch_utils import load_gguf_checkpoint

                # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was
                # passed directly as a kwarg from now on
                with torch.device("meta"):
                    dummy_model = cls(config)
                state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[
                    "tensors"
                ]

            # Find the correct dtype based on current state
            config, dtype, dtype_orig = _get_dtype(
                cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only
            )

        config.name_or_path = pretrained_model_name_or_path
        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)
        config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.
        with ContextManagers(model_init_context):
            # Let's make sure we don't run the init function of buffer modules
>           model = cls(config, *model_args, **model_kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'

venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: TypeError
----------------------------- Captured stderr call -----------------------------
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]
_______________ TestHotSwapping.test_hotswap_works[True-config0] _______________
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_initialization.TestHotSwapping object at 0x7f005fda46d0>
config = LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revisio...time_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None)
do_compile = True
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/popen-gw1/test_hotswap_works_True_config0')

    @pytest.mark.parametrize(
        "config",
        [
            LoraConfig(init_lora_weights=0, target_modules=["lin0"]),
            LoraConfig(init_lora_weights=0, target_modules=["lin0", "lin1"]),
        ],
    )
    @pytest.mark.parametrize("do_compile", [False, True])
    def test_hotswap_works(self, config, do_compile, tmp_path):
        # Load 2 different adapters and check that we can hotswap between them, with the model optionally being
        # compiled.
        atol, rtol = 1e-4, 1e-4
        inputs = torch.rand(3, 10).to(self.torch_device)

        # create adapter 0
        model = self.get_model()
        torch.manual_seed(0)
        model = get_peft_model(model, config)
        model = self.compile(model, do_compile=do_compile)
        model.eval()
        with torch.inference_mode():
>           output0 = model(inputs)
                      ^^^^^^^^^^^^^

tests/test_initialization.py:3309:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:375: in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:749: in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:923: in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:907: in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1578: in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1456: in codegen_and_compile
    compiled_module = graph.compile_to_module()
                      ^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2293: in compile_to_module
    return self._compile_to_module()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2299: in _compile_to_module
    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
                                                             ^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2238: in codegen
    self.scheduler.codegen()
venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py:4598: in codegen
    else self._codegen(self.nodes)
         ^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py:4750: in _codegen
    self.get_backend(device).codegen_node(node)
venv/lib/python3.11/site-packages/torch/_inductor/codegen/cpp.py:5076: in codegen_node
    cpp_kernel_proxy = self.kernel_proxy_cls(kernel_group)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/codegen/cpp.py:3816: in __init__
    self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:432: in pick_vec_isa
    _valid_vec_isa_list: list[VecISA] = valid_vec_isa_list()
                                        ^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:419: in valid_vec_isa_list
    isa_list.extend(
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:419: in <genexpr>
    isa_list.extend(
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:147: in __bool__
    return self.__bool__impl(config.cpp.vec_isa_ok)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:157: in __bool__impl
    return self.check_build(VecISA._avx_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:102: in check_build
    extra=_get_isa_dry_compile_fingerprint(self._arch_flags),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:28: in _get_isa_dry_compile_fingerprint
    compiler_info = get_compiler_version_info(get_cpp_compiler())
                                              ^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:155: in get_cpp_compiler
    compiler = cpp_compiler_search(search)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

search = (None, 'g++')

    @functools.lru_cache(1)
    def cpp_compiler_search(search: str) -> str:
        from torch._inductor.codecache import get_lock_dir, LOCK_TIMEOUT

        for cxx in search:
            try:
                if cxx is None:
                    # gxx package is only available for Linux
                    # according to https://anaconda.org/conda-forge/gxx/
                    if sys.platform != "linux":
                        continue
                    # Do not install GXX by default
                    if not os.getenv("TORCH_INDUCTOR_INSTALL_GXX"):
                        continue
                    from torch.utils._filelock import FileLock

                    lock_dir = get_lock_dir()
                    lock = FileLock(
                        os.path.join(lock_dir, "g++.lock"), timeout=LOCK_TIMEOUT
                    )
                    with lock:
                        cxx = install_gcc_via_conda()
                subprocess.check_output([cxx, "--version"])
                return cxx
            except (subprocess.SubprocessError, FileNotFoundError, ImportError):
                continue
>       raise exc.InvalidCxxCompiler
E       torch._inductor.exc.InductorError: InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')
E
E       Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

venv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:101: InductorError
_______________ TestHotSwapping.test_hotswap_works[True-config1] _______________
[gw1] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_initialization.TestHotSwapping object at 0x7f005f776410>
config = LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revisio...time_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None)
do_compile = True
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/popen-gw1/test_hotswap_works_True_config1')

    @pytest.mark.parametrize(
        "config",
        [
            LoraConfig(init_lora_weights=0, target_modules=["lin0"]),
            LoraConfig(init_lora_weights=0, target_modules=["lin0", "lin1"]),
        ],
    )
    @pytest.mark.parametrize("do_compile", [False, True])
    def test_hotswap_works(self, config, do_compile, tmp_path):
        # Load 2 different adapters and check that we can hotswap between them, with the model optionally being
        # compiled.
        atol, rtol = 1e-4, 1e-4
        inputs = torch.rand(3, 10).to(self.torch_device)

        # create adapter 0
        model = self.get_model()
        torch.manual_seed(0)
        model = get_peft_model(model, config)
        model = self.compile(model, do_compile=do_compile)
        model.eval()
        with torch.inference_mode():
>           output0 = model(inputs)
                      ^^^^^^^^^^^^^

tests/test_initialization.py:3309:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:375: in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:749: in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:923: in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:907: in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1578: in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1456: in codegen_and_compile
    compiled_module = graph.compile_to_module()
                      ^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2293: in compile_to_module
    return self._compile_to_module()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2299: in _compile_to_module
    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
                                                             ^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2238: in codegen
    self.scheduler.codegen()
venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py:4598: in codegen
    else self._codegen(self.nodes)
         ^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py:4750: in _codegen
    self.get_backend(device).codegen_node(node)
venv/lib/python3.11/site-packages/torch/_inductor/codegen/cpp.py:5076: in codegen_node
    cpp_kernel_proxy = self.kernel_proxy_cls(kernel_group)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/codegen/cpp.py:3816: in __init__
    self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:432: in pick_vec_isa
    _valid_vec_isa_list: list[VecISA] = valid_vec_isa_list()
                                        ^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:419: in valid_vec_isa_list
    isa_list.extend(
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:419: in <genexpr>
    isa_list.extend(
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:147: in __bool__
    return self.__bool__impl(config.cpp.vec_isa_ok)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:157: in __bool__impl
    return self.check_build(VecISA._avx_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:102: in check_build
    extra=_get_isa_dry_compile_fingerprint(self._arch_flags),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:28: in _get_isa_dry_compile_fingerprint
    compiler_info = get_compiler_version_info(get_cpp_compiler())
                                              ^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:155: in get_cpp_compiler
    compiler = cpp_compiler_search(search)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

search = (None, 'g++')

    @functools.lru_cache(1)
    def cpp_compiler_search(search: str) -> str:
        from torch._inductor.codecache import get_lock_dir, LOCK_TIMEOUT

        for cxx in search:
            try:
                if cxx is None:
                    # gxx package is only available for Linux
                    # according to https://anaconda.org/conda-forge/gxx/
                    if sys.platform != "linux":
                        continue
                    # Do not install GXX by default
                    if not os.getenv("TORCH_INDUCTOR_INSTALL_GXX"):
                        continue
                    from torch.utils._filelock import FileLock

                    lock_dir = get_lock_dir()
                    lock = FileLock(
                        os.path.join(lock_dir, "g++.lock"), timeout=LOCK_TIMEOUT
                    )
                    with lock:
                        cxx = install_gcc_via_conda()
                subprocess.check_output([cxx, "--version"])
                return cxx
            except (subprocess.SubprocessError, FileNotFoundError, ImportError):
                continue
>       raise exc.InvalidCxxCompiler
E       torch._inductor.exc.InductorError: InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')
E
E       Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

venv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:101: InductorError
____ PeftCustomKwargsTester.test_maybe_include_all_linear_layers_diffusion _____
[gw2] linux -- Python 3.11.0 /app/venv/bin/python

self = <tests.test_tuners_utils.PeftCustomKwargsTester testMethod=test_maybe_include_all_linear_layers_diffusion>

    def test_maybe_include_all_linear_layers_diffusion(self):
        model_id = "hf-internal-testing/tiny-sd-pipe"
        with hub_online_once(model_id):
>           model = StableDiffusionPipeline.from_pretrained(model_id)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_tuners_utils.py:347:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'transformers.models.clip.modeling_clip.CLIPTextModel'>
pretrained_model_name_or_path = '/root/.cache/huggingface/hub/models--hf-internal-testing--tiny-sd-pipe/snapshots/39f7e1819d772fbb8dd7f2aa3da8e2fc5a9bd917/text_encoder'
config = CLIPTextConfig {
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dty...en_layers": 5,
  "pad_token_id": 1,
  "projection_dim": 32,
  "transformers_version": "4.57.0",
  "vocab_size": 1000
}

cache_dir = None, ignore_mismatched_sizes = False, force_download = False
local_files_only = False, token = None, revision = 'main'
use_safetensors = None, weights_only = True, model_args = ()
kwargs = {'offload_state_dict': None}, state_dict = None

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        **kwargs,
    ) -> SpecificPreTrainedModelType:
        r"""
        Instantiate a pretrained pytorch model from a pre-trained model configuration.

        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you should first set it back in training mode with `model.train()`.

        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come
        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
        task.

        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those
        weights are discarded.

        Parameters:
            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):
                Can be either:

                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
                    - A path to a *directory* containing model weights saved using
                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In
                      this case, `from_tf` should be set to `True` and a configuration object should be provided as
                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a
                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.
                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,
                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to
                      `True`.
                    - `None` if you are both providing the configuration and state dictionary (resp. with keyword
                      arguments `config` and `state_dict`).
            model_args (sequence of positional arguments, *optional*):
                All remaining positional arguments will be passed to the underlying model's `__init__` method.
            config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):
                Can be either:

                    - an instance of a class derived from [`PretrainedConfig`],
                    - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].

                Configuration for the model to use instead of an automatically loaded configuration. Configuration can
                be automatically loaded when:

                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained
                      model).
                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the
                      save directory.
                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a
                      configuration JSON file named *config.json* is found in the directory.
            state_dict (`dict[str, torch.Tensor]`, *optional*):
                A state dictionary to use instead of a state dictionary loaded from saved weights file.

                This option can be used if you want to create a model from a pretrained configuration but load your own
                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and
                [`~PreTrainedModel.from_pretrained`] is not a simpler option.
            cache_dir (`Union[str, os.PathLike]`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the
                standard cache should not be used.
            from_tf (`bool`, *optional*, defaults to `False`):
                Load the model weights from a TensorFlow checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            from_flax (`bool`, *optional*, defaults to `False`):
                Load the model weights from a Flax checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):
                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size
                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a
                checkpoint with 3 labels).
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
                cached versions if they exist.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
            output_loading_info(`bool`, *optional*, defaults to `False`):
                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.
            local_files_only(`bool`, *optional*, defaults to `False`):
                Whether or not to only look at local files (i.e., do not try to download the model).
            token (`str` or `bool`, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use
                the token generated when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.

                <Tip>

                To test a pull request you made on the Hub, you can pass `revision="refs/pr/<pr_number>"`.

                </Tip>
            attn_implementation (`str`, *optional*):
                The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)), or `"flash_attention_3"` (using [Dao-AILab/flash-attention/hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.

                Accept HF kernel references in the form:
                  <namespace>/<repo_name>[@<revision>][:<kernel_name>]

                - <namespace> and <repo_name> are any non-"/" and non-":" sequences.
                - "@<revision>" is optional (branch, tag, or commit-ish), e.g. "@main", "@v1.2.0", "@abc123".
                - ":<kernel_name>" is optional and selects a function inside the kernel repo.
                - Both options can appear together and in this order only: @revision first, then :kernel_name.
                - We intentionally allow a leading "<wrapper>|" prefix (e.g., "flash|...") because the code
                  strips it before loading; '|' is not excluded in the character classes here.

                Examples that match:
                  "org/model"
                  "org/model@main"
                  "org/model:custom_kernel"
                  "org/model@v1.2.3:custom_kernel"

            > Parameters for big model inference

            dtype (`str` or `torch.dtype`, *optional*):
                Override the default `torch_dtype` and load the model under a specific `dtype`. The different options
                are:

                1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified
                  `dtype`, ignoring the model's `config.dtype` if one exists. If not specified
                  - the model will get loaded in `torch.float` (fp32).

                2. `"auto"` - A `dtype` or `torch_dtype` entry in the `config.json` file of the model will be
                  attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in
                  the checkpoint that's of a floating point type and use that as `dtype`. This will load the model
                  using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how
                  the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.

                3. A string that is a valid `torch.dtype`. E.g. "float32" loads the model in `torch.float32`, "float16" loads in `torch.float16` etc.

                <Tip>

                For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or
                reach out to the authors and ask them to add this information to the model's card and to insert the
                `dtype` or `torch_dtype` entry in `config.json` on the hub.

                </Tip>

            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):
                A map that specifies where each submodule should go. It doesn't need to be refined to each
                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
                same device. If we only pass the device (*e.g.*, `"cpu"`, `"cuda:1"`, `"mps"`, or a GPU ordinal rank
                like `1`) on which the model will be allocated, the device map will map the entire model to this
                device. Passing `device_map = 0` means put the whole model on GPU 0.

                To have Accelerate compute the most optimized `device_map` automatically, set `device_map="auto"`. For
                more information about each option see [designing a device
                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).
            max_memory (`Dict`, *optional*):
                A dictionary device identifier to maximum memory if using `device_map`. Will default to the maximum memory available for each
                GPU and the available CPU RAM if unset.
            tp_plan (`str`, *optional*):
                A torch tensor parallel plan, see [here](https://pytorch.org/tutorials/intermediate/TP_tutorial.html). Currently, it only accepts
                `tp_plan="auto"` to use predefined plan based on the model. Note that if you use it, you should launch your script accordingly with
                `torchrun [args] script.py`. This will be much faster than using a `device_map`, but has limitations.
            tp_size (`str`, *optional*):
                A torch tensor parallel degree. If not provided would default to world size.
            device_mesh (`torch.distributed.DeviceMesh`, *optional*):
                A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.
                If provided, it has to contain dimension named `"tp"` in case it's > 1 dimensional, this dimension will be used for tensor parallelism
            offload_folder (`str` or `os.PathLike`, *optional*):
                If the `device_map` contains any value `"disk"`, the folder where we will offload weights.
            offload_buffers (`bool`, *optional*):
                Whether or not to offload the buffers with the model parameters.
            quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):
                A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g
                bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and
                `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes
                quantizations and not preferred. consider inserting all such arguments into quantization_config
                instead.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            variant (`str`, *optional*):
                If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is
                ignored when using `from_tf` or `from_flax`.
            use_safetensors (`bool`, *optional*, defaults to `None`):
                Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`
                is not installed, it will be set to `False`.
            weights_only (`bool`, *optional*, defaults to `True`):
                Indicates whether unpickler should be restricted to loading only tensors, primitive types,
                dictionaries and any types added via torch.serialization.add_safe_globals().
                When set to False, we can load wrapper tensor subclass weights.
            key_mapping (`dict[str, str], *optional*):
                A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers
                architecture, but was not converted accordingly.
            kwargs (remaining dictionary of keyword arguments, *optional*):
                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or
                automatically loaded:

                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the
                      underlying model's `__init__` method (we assume all relevant updates to the configuration have
                      already been done)
                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class
                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that
                      corresponds to a configuration attribute will be used to override said attribute with the
                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute
                      will be passed to the underlying model's `__init__` function.

        <Tip>

        Activate the special ["offline-mode"](https://huggingface.co/transformers/installation.html#offline-mode) to
        use this method in a firewalled environment.

        </Tip>

        Examples:

        ```python
        >>> from transformers import BertConfig, BertModel

        >>> # Download model and configuration from huggingface.co and cache.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased")
        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
        >>> model = BertModel.from_pretrained("./test/saved_model/")
        >>> # Update configuration during loading.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", output_attentions=True)
        >>> assert model.config.output_attentions == True
        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).
        >>> config = BertConfig.from_json_file("./tf_model/my_tf_model_config.json")
        >>> model = BertModel.from_pretrained("./tf_model/my_tf_checkpoint.ckpt.index", from_tf=True, config=config)
        >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", from_flax=True)
        ```
        """
        state_dict = kwargs.pop("state_dict", None)
        from_tf = kwargs.pop("from_tf", False)
        from_flax = kwargs.pop("from_flax", False)
        proxies = kwargs.pop("proxies", None)
        output_loading_info = kwargs.pop("output_loading_info", False)
        use_auth_token = kwargs.pop("use_auth_token", None)
        from_pipeline = kwargs.pop("_from_pipeline", None)
        from_auto_class = kwargs.pop("_from_auto", False)
        dtype = kwargs.pop("dtype", None)
        torch_dtype = kwargs.pop("torch_dtype", None)  # kept for BC
        device_map = kwargs.pop("device_map", None)
        max_memory = kwargs.pop("max_memory", None)
        offload_folder = kwargs.pop("offload_folder", None)
        offload_buffers = kwargs.pop("offload_buffers", False)
        load_in_8bit = kwargs.pop("load_in_8bit", False)
        load_in_4bit = kwargs.pop("load_in_4bit", False)
        quantization_config = kwargs.pop("quantization_config", None)
        subfolder = kwargs.pop("subfolder", "")
        commit_hash = kwargs.pop("_commit_hash", None)
        variant = kwargs.pop("variant", None)
        adapter_kwargs = kwargs.pop("adapter_kwargs", {})
        adapter_name = kwargs.pop("adapter_name", "default")
        generation_config = kwargs.pop("generation_config", None)
        gguf_file = kwargs.pop("gguf_file", None)
        tp_plan = kwargs.pop("tp_plan", None)
        tp_size = kwargs.pop("tp_size", None)
        distributed_config: DistributedConfig = kwargs.pop("distributed_config", None)
        device_mesh = kwargs.pop("device_mesh", None)
        trust_remote_code = kwargs.pop("trust_remote_code", None)
        use_kernels = kwargs.pop("use_kernels", False)

        key_mapping = kwargs.pop("key_mapping", None)
        # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model
        if key_mapping is None and any(
            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS
        ):
            key_mapping = cls._checkpoint_conversion_mapping

        if distributed_config is not None:
            tp_plan = "auto"

        # Not used anymore -- remove them from the kwargs
        _ = kwargs.pop("resume_download", None)
        _ = kwargs.pop("mirror", None)
        _ = kwargs.pop("_fast_init", True)
        _ = kwargs.pop("low_cpu_mem_usage", None)

        # For BC on torch_dtype argument
        if torch_dtype is not None:
            logger.warning_once("`torch_dtype` is deprecated! Use `dtype` instead!")
            # If both kwargs are provided, use `dtype`
            dtype = dtype if dtype is not None else torch_dtype

        if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):
            raise ValueError(
                "`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies."
            )
        if tp_size is not None and tp_plan is None:
            raise ValueError("tp_plan has to be set when tp_size is passed.")
        if tp_plan is not None and tp_plan != "auto":
            # TODO: we can relax this check when we support taking tp_plan from a json file, for example.
            raise ValueError(f"tp_plan supports 'auto' only for now but got {tp_plan}.")
        if tp_plan is not None and device_map is not None:
            raise ValueError(
                "`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization."
            )

        if device_map == "auto" and int(os.environ.get("WORLD_SIZE", "0")):
            logger.info(
                "You've set device_map=`auto` while triggering a distributed run with torchrun. This might lead to unexpected behavior. "
                "If your plan is to load the model on each device, you should set device_map={"
                ": PartialState().process_index} where PartialState comes from accelerate library"
            )

        # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple
        # `device_map` pointing to the correct device
        if tp_plan is not None:
            if device_mesh is None:
                tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(tp_plan, tp_size=tp_size)
            else:
                if device_mesh.ndim > 1:
                    if "tp" not in device_mesh.mesh_dim_names:
                        raise ValueError(
                            "When using `tp_plan` and n-d `device_mesh`, it must contain a 'tp' dimension. "
                            "Please provide a valid `device_mesh`."
                        )
                    device_mesh = device_mesh["tp"]
                tp_size = device_mesh.size()
                device_map = torch.device(f"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}")

            if tp_size is None:
                tp_size = torch.distributed.get_world_size()

        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError(
                    "`token` and `use_auth_token` are both specified. Please set only the argument `token`."
                )
            token = use_auth_token

        if token is not None and adapter_kwargs is not None and "token" not in adapter_kwargs:
            adapter_kwargs["token"] = token

        if gguf_file is not None and not is_accelerate_available():
            raise ValueError("accelerate is required when loading a GGUF file `pip install accelerate`.")

        if commit_hash is None:
            if not isinstance(config, PretrainedConfig):
                # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible
                resolved_config_file = cached_file(
                    pretrained_model_name_or_path,
                    CONFIG_NAME,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    token=token,
                    revision=revision,
                    subfolder=subfolder,
                    _raise_exceptions_for_gated_repo=False,
                    _raise_exceptions_for_missing_entries=False,
                    _raise_exceptions_for_connection_errors=False,
                )
                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)
            else:
                commit_hash = getattr(config, "_commit_hash", None)

        if is_peft_available():
            _adapter_model_path = adapter_kwargs.pop("_adapter_model_path", None)

            if _adapter_model_path is None:
                _adapter_model_path = find_adapter_config_file(
                    pretrained_model_name_or_path,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    _commit_hash=commit_hash,
                    **adapter_kwargs,
                )
            if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):
                with open(_adapter_model_path, "r", encoding="utf-8") as f:
                    _adapter_model_path = pretrained_model_name_or_path
                    pretrained_model_name_or_path = json.load(f)["base_model_name_or_path"]
        else:
            _adapter_model_path = None

        # Potentially detect context manager or global device, and use it (only if no device_map was provided)
        if device_map is None and not is_deepspeed_zero3_enabled():
            device_in_context = get_torch_context_manager_or_global_device()
            if device_in_context == torch.device("meta"):
                raise RuntimeError(
                    "You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\n"
                    "This is an anti-pattern as `from_pretrained` wants to load existing weights.\nIf you want to initialize an "
                    "empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`"
                )
            device_map = device_in_context

        # change device_map into a map if we passed an int, a str or a torch.device
        if isinstance(device_map, torch.device):
            device_map = {"": device_map}
        elif isinstance(device_map, str) and device_map not in ["auto", "balanced", "balanced_low_0", "sequential"]:
            try:
                device_map = {"": torch.device(device_map)}
            except RuntimeError:
                raise ValueError(
                    "When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or "
                    f"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}."
                )
        elif isinstance(device_map, int):
            if device_map < 0:
                raise ValueError(
                    "You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' "
                )
            else:
                device_map = {"": device_map}

        if device_map is not None:
            if is_deepspeed_zero3_enabled():
                raise ValueError("DeepSpeed Zero-3 is not compatible with passing a `device_map`.")
            if not is_accelerate_available():
                raise ValueError(
                    "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` "
                    "requires `accelerate`. You can install it with `pip install accelerate`"
                )

        # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.
        if load_in_4bit or load_in_8bit:
            if quantization_config is not None:
                raise ValueError(
                    "You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing "
                    "`quantization_config` argument at the same time."
                )

            # preparing BitsAndBytesConfig from kwargs
            config_dict = {k: v for k, v in kwargs.items() if k in inspect.signature(BitsAndBytesConfig).parameters}
            config_dict = {**config_dict, "load_in_4bit": load_in_4bit, "load_in_8bit": load_in_8bit}
            quantization_config, kwargs = BitsAndBytesConfig.from_dict(
                config_dict=config_dict, return_unused_kwargs=True, **kwargs
            )
            logger.warning(
                "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. "
                "Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead."
            )

        from_pt = not (from_tf | from_flax)

        user_agent = {"file_type": "model", "framework": "pytorch", "from_auto_class": from_auto_class}
        if from_pipeline is not None:
            user_agent["using_pipeline"] = from_pipeline

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True

        # Load config if we don't provide a configuration
        if not isinstance(config, PretrainedConfig):
            config_path = config if config is not None else pretrained_model_name_or_path
            config, model_kwargs = cls.config_class.from_pretrained(
                config_path,
                cache_dir=cache_dir,
                return_unused_kwargs=True,
                force_download=force_download,
                proxies=proxies,
                local_files_only=local_files_only,
                token=token,
                revision=revision,
                subfolder=subfolder,
                gguf_file=gguf_file,
                _from_auto=from_auto_class,
                _from_pipeline=from_pipeline,
                **kwargs,
            )
            if "gguf_file" in model_kwargs:
                model_kwargs.pop("gguf_file")
        else:
            config = copy.deepcopy(config)
            model_kwargs = kwargs

        # Because some composite configs call super().__init__ before instantiating the sub-configs, we need this call
        # to correctly redispatch recursively if the kwarg is provided
        if "attn_implementation" in kwargs:
            config._attn_implementation = kwargs.pop("attn_implementation")

        transformers_explicit_filename = getattr(config, "transformers_weights", None)

        if transformers_explicit_filename is not None:
            if not transformers_explicit_filename.endswith(
                ".safetensors"
            ) and not transformers_explicit_filename.endswith(".safetensors.index.json"):
                raise ValueError(
                    "The transformers file in the config seems to be incorrect: it is neither a safetensors file "
                    "(*.safetensors) nor a safetensors index file (*.safetensors.index.json): "
                    f"{transformers_explicit_filename}"
                )

        hf_quantizer, config, dtype, device_map = get_hf_quantizer(
            config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent
        )

        if gguf_file is not None and hf_quantizer is not None:
            raise ValueError(
                "You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub."
            )

        if (
            gguf_file
            and device_map is not None
            and ((isinstance(device_map, dict) and "disk" in device_map.values()) or "disk" in device_map)
        ):
            raise RuntimeError(
                "One or more modules is configured to be mapped to disk. Disk offload is not supported for models "
                "loaded from GGUF files."
            )

        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
            pretrained_model_name_or_path=pretrained_model_name_or_path,
            subfolder=subfolder,
            variant=variant,
            gguf_file=gguf_file,
            from_tf=from_tf,
            from_flax=from_flax,
            use_safetensors=use_safetensors,
            cache_dir=cache_dir,
            force_download=force_download,
            proxies=proxies,
            local_files_only=local_files_only,
            token=token,
            user_agent=user_agent,
            revision=revision,
            commit_hash=commit_hash,
            is_remote_code=cls._auto_class is not None,
            transformers_explicit_filename=transformers_explicit_filename,
        )

        is_sharded = sharded_metadata is not None
        is_quantized = hf_quantizer is not None
        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None

        if is_from_file and not is_sharded and checkpoint_files[0].endswith(".safetensors"):
            with safe_open(checkpoint_files[0], framework="pt") as f:
                metadata = f.metadata()

            if metadata is None:
                # Assume it's a pytorch checkpoint (introduced for timm checkpoints)
                pass
            elif metadata.get("format") == "pt":
                pass
            elif metadata.get("format") == "tf":
                from_tf = True
                logger.info("A TensorFlow safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "flax":
                from_flax = True
                logger.info("A Flax safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "mlx":
                # This is a mlx file, we assume weights are compatible with pt
                pass
            else:
                raise ValueError(
                    f"Incompatible safetensors file. File metadata is not ['pt', 'tf', 'flax', 'mlx'] but {metadata.get('format')}"
                )

        from_pt = not (from_tf | from_flax)

        if from_pt:
            if gguf_file:
                from .modeling_gguf_pytorch_utils import load_gguf_checkpoint

                # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was
                # passed directly as a kwarg from now on
                with torch.device("meta"):
                    dummy_model = cls(config)
                state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[
                    "tensors"
                ]

            # Find the correct dtype based on current state
            config, dtype, dtype_orig = _get_dtype(
                cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only
            )

        config.name_or_path = pretrained_model_name_or_path
        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)
        config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.
        with ContextManagers(model_init_context):
            # Let's make sure we don't run the init function of buffer modules
>           model = cls(config, *model_args, **model_kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'

venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: TypeError
----------------------------- Captured stderr call -----------------------------
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]
=============================== warnings summary ===============================
src/peft/tuners/bone/config.py:126: 3 warnings
tests/test_custom_models.py: 241 warnings
tests/test_config.py: 16 warnings
tests/test_decoder_models.py: 166 warnings
tests/test_encoder_decoder_models.py: 38 warnings
tests/test_feature_extraction_models.py: 54 warnings
tests/test_seq_classifier.py: 27 warnings
  /app/src/peft/tuners/bone/config.py:126: UserWarning: Bone will be removed in v0.19.0 of PEFT, use `MissConfig` instead. If you already have a Bone checkpoint, you can use `/scripts/convert-bone-to-miss.py` to convert it into
    warnings.warn(

tests/test_custom_models.py: 3114 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MLP' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 811 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2d' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 300 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv3d' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 63 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MHA' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 26 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MlpUsingParameters' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 26 warnings
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'tests.test_custom_models._LinearUsingParameter'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_custom_models.py: 385 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'EmbConv1D' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 87 warnings
tests/test_decoder_models.py: 20 warnings
  /app/src/peft/tuners/ia3/model.py:130: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py: 29 warnings
  /app/src/peft/tuners/ia3/model.py:122: UserWarning: fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. Setting fan_in_fan_out to False.
    warnings.warn(

tests/test_custom_models.py: 160 warnings
tests/test_decoder_models.py: 100 warnings
tests/test_helpers.py: 2 warnings
tests/test_initialization.py: 4 warnings
tests/test_tuners_utils.py: 2 warnings
  /app/src/peft/tuners/lora/layer.py:2264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py: 207 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv1d' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 52 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2dGroups' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 104 warnings
tests/test_initialization.py: 6 warnings
  /app/src/peft/tuners/lora/layer.py:1138: UserWarning: LoRA adapter added to ConvNd layer with groups > 1. Merging is not supported.
    warnings.warn("LoRA adapter added to ConvNd layer with groups > 1. Merging is not supported.")

tests/test_custom_models.py: 52 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2dGroups2' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 116 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv1dBigger' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 58 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2d1x1' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 129 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MLP_LayerNorm' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 79 warnings
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'torch.nn.modules.normalization.LayerNorm'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_custom_models.py::TestPeftCustomModel::test_training_custom_models[Vanilla MLP 1 BOFT-MLP-BOFTConfig-config_kwargs137]
tests/test_custom_models.py::TestPeftCustomModel::test_save_pretrained_pickle[Vanilla MLP 1 BOFT-MLP-BOFTConfig-config_kwargs137]
tests/test_boft.py::TestBoft::test_boft_state_dict
  /app/venv/lib/python3.11/site-packages/pkg_resources/_vendor/pyparsing.py:87: DeprecationWarning: module 'sre_constants' is deprecated
    import sre_constants

tests/test_custom_models.py::TestPeftCustomModel::test_training_custom_models[Vanilla MLP 1 BOFT-MLP-BOFTConfig-config_kwargs137]
tests/test_custom_models.py::TestPeftCustomModel::test_save_pretrained_pickle[Vanilla MLP 1 BOFT-MLP-BOFTConfig-config_kwargs137]
tests/test_boft.py::TestBoft::test_boft_state_dict
  /app/src/peft/tuners/boft/layer.py:95: UserWarning: Failed to load the CUDA extension: Ninja is required to load C++ extensions (pip install ninja to get it), check if ninja is available.
    warnings.warn(f"Failed to load the CUDA extension: {e}, check if ninja is available.")

tests/test_custom_models.py: 353 warnings
tests/test_boft.py: 2 warnings
tests/test_decoder_models.py: 160 warnings
tests/test_encoder_decoder_models.py: 36 warnings
tests/test_feature_extraction_models.py: 46 warnings
tests/test_seq_classifier.py: 27 warnings
  /app/src/peft/tuners/boft/layer.py:96: UserWarning: Setting boft_n_butterfly_factor to 1 to speed up the finetuning process.
    warnings.warn("Setting boft_n_butterfly_factor to 1 to speed up the finetuning process.")

tests/test_custom_models.py: 351 warnings
tests/test_boft.py: 2 warnings
tests/test_decoder_models.py: 160 warnings
tests/test_encoder_decoder_models.py: 36 warnings
tests/test_feature_extraction_models.py: 46 warnings
tests/test_seq_classifier.py: 27 warnings
  /app/src/peft/tuners/boft/layer.py:95: UserWarning: Failed to load the CUDA extension: /root/.cache/torch_extensions/py311_cu128/fbd_cuda/fbd_cuda.so: cannot open shared object file: No such file or directory, check if ninja is available.
    warnings.warn(f"Failed to load the CUDA extension: {e}, check if ninja is available.")

tests/test_custom_models.py: 56 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MLP2' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 56 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2d2' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 27 warnings
tests/test_decoder_models.py: 17 warnings
  /app/src/peft/tuners/vera/model.py:275: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py: 30 warnings
tests/test_decoder_models.py: 19 warnings
  /app/src/peft/tuners/vblora/model.py:141: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py: 36 warnings
tests/test_config.py: 1 warning
tests/test_decoder_models.py: 24 warnings
tests/test_encoder_decoder_models.py: 6 warnings
tests/test_feature_extraction_models.py: 8 warnings
tests/test_seq_classifier.py: 6 warnings
tests/test_vision_models.py: 1 warning
  /app/src/peft/tuners/oft/config.py:206: UserWarning: The cayley-neumann parameterization has been slightly changed to be more numerically stable in PEFT 0.18.0. Please retrain your adapter weights with newer PEFT versions. Alternatively, downgrade PEFT to version 0.17.0 to use the old parameterization.
    warnings.warn(msg)

tests/test_custom_models.py: 14 warnings
  /app/src/peft/tuners/tuners_utils.py:1683: UserWarning: All adapters are already merged, nothing to do.
    warnings.warn("All adapters are already merged, nothing to do.")

tests/test_custom_models.py: 10 warnings
tests/test_tuners_utils.py: 2 warnings
  /app/src/peft/tuners/lora/layer.py:728: UserWarning: Already unmerged. Nothing to do.
    warnings.warn("Already unmerged. Nothing to do.")

tests/test_custom_models.py: 110 warnings
tests/test_decoder_models.py: 7 warnings
tests/test_encoder_decoder_models.py: 2 warnings
tests/test_feature_extraction_models.py: 4 warnings
  /app/src/peft/tuners/ia3/layer.py:142: UserWarning: Unmerge result can be inaccurate for (IA)^3.
    warnings.warn("Unmerge result can be inaccurate for (IA)^3.")

tests/test_custom_models.py: 60 warnings
  /app/src/peft/tuners/ia3/layer.py:268: UserWarning: Unmerge result can be inaccurate for (IA)^3.
    warnings.warn("Unmerge result can be inaccurate for (IA)^3.")

tests/test_custom_models.py: 160 warnings
tests/test_decoder_models.py: 112 warnings
tests/test_encoder_decoder_models.py: 22 warnings
tests/test_feature_extraction_models.py: 44 warnings
  /app/src/peft/tuners/tuners_utils.py:1350: UserWarning: Adapter delete_me was active which is now deleted. Setting active adapter to default.
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter[MHA 1 LoRA-MHA-LoraConfig-config_kwargs33]
tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter[MHA 2 LoRA-MHA-LoraConfig-config_kwargs34]
tests/test_custom_models.py::TestPeftCustomModel::test_delete_inactive_adapter[MHA 1 LoRA-MHA-LoraConfig-config_kwargs33]
tests/test_custom_models.py::TestPeftCustomModel::test_delete_inactive_adapter[MHA 2 LoRA-MHA-LoraConfig-config_kwargs34]
  /app/src/peft/tuners/lora/layer.py:1679: UserWarning: Already unmerged. Nothing to do.
    warnings.warn("Already unmerged. Nothing to do.")

tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_with_multiple_adapters_works
tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_modules_to_save
tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_trainable_token_indices
tests/test_mixed.py::TestMixedAdapterTypes::test_delete_adapter
  /app/src/peft/tuners/tuners_utils.py:1350: UserWarning: Adapter adapter0 was active which is now deleted. Setting active adapter to adapter1.
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_modules_to_save
tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_trainable_token_indices
  /app/src/peft/tuners/tuners_utils.py:1350: UserWarning: Adapter adapter1 was active which is now deleted. Setting active adapter to adapter2.
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_trainable_token_indices
  /app/src/peft/tuners/tuners_utils.py:1350: UserWarning: Adapter adapter0 was active which is now deleted. Setting active adapter to adapter2.
    warnings.warn(

tests/test_decoder_models.py: 14 warnings
  /app/src/peft/tuners/adalora/model.py:211: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_config.py::TestPeftConfig::test_from_peft_type
  /app/src/peft/tuners/xlora/config.py:83: UserWarning: No value was provided for `hidden_size`. This will be set to 4096 by default, please ensure that this is correct.
    warnings.warn(

tests/test_config.py::TestPeftConfig::test_from_peft_type
  /app/src/peft/tuners/xlora/config.py:88: UserWarning: No value was provided for for `adapters`. This will be set to empty, please ensure that this is correct.
    warnings.warn(

tests/test_config.py::TestPeftConfig::test_from_peft_type
tests/test_cpt.py::test_model_initialization_text
tests/test_cpt.py::test_model_initialization_random
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-hf-internal-testing/tiny-random-gpt2]
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-hf-internal-testing/tiny-random-OPTForCausalLM]
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-hf-internal-testing/tiny-random-MistralForCausalLM]
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-peft-internal-testing/tiny-dummy-qwen2]
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-trl-internal-testing/tiny-random-LlamaForCausalLM]
  /app/src/peft/tuners/cpt/config.py:85: FutureWarning: CPTConfig only supports task_type = CAUSAL_LM, setting it automatically. This will raise an error starting from PEFT v0.18.0.
    warnings.warn(

tests/test_custom_models.py: 1 warning
tests/test_mapping.py: 2 warnings
tests/test_tuners_utils.py: 9 warnings
  /app/src/peft/tuners/tuners_utils.py:279: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_load_resized_embedding_ignore_mismatched_sizes
  /app/src/peft/utils/save_and_load.py:561: UserWarning: Some weights of PeftModel were not initialized from the model checkpoint and are being ignored because you passed `ignore_mismatched_sizes=True`: - base_model.model.emb.lora_embedding_A.default: found shape torch.Size([8, 100]) in the checkpoint and torch.Size([8, 105]) in the model instantiated.
    warnings.warn(msg)

tests/test_custom_models.py::TestPeftCustomModel::test_load_resized_embedding_ignore_mismatched_sizes
  /app/src/peft/peft_model.py:587: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.emb.lora_embedding_A.default'].
    warnings.warn(warn_message)

tests/test_decoder_models.py: 18 warnings
  /app/src/peft/tuners/fourierft/model.py:116: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_config.py: 24 warnings
  /app/src/peft/config.py:225: UserWarning: The configuration file contains a `runtime_config` key. This is ignored. Runtime configurations are only valid at runtime.
    warnings.warn(

tests/test_decoder_models.py: 57 warnings
tests/test_encoder_decoder_models.py: 17 warnings
tests/test_seq_classifier.py: 9 warnings
  /app/src/peft/tuners/oft/layer.py:452: UserWarning: Invalid `oft_block_size` (32)! Adjusted `oft_block_size` to (16).
    warnings.warn(

tests/test_decoder_models.py: 19 warnings
  /app/src/peft/tuners/oft/layer.py:452: UserWarning: Invalid `oft_block_size` (32)! Adjusted `oft_block_size` to (8).
    warnings.warn(

tests/test_cpt.py: 2 warnings
tests/test_decoder_models.py: 16 warnings
  /app/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
    warnings.warn(warn_msg)

tests/test_custom_models.py::TestDynamicDispatch::test_custom_lora_layer_used
tests/test_custom_models.py::TestDynamicDispatch::test_training_works
tests/test_custom_models.py::TestDynamicDispatch::test_saving_and_loading
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'tests.test_custom_models.TestDynamicDispatch.custom_module_cls.<locals>.MyModule'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_decoder_models.py::TestDecoderModels::test_prompt_tuning_text_prepare_for_training[PromptTuningConfig-config_kwargs15-trl-internal-testing/tiny-random-LlamaForCausalLM]
tests/test_decoder_models.py::TestDecoderModels::test_prefix_tuning_mistral
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute

tests/test_decoder_models.py::TestDecoderModels::test_prompt_tuning_text_prepare_for_training[PromptTuningConfig-config_kwargs15-trl-internal-testing/tiny-random-LlamaForCausalLM]
tests/test_decoder_models.py::TestDecoderModels::test_prefix_tuning_mistral
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute

tests/test_decoder_models.py: 135 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-OPTForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 87 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-GPT2LMHeadModel - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 135 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-BloomForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 135 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-gpt_neo - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 135 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-GPTJForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 135 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-GPTBigCodeForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 135 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in trl-internal-testing/tiny-random-LlamaForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 135 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in peft-internal-testing/tiny-dummy-qwen2 - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 135 warnings
tests/test_hub_features.py: 7 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-Gemma3ForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 104 warnings
tests/test_multitask_prompt_tuning.py: 7 warnings
  /app/src/peft/peft_model.py:2104: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.
    warnings.warn("Position ids are not supported for parameter efficient tuning. Ignoring position ids.")

tests/test_decoder_models.py: 9 warnings
tests/test_encoder_decoder_models.py: 2 warnings
  /app/src/peft/tuners/adalora/config.py:96: UserWarning: Note that `r` is not used in AdaLora and will be ignored.If you intended to set the initial rank, use `init_r` instead.
    warnings.warn(

tests/test_decoder_models.py::TestDecoderModels::test_lora_layer_replication
  /app/tests/test_decoder_models.py:599: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
    layers[0].mlp.up_proj.base_layer.weight.data.storage().data_ptr()

tests/test_encoder_decoder_models.py: 112 warnings
tests/test_hub_features.py: 2 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-BartForConditionalGeneration - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_encoder_decoder_models.py: 113 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in ybelkada/tiny-random-T5ForConditionalGeneration-calibrated - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_feature_extraction_models.py: 77 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-BertModel - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_feature_extraction_models.py: 76 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-RobertaModel - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_feature_extraction_models.py: 75 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-DebertaModel - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_feature_extraction_models.py: 75 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-DebertaV2Model - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 18 warnings
  /app/src/peft/tuners/lora/variants.py:714: UserWarning: Cannot calculate aLoRA offsets when only inputs_embeds are provided. Disabling aLoRA for this forward pass.
    warnings.warn(

tests/test_initialization.py::TestLoraInitialization::test_lora_with_bias_incompatible_arguments[extra_kwargs1]
  /app/src/peft/tuners/lora/config.py:718: UserWarning: `init_lora_weights` is 'eva' but `eva_config` is not specified. Using default EVA config.
    warnings.warn("`init_lora_weights` is 'eva' but `eva_config` is not specified. Using default EVA config.")

tests/test_initialization.py::TestVeraInitialization::test_vera_mixing_save_projection_raises
tests/test_vera.py::TestVera::test_multiple_adapters_save_projection_false_contains_no_vera_A_vera_B
tests/test_vera.py::TestVera::test_multiple_adapters_save_load_save_projection_false
  /app/src/peft/tuners/vera/config.py:158: UserWarning: Specified to not save vera_A and vera_B within the state dictionary, instead they will be restored using the PRNG key store in `config.projection_prng_key`. Consider setting `config.save_projection` to `True` to guarantee restoring the checkpoint correctly on all system configurations.
    warnings.warn(

tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_transformers_irregular_targets
tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_transformers_target_parameters_raises
tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_transformers_target_parameters_fails
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in facebook/opt-125m - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_extra_keys_warning
  /app/src/peft/tuners/tuners_utils.py:877: UserWarning: You have passed exclude_modules=['model.decoder.layers.5.self_attn.q_proj'] but no modules were excluded. Please check that exclude_modules was set correctly.
    warnings.warn(

tests/test_initialization.py::TestLoraInitialization::test_lora_pissa_conversion_same_output_after_loading
tests/test_initialization.py::TestLoraInitialization::test_lora_pissa_conversion_same_output_after_loading_with_rank_pattern
tests/test_initialization.py::TestLoraInitialization::test_lora_pissa_conversion_same_output_after_loading_with_alpha_pattern
tests/test_initialization.py::TestLoraInitialization::test_lora_pissa_conversion_same_output_after_loading_with_rslora
  /app/src/peft/peft_model.py:1257: UserWarning: PiSSA changes the base weights of the model and should thus not be used with other adapters. Consider converting the PiSSA adapter into a normal LoRA adapter: https://github.com/huggingface/peft/tree/main/examples/pissa_finetuning#convert-pissa-to-lora
    warnings.warn(msg)

tests/test_initialization.py::TestLoraInitialization::test_pissa_rank_pattern_and_rslora_raises
tests/test_initialization.py::TestLoraInitialization::test_pissa_alpha_pattern_and_rslora_raises
tests/test_initialization.py::TestLoraInitialization::test_olora_rank_pattern_and_rslora_raises
tests/test_initialization.py::TestLoraInitialization::test_olora_alpha_pattern_and_rslora_raises
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_rank_pattern_and_rslora_raises[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_rank_pattern_and_rslora_raises[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_alpha_pattern_and_rslora_raises[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_alpha_pattern_and_rslora_raises[kpm]
  /app/src/peft/tuners/lora/config.py:762: UserWarning: Using Rank-Stabilized LoRA with rank_pattern/alpha_pattern and post-training conversion of modified base weights PiSSA/CorDA/OLoRA means that you won't be able to pass `path_initial_model_for_weight_conversion` to `save_pretrained` to restore the initial values of the base weights; if you intend to do this, please ensure not to use rslora or rank_pattern/alpha_pattern.
    warnings.warn(msg)

tests/test_initialization.py::TestLoraInitialization::test_olora_conversion_same_output_after_loading
tests/test_initialization.py::TestLoraInitialization::test_olora_conversion_same_output_after_loading_with_rank_pattern
tests/test_initialization.py::TestLoraInitialization::test_olora_conversion_same_output_after_loading_with_alpha_pattern
tests/test_initialization.py::TestLoraInitialization::test_olora_conversion_same_output_after_loading_with_rslora
  /app/src/peft/peft_model.py:1271: UserWarning: OLoRA changes the base weights of the model and should thus not be used with other adapters. Consider converting the OLoRA adapter into a normal LoRA adapter: https://github.com/huggingface/peft/tree/main/examples/olora_finetuning#olora-and-lora
    warnings.warn(msg)

tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rank_pattern[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rank_pattern[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_alpha_pattern[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_alpha_pattern[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rslora[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rslora[kpm]
  /app/src/peft/tuners/lora/config.py:729: UserWarning: `corda_config` specified but will be ignored when `init_lora_weights` is not 'corda'.
    warnings.warn("`corda_config` specified but will be ignored when `init_lora_weights` is not 'corda'.")

tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rank_pattern[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rank_pattern[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_alpha_pattern[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_alpha_pattern[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rslora[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rslora[kpm]
  /app/src/peft/peft_model.py:1264: UserWarning: CorDA changes the base weights of the model and should thus not be used with other adapters. Consider converting the CorDA adapter into a normal LoRA adapter: https://github.com/huggingface/peft/tree/main/examples/corda_finetuning#convert-corda-to-lora
    warnings.warn(msg)

tests/test_other.py::TestTargetingAuxiliaryTrainingWrapper::test_targeting_trainable_tokens_raises
tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/tuners/tuners_utils.py:899: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
    warnings.warn(

tests/test_randlora.py::TestRandLora::test_multiple_adapters_save_load_save_projection_false
tests/test_randlora.py::TestRandLora::test_multiple_adapters_save_projection_false_contains_no_randlora_A_randlora_B
  /app/src/peft/tuners/randlora/config.py:195: UserWarning: Specified to not save basis_A and basis_B within the state dictionary, instead they will be restored using the PRNG key store in `config.projection_prng_key`. Consider setting `config.save_projection` to `True` to guarantee restoring the checkpoint correctly on all system configurations.
    warnings.warn(

tests/test_initialization.py::TestEvaInitialization::test_eva_state_dict_adjust_scaling_factors[eva_config0]
tests/test_initialization.py::TestEvaInitialization::test_load_eva_state_dict[True]
tests/test_initialization.py::TestEvaInitialization::test_load_eva_state_dict[False]
tests/test_initialization.py::TestEvaInitialization::test_missing_eva_inits
tests/test_initialization.py::TestEvaInitialization::test_load_eva_model
  /app/src/peft/mapping_func.py:96: UserWarning: lora with eva initialization used with low_cpu_mem_usage=False. Setting low_cpu_mem_usage=True can improve the maximum batch size possible for eva initialization.
    warnings.warn(

tests/test_seq_classifier.py: 90 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-RobertaForSequenceClassification - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_seq_classifier.py: 90 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in trl-internal-testing/tiny-LlamaForSequenceClassification-3.2 - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_seq_classifier.py: 90 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-BertForSequenceClassification - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_initialization.py::TestHotSwapping::test_prepare_model_for_compiled_hotswap_lora_bias
  /app/src/peft/tuners/lora/layer.py:170: PeftWarning: `lora_bias=True` was passed but the targeted layer of type Linear has no bias. This means that merging LoRA weights won't be possible.
    warnings.warn(

tests/test_lora_variants.py::TestLoraVariants::test_variant_is_applied_to_layers[alora-LoraConfig-config_kwargs1]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets[input_ids0-alora_invocation_tokens0-expected_offsets0]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets[input_ids1-alora_invocation_tokens1-expected_offsets1]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets[input_ids2-alora_invocation_tokens2-expected_offsets2]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets_with_adapter_names[input_ids0-alora_invocations0-expected_offsets0]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets_with_adapter_names[input_ids1-alora_invocations1-expected_offsets1]
tests/test_lora_variants.py::TestActivatedLora::test_alora_activation_matches_base_until_invocation
tests/test_lora_variants.py::TestActivatedLora::test_input_embeds_warning
tests/test_lora_variants.py::TestActivatedLora::test_num_beams_error
  /app/src/peft/tuners/lora/config.py:741: UserWarning: aLoRA is currently only supported for CAUSAL_LM task.
    warnings.warn("aLoRA is currently only supported for CAUSAL_LM task.")

tests/test_target_parameters.py: 81 warnings
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'transformers.models.llama4.modeling_llama4.Llama4TextExperts'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_target_parameters.py: 19 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in trl-internal-testing/tiny-Llama4ForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_target_parameters.py: 80 warnings
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssExperts'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_target_parameters.py: 19 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in trl-internal-testing/tiny-GptOssForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_shira.py::TestShira::test_mlp_single_adapter_shapes
  /app/venv/lib/python3.11/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_shira.py::TestShira::test_mlp_single_adapter_shapes returned <class 'peft.peft_model.PeftModel'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_shira.py::TestShira::test_multiple_adapters_save_load
  /app/venv/lib/python3.11/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_shira.py::TestShira::test_multiple_adapters_save_load returned <class 'peft.peft_model.PeftModel'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_shira.py::TestShira::test_save_load_custom_mask_function
  /app/src/peft/tuners/shira/config.py:126: UserWarning: Argument self.mask_type='custom' is not recognized, please supply your own masking function by calling `config.mask_fn = my_mask_fn`.
    warnings.warn(

tests/test_shira.py::TestShira::test_save_load_custom_mask_function
  /app/venv/lib/python3.11/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_shira.py::TestShira::test_save_load_custom_mask_function returned <class 'peft.peft_model.PeftModel'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_shira.py::TestShira::test_save_load_default_random_mask_with_seed_function
  /app/venv/lib/python3.11/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_shira.py::TestShira::test_save_load_default_random_mask_with_seed_function returned <class 'peft.peft_model.PeftModel'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_trainable_tokens.py::TestTrainableTokens::test_save_pretrained_auto[peft_config0-True]
tests/test_trainable_tokens.py::TestTrainableTokens::test_save_pretrained_auto[peft_config1-True]
  /app/src/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
    warnings.warn(

tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_0_HuggingFaceH4_tiny_random_LlamaForCausalLM
tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_1_HuggingFaceH4_tiny_random_LlamaForCausalLM
tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_2_hf_internal_testing_tiny_random_gpt2
tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_3_hf_internal_testing_tiny_random_t5
tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_4_hf_internal_testing_tiny_random_GPTNeoXForCausalLM
  /app/src/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
    warnings.warn(

tests/test_tuners_utils.py::TestModelAndLayerStatus::test_merge_adapters_large
  /app/src/peft/tuners/tuners_utils.py:1678: UserWarning: Already following adapters were merged other. You are now additionally merging default.
    warnings.warn(

tests/test_tuners_utils.py::TestModelAndLayerStatus::test_model_merged_adapters_large
  /app/src/peft/tuners/lora/layer.py:984: UserWarning: Already unmerged. Nothing to do.
    warnings.warn("Already unmerged. Nothing to do.")

tests/test_tuners_utils.py::TestModelAndLayerStatus::test_model_merged_adapters_large
  /app/src/peft/tuners/lora/layer.py:1317: UserWarning: Already unmerged. Nothing to do.
    warnings.warn("Already unmerged. Nothing to do.")

tests/test_vera.py::TestVera::test_multiple_adapters_save_load_save_projection_false
  /app/src/peft/utils/save_and_load.py:505: UserWarning: Specified to not load vera_A and vera_B from state dictionary. This means we will be relying on PRNG initialisation to restore these projections using `config.projection_prng_key`, which may not be accurate on all system configurations.
    warnings.warn(

tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/utils/save_and_load.py:279: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.
    warnings.warn("Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.")

tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/tuners/tuners_utils.py:899: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
    warnings.warn(

tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/tuners/tuners_utils.py:899: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
    warnings.warn(

tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/tuners/tuners_utils.py:899: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens', 'model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
_____________ coverage: platform linux, python 3.11.0-candidate-1 ______________

Name                                                  Stmts   Miss  Cover   Missing
-----------------------------------------------------------------------------------
src/peft/__init__.py                                     10      0   100%
src/peft/auto.py                                         71      4    94%   61, 92, 99, 111
src/peft/config.py                                      133      5    96%   89, 156, 242, 267-268
src/peft/functional.py                                    4      4     0%   21-26
src/peft/helpers.py                                      72     21    71%   48-58, 84-98, 124-132, 235
src/peft/import_utils.py                                 94     42    55%   33-35, 41-46, 55-73, 87-98, 131-147, 158, 161-166
src/peft/mapping.py                                      18      3    83%   44, 78, 81
src/peft/mapping_func.py                                 36      0   100%
src/peft/mixed_model.py                                 151     24    84%   48, 51-54, 116, 124, 141, 147, 167-169, 239-242, 280, 297, 378, 387, 442-445, 449, 454, 458
src/peft/optimizers/__init__.py                           3      0   100%
src/peft/optimizers/lorafa.py                           105     25    76%   67, 69, 71, 73, 93, 113, 127-131, 157, 176, 182-210
src/peft/optimizers/loraplus.py                          36     27    25%   61-121
src/peft/peft_model.py                                 1342    425    68%   167-170, 176, 223, 232, 249, 264, 304, 328-331, 454, 474, 477-504, 509, 512, 516-538, 582, 635, 659, 684, 703-704, 751-752, 760, 770-772, 796, 811, 845, 851-857, 876-878, 913-915, 923, 1008, 1174-1241, 1246, 1376-1423, 1455, 1462, 1479, 1532, 1537-1539, 1543-1546, 1552, 1678-1730, 1743-1796, 1869-1871, 1882, 1904-1905, 1907-1908, 2044, 2052, 2062-2067, 2076-2098, 2108-2111, 2122, 2239-2243, 2246-2247, 2249-2250, 2293-2323, 2340, 2342-2345, 2347-2350, 2362-2363, 2384-2390, 2402, 2480-2481, 2513-2520, 2534-2587, 2600-2636, 2701-2702, 2734-2741, 2758-2814, 2828-2879, 2945, 2966-2967, 2969-2970, 3071, 3216
src/peft/tuners/__init__.py                              29      0   100%
src/peft/tuners/_buffer_dict.py                          62      8    87%   83, 92-94, 123, 137, 141, 159
src/peft/tuners/adalora/__init__.py                      16      7    56%   33-43
src/peft/tuners/adalora/bnb.py                           74     74     0%   15-143
src/peft/tuners/adalora/config.py                        36      2    94%   88, 92
src/peft/tuners/adalora/gptq.py                          31     26    16%   30-37, 40-67
src/peft/tuners/adalora/layer.py                        219     94    57%   31, 53, 58, 126, 142, 152-153, 169, 217, 236-252, 256-273, 278, 281-283, 286-335, 339-347, 351-360
src/peft/tuners/adalora/model.py                        152     79    48%   78, 104, 129, 136, 159-161, 163, 168, 181-188, 190-198, 200, 204-208, 217, 230-252, 256-284, 287-300, 323-342, 346
src/peft/tuners/adaption_prompt/__init__.py               6      0   100%
src/peft/tuners/adaption_prompt/config.py                25      1    96%   81
src/peft/tuners/adaption_prompt/layer.py                 86      5    94%   39, 54, 171, 185, 192
src/peft/tuners/adaption_prompt/model.py                 85      5    94%   64, 72, 98, 100, 168
src/peft/tuners/adaption_prompt/utils.py                 68     34    50%   47-58, 73, 84-89, 100-128, 141-146
src/peft/tuners/boft/__init__.py                          6      0   100%
src/peft/tuners/boft/config.py                           30      3    90%   152, 154, 158
src/peft/tuners/boft/fbd/__init__.py                      0      0   100%
src/peft/tuners/boft/layer.py                           495     97    80%   65-71, 78, 130-132, 136-138, 168, 234, 240-244, 247-254, 257-261, 283, 288, 301, 306-311, 319, 324-329, 336, 342-346, 388, 422-446, 528, 550-551, 583, 595, 625, 639, 713, 718, 724, 736, 741-746, 754, 759-764, 771, 777-781, 872-873, 916, 928, 951, 962, 975
src/peft/tuners/boft/model.py                            35      5    86%   78, 111, 117-121, 126
src/peft/tuners/bone/__init__.py                          6      0   100%
src/peft/tuners/bone/config.py                           26      2    92%   120, 124
src/peft/tuners/bone/layer.py                           186     29    84%   46, 64, 74, 79, 99-106, 109-113, 169, 188-189, 273-295, 327, 342-343
src/peft/tuners/bone/model.py                            29      3    90%   89, 115, 122
src/peft/tuners/c3a/__init__.py                           6      0   100%
src/peft/tuners/c3a/config.py                            23      2    91%   133, 137
src/peft/tuners/c3a/layer.py                            112     13    88%   47, 51, 61, 97, 103-108, 155, 171-172, 192
src/peft/tuners/c3a/model.py                             32      2    94%   63, 90
src/peft/tuners/c3a/utils.py                             30      0   100%
src/peft/tuners/cpt/__init__.py                           5      0   100%
src/peft/tuners/cpt/config.py                            35      1    97%   106
src/peft/tuners/cpt/model.py                             84      0   100%
src/peft/tuners/fourierft/__init__.py                     6      0   100%
src/peft/tuners/fourierft/config.py                      30      3    90%   199, 203, 206
src/peft/tuners/fourierft/layer.py                      104      5    95%   52, 58, 60, 159-160
src/peft/tuners/fourierft/model.py                       47      5    89%   67, 102, 108-112, 121
src/peft/tuners/hra/__init__.py                           6      0   100%
src/peft/tuners/hra/config.py                            27      3    89%   125, 129, 133
src/peft/tuners/hra/layer.py                            248     40    84%   50, 70, 85, 99-100, 111-118, 121-125, 177, 193-194, 213-220, 262, 330, 358-359, 385-392, 424, 447
src/peft/tuners/hra/model.py                             31      3    90%   89, 117, 126
src/peft/tuners/ia3/__init__.py                          15      7    53%   29-39
src/peft/tuners/ia3/bnb.py                               67     67     0%   15-129
src/peft/tuners/ia3/config.py                            22      0   100%
src/peft/tuners/ia3/layer.py                            194     10    95%   44, 50, 139-140, 246, 265-266, 298, 322, 330
src/peft/tuners/ia3/model.py                            120     23    81%   80-82, 85, 92, 97-105, 107-115, 138, 192, 198, 219, 222, 234, 241, 247, 251, 256, 284, 307, 314
src/peft/tuners/ln_tuning/__init__.py                     5      0   100%
src/peft/tuners/ln_tuning/config.py                      13      0   100%
src/peft/tuners/ln_tuning/layer.py                       64      7    89%   76, 82-83, 93-94, 106, 112
src/peft/tuners/ln_tuning/model.py                       44      1    98%   102
src/peft/tuners/loha/__init__.py                          6      0   100%
src/peft/tuners/loha/config.py                           25      1    96%   143
src/peft/tuners/loha/layer.py                           207      2    99%   126, 166
src/peft/tuners/loha/model.py                            22      0   100%
src/peft/tuners/lokr/__init__.py                          6      0   100%
src/peft/tuners/lokr/config.py                           28      1    96%   155
src/peft/tuners/lokr/layer.py                           230     15    93%   82-88, 146-147, 156, 186, 239, 286, 479-481
src/peft/tuners/lokr/model.py                            23      0   100%
src/peft/tuners/lora/__init__.py                         21      6    71%   50-52, 55-57, 60-62
src/peft/tuners/lora/aqlm.py                             48     31    35%   25, 42-49, 62-85, 88-89, 111-112
src/peft/tuners/lora/arrow.py                           210     16    92%   177, 212, 217, 316, 318, 339, 346-347, 351, 389, 400, 411, 420, 434, 444, 450
src/peft/tuners/lora/awq.py                              55     37    33%   39-50, 62-85, 88-89, 105-119
src/peft/tuners/lora/bnb.py                             316    316     0%   14-611
src/peft/tuners/lora/config.py                          134      8    94%   117, 119, 707, 709, 714-715, 724-727
src/peft/tuners/lora/corda.py                           173     17    90%   51, 111, 168, 181, 186, 246, 262, 284, 293, 298, 303, 333, 337, 342, 347, 352, 357
src/peft/tuners/lora/dora.py                            107      4    96%   48, 63, 97, 164
src/peft/tuners/lora/eetq.py                             55     42    24%   24-96, 112-116
src/peft/tuners/lora/eva.py                             333     55    83%   63, 67, 82-102, 132-135, 156-157, 164-165, 221-222, 234-238, 250, 253, 273, 277, 310-313, 322, 332-334, 383-385, 392, 434-436, 454, 461, 470, 544, 632, 713, 718, 722, 727
src/peft/tuners/lora/gptq.py                             65     43    34%   42-52, 66-80, 84-114, 117-118, 142-146, 151-152
src/peft/tuners/lora/hqq.py                             132    115    13%   30-237, 249
src/peft/tuners/lora/inc.py                              23     10    57%   31-59, 71-76
src/peft/tuners/lora/layer.py                          1182    104    91%   58, 63, 91, 167, 220-221, 250, 260, 271, 281, 285, 299-306, 308-313, 322, 341, 358, 368, 380, 384, 388, 394, 400, 406, 428-446, 479, 492, 505, 701, 892, 901, 923, 967, 1047, 1091, 1136, 1181, 1197, 1225, 1265, 1278, 1291, 1360, 1367, 1399, 1431, 1448, 1465, 1516, 1534, 1635, 1643, 1789-1790, 1870-1873, 1913, 1920, 1922, 1924, 1926, 1928, 1965, 1971, 1978, 1991, 2001-2002, 2004-2005, 2007-2008, 2010-2011, 2013, 2015-2016, 2023, 2042, 2047, 2075-2076, 2081, 2085, 2104, 2126, 2166-2167, 2179, 2203, 2255-2259
src/peft/tuners/lora/model.py                           346     49    86%   170, 221, 243, 270, 272, 295, 310-312, 315-317, 341, 393, 428, 430, 437, 449, 453, 497, 501, 503, 509, 515, 573, 601-605, 616-617, 624, 680, 694, 698-702, 713-717, 719-720, 740-744, 763, 779
src/peft/tuners/lora/torchao.py                          80     57    29%   35-40, 44-54, 57-91, 94-124, 127-128, 150-156
src/peft/tuners/lora/tp_layer.py                        169    140    17%   56-99, 117-186, 189-216, 231-256, 262-270, 280-304, 307-308, 325, 333-346
src/peft/tuners/lora/variants.py                        396     66    83%   55, 121, 129, 144-152, 229-230, 248, 338, 407-408, 454-486, 490, 494, 498, 502, 512-541, 551, 555, 559, 580, 621, 637-639, 694-695, 726, 750
src/peft/tuners/lycoris_utils.py                        104     26    75%   98-101, 140, 153-156, 159-166, 173-174, 181-188, 241-242, 257-258
src/peft/tuners/miss/__init__.py                          6      0   100%
src/peft/tuners/miss/config.py                           26      2    92%   136, 140
src/peft/tuners/miss/layer.py                           214     41    81%   50, 70, 75, 86, 91, 94-100, 116, 122-129, 132-136, 190-191, 197, 207-208, 219-220, 231, 307, 310-332, 364, 378, 383-384
src/peft/tuners/miss/model.py                            29      3    90%   89, 119, 126
src/peft/tuners/mixed/__init__.py                         2      0   100%
src/peft/tuners/mixed/model.py                          166     40    76%   82, 102-107, 119, 127-131, 150-160, 167, 172, 182-187, 195-196, 204, 220, 249-255, 260, 270, 276
src/peft/tuners/multitask_prompt_tuning/__init__.py       5      0   100%
src/peft/tuners/multitask_prompt_tuning/config.py        21      0   100%
src/peft/tuners/multitask_prompt_tuning/model.py         48      4    92%   38, 65, 71-73
src/peft/tuners/oft/__init__.py                          19     10    47%   37-52
src/peft/tuners/oft/aqlm.py                              41     25    39%   25, 45-49, 64-82, 85-86, 97, 102-103
src/peft/tuners/oft/awq.py                               49     32    35%   42-50, 64-83, 86-87, 98, 103-117
src/peft/tuners/oft/bnb.py                              188    188     0%   14-388
src/peft/tuners/oft/config.py                            48      4    92%   174, 176, 180, 194
src/peft/tuners/oft/eetq.py                              48     36    25%   24-94, 105, 110-114
src/peft/tuners/oft/gptq.py                              49     30    39%   41-48, 63-84, 87-88, 99, 106-110, 115-116
src/peft/tuners/oft/hqq.py                               94     79    16%   29-172, 179, 184
src/peft/tuners/oft/inc.py                               23     11    52%   31-59, 66, 71-76
src/peft/tuners/oft/layer.py                            430     80    81%   52-69, 199, 224, 255, 350-374, 386-390, 393-400, 403-407, 458-460, 463, 506, 513-515, 519-526, 600, 621-622, 661, 736, 744, 749-755, 758-760, 763, 856-857, 906, 931, 939-943
src/peft/tuners/oft/model.py                             54      9    83%   101, 122, 154-156, 159-161, 183, 197, 199
src/peft/tuners/p_tuning/__init__.py                      5      0   100%
src/peft/tuners/p_tuning/config.py                       17      0   100%
src/peft/tuners/p_tuning/model.py                        34      7    79%   84-96, 119, 124, 128
src/peft/tuners/poly/__init__.py                          6      0   100%
src/peft/tuners/poly/config.py                           21      0   100%
src/peft/tuners/poly/layer.py                            89      9    90%   49, 56, 103-108, 137
src/peft/tuners/poly/model.py                            52      5    90%   43, 52, 58, 65, 73
src/peft/tuners/poly/router.py                           34      3    91%   31, 64, 66
src/peft/tuners/prefix_tuning/__init__.py                 5      0   100%
src/peft/tuners/prefix_tuning/config.py                  10      0   100%
src/peft/tuners/prefix_tuning/model.py                   19      4    79%   65-66, 76-77
src/peft/tuners/prompt_tuning/__init__.py                 5      0   100%
src/peft/tuners/prompt_tuning/config.py                  23      0   100%
src/peft/tuners/prompt_tuning/model.py                   30      0   100%
src/peft/tuners/randlora/__init__.py                     15      7    53%   30-40
src/peft/tuners/randlora/bnb.py                         209    209     0%   14-456
src/peft/tuners/randlora/config.py                       26      0   100%
src/peft/tuners/randlora/layer.py                       176     13    93%   80-81, 106, 109, 133, 147, 150, 159, 162, 235, 251-252, 339
src/peft/tuners/randlora/model.py                       144     27    81%   60, 119-123, 133-134, 239, 255, 292-294, 297, 304, 309-317, 319-327, 330-343
src/peft/tuners/road/__init__.py                         15      7    53%   37-47
src/peft/tuners/road/bnb.py                             195    195     0%   14-407
src/peft/tuners/road/config.py                           21      1    95%   124
src/peft/tuners/road/layer.py                           199      8    96%   70, 102, 265, 276, 298-299, 380, 411
src/peft/tuners/road/model.py                            77      5    94%   55, 97-99, 102-104
src/peft/tuners/shira/__init__.py                         6      0   100%
src/peft/tuners/shira/config.py                          25      0   100%
src/peft/tuners/shira/layer.py                          102     12    88%   48, 64, 72, 95, 103, 106-109, 128, 163, 174-175
src/peft/tuners/shira/mask_functions.py                  14      0   100%
src/peft/tuners/shira/model.py                           42      6    86%   72, 81, 109, 115-121
src/peft/tuners/trainable_tokens/__init__.py              6      0   100%
src/peft/tuners/trainable_tokens/config.py               13      0   100%
src/peft/tuners/trainable_tokens/layer.py               115     13    89%   88-105, 127, 180, 187-188, 242
src/peft/tuners/trainable_tokens/model.py                41      0   100%
src/peft/tuners/tuners_utils.py                         775     82    89%   78-79, 88-105, 110, 114-123, 160, 177, 181, 184, 187, 190, 193, 196, 199, 284, 346, 434-439, 456-460, 478, 499, 648, 857, 859, 870, 872, 987, 1035-1039, 1046, 1048, 1051-1054, 1214, 1226, 1229, 1251, 1373, 1393, 1497, 1530, 1575, 1603, 1671, 1718, 1725-1730, 1732, 1749-1754, 1789-1790
src/peft/tuners/vblora/__init__.py                        6      0   100%
src/peft/tuners/vblora/config.py                         29      1    97%   196
src/peft/tuners/vblora/layer.py                         130      5    96%   75, 77, 165, 175-176
src/peft/tuners/vblora/model.py                          77     12    84%   90, 127, 133-137, 146, 184-189, 205-206
src/peft/tuners/vera/__init__.py                         15      7    53%   30-40
src/peft/tuners/vera/bnb.py                             208    208     0%   14-411
src/peft/tuners/vera/config.py                           28      1    96%   156
src/peft/tuners/vera/layer.py                           149      7    95%   81, 99, 115, 125, 208-209, 268
src/peft/tuners/vera/model.py                           119     18    85%   57, 123, 133-134, 194, 229-231, 234, 241, 246-254, 256-264, 267-271, 280
src/peft/tuners/xlora/__init__.py                         5      0   100%
src/peft/tuners/xlora/classifier.py                      88     11    88%   74-77, 80, 118-120, 142-143, 185-186
src/peft/tuners/xlora/config.py                          36      3    92%   94, 97, 102
src/peft/tuners/xlora/layer.py                          110     29    74%   114, 158, 161, 170-171, 186, 194-223
src/peft/tuners/xlora/model.py                          209     17    92%   75-85, 148, 241, 257, 261, 266-267, 350, 399, 405, 437, 442, 445
src/peft/utils/__init__.py                                7      0   100%
src/peft/utils/constants.py                              63     16    75%   23-32, 37-43, 50, 59
src/peft/utils/hotswap.py                               201     28    86%   46, 50, 75-76, 130, 136, 172, 175, 210, 216, 356, 360, 423, 457, 474-500, 542
src/peft/utils/incremental_pca.py                       148     10    93%   74, 76-77, 103, 121, 142, 146, 148, 199-200
src/peft/utils/integrations.py                          159     84    47%   33, 45-49, 61-62, 65-66, 70-73, 79-86, 94-124, 130, 132, 142-163, 175-191, 214, 228-230, 234, 242-246, 251, 253, 258, 260
src/peft/utils/loftq_utils.py                           234    202    14%   37-49, 53-61, 65-87, 90-103, 106-113, 116-154, 158-170, 177-187, 192-237, 242-258, 270-308, 311-327, 366-409
src/peft/utils/merge_utils.py                            79      7    91%   91-92, 94, 100, 120-123
src/peft/utils/other.py                                 652    144    78%   113, 115, 117, 119, 121, 170, 181-209, 223-232, 263, 267, 274, 316, 322, 333, 360-368, 377-381, 384, 389, 393, 456, 473, 479, 483, 492, 500, 544-547, 554-557, 591, 622, 632, 653-654, 691, 711-713, 770-773, 833, 866-867, 898, 910, 957, 1019, 1030, 1043, 1086, 1101-1105, 1108-1114, 1140-1144, 1154, 1172, 1183-1209, 1216-1246, 1265-1267, 1289-1293, 1309, 1328-1329, 1437-1479
src/peft/utils/peft_types.py                             60      6    90%   141, 144, 147, 160, 163, 167
src/peft/utils/save_and_load.py                         337     46    86%   54, 103-112, 117-119, 138-149, 171, 193, 208-211, 228, 236, 342, 349, 424, 476, 479, 495, 499, 523-527, 600, 625, 657-658, 665, 683
src/peft/utils/warning.py                                 1      0   100%
-----------------------------------------------------------------------------------
TOTAL                                                 16919   4476    74%
============================= slowest 10 durations =============================
96.90s call     tests/test_xlora.py::TestXlora::test_save_load_functional_pt
85.30s call     tests/test_xlora.py::TestXlora::test_scalings_logging_methods
67.30s call     tests/test_xlora.py::TestXlora::test_save_load_functional
45.53s call     tests/test_xlora.py::TestXlora::test_misc_methods
41.36s call     tests/test_xlora.py::TestXlora::test_disable_adapter
29.16s call     tests/test_xlora.py::TestXlora::test_topk_lora
26.23s call     tests/test_xlora.py::TestXlora::test_softmax_topk
24.18s call     tests/test_xlora.py::TestXlora::test_functional
22.99s call     tests/test_xlora.py::TestXlora::test_forward_hooks_are_cleaned_up
19.36s call     tests/test_poly.py::TestPoly::test_poly
=========================== short test summary info ============================
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 1 OFT-MLP-OFTConfig-config_kwargs96]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 2 OFT-MLP-OFTConfig-config_kwargs97]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 5 OFT-MLP-OFTConfig-config_kwargs98]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 6 OFT-MLP-OFTConfig-config_kwargs99]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 7 OFT-MLP-OFTConfig-config_kwargs100]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 8 OFT-MLP-OFTConfig-config_kwargs101]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 9 OFT-MLP-OFTConfig-config_kwargs102]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 10 OFT-MLP-OFTConfig-config_kwargs103]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 11 OFT-MLP-OFTConfig-config_kwargs104]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 12 OFT-MLP-OFTConfig-config_kwargs105]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Vanilla MLP 13 OFT-MLP-OFTConfig-config_kwargs106]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Conv2d 1 OFT-Conv2d-OFTConfig-config_kwargs107]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Conv2d 3 OFT-Conv2d-OFTConfig-config_kwargs108]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Conv2d 4 OFT-Conv2d-OFTConfig-config_kwargs109]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_load_model_low_cpu_mem_usage[Conv2d 5 OFT-Conv2d-OFTConfig-config_kwargs110]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_disable_adapters_with_merging[Conv2d 1 OFT-Conv2d-OFTConfig-config_kwargs107]
FAILED tests/test_custom_models.py::TestPeftCustomModel::test_disable_adapters_with_merging[Conv2d 4 OFT-Conv2d-OFTConfig-config_kwargs109]
FAILED tests/test_initialization.py::TestOft::test_load_outdated_oft_checkpoint_warns[0.17.0]
FAILED tests/test_initialization.py::TestOft::test_load_outdated_oft_checkpoint_warns[None]
FAILED tests/test_helpers.py::TestScalingAdapters::test_diffusers_pipeline - ...
FAILED tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_stable_diffusion
FAILED tests/test_initialization.py::TestHotSwapping::test_hotswap_works[True-config0]
FAILED tests/test_initialization.py::TestHotSwapping::test_hotswap_works[True-config1]
FAILED tests/test_tuners_utils.py::PeftCustomKwargsTester::test_maybe_include_all_linear_layers_diffusion
ERROR tests/test_stablediffusion.py - TypeError: CLIPTextModel.__init__() got...
ERROR tests/test_stablediffusion.py - TypeError: CLIPTextModel.__init__() got...
ERROR tests/test_stablediffusion.py - TypeError: CLIPTextModel.__init__() got...
= 24 failed, 13875 passed, 4334 skipped, 10 xfailed, 3 xpassed, 11174 warnings, 3 errors in 972.88s (0:16:12) =
make: *** [Makefile:20: test] Error 1
