Requirement already satisfied: pip in ./.venv/lib/python3.11/site-packages (25.2)
Obtaining file:///app
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (2.3.3)
Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (25.0)
Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (7.1.0)
Requirement already satisfied: pyyaml in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (6.0.3)
Requirement already satisfied: torch>=1.13.0 in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (2.8.0)
Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (4.57.0)
Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (4.67.1)
Requirement already satisfied: accelerate>=0.21.0 in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (1.10.1)
Requirement already satisfied: safetensors in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (0.6.2)
Requirement already satisfied: huggingface_hub>=0.25.0 in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (0.35.3)
Requirement already satisfied: black in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (25.9.0)
Requirement already satisfied: hf-doc-builder in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (0.5.0)
Requirement already satisfied: ruff~=0.12.8 in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (0.12.12)
Requirement already satisfied: pytest in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (8.4.2)
Requirement already satisfied: pytest-cov in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (7.0.0)
Requirement already satisfied: pytest-xdist in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (3.8.0)
Requirement already satisfied: parameterized in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (0.9.0)
Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (4.1.1)
Requirement already satisfied: diffusers in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (0.35.1)
Requirement already satisfied: scipy in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (1.16.2)
Requirement already satisfied: protobuf in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (6.32.1)
Requirement already satisfied: sentencepiece in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (0.2.1)
Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (3.19.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (2025.9.0)
Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (2.32.5)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (4.15.0)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (1.1.10)
Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (1.14.0)
Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (3.5)
Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (3.1.6)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.8.93)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.8.90)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.8.90)
Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (9.10.2.21)
Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.8.4.1)
Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (11.3.3.83)
Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (10.3.9.90)
Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (11.7.3.90)
Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.5.8.93)
Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (0.7.1)
Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (2.27.3)
Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.8.90)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.8.93)
Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (1.13.1.3)
Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (3.4.0)
Requirement already satisfied: setuptools>=40.8.0 in ./.venv/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.13.0->peft==0.17.2.dev0) (59.6.0)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.17.2.dev0) (1.3.0)
Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.11/site-packages (from black->peft==0.17.2.dev0) (8.3.0)
Requirement already satisfied: mypy-extensions>=0.4.3 in ./.venv/lib/python3.11/site-packages (from black->peft==0.17.2.dev0) (1.1.0)
Requirement already satisfied: pathspec>=0.9.0 in ./.venv/lib/python3.11/site-packages (from black->peft==0.17.2.dev0) (0.12.1)
Requirement already satisfied: platformdirs>=2 in ./.venv/lib/python3.11/site-packages (from black->peft==0.17.2.dev0) (4.4.0)
Requirement already satisfied: pytokens>=0.1.10 in ./.venv/lib/python3.11/site-packages (from black->peft==0.17.2.dev0) (0.1.10)
Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.11/site-packages (from datasets->peft==0.17.2.dev0) (21.0.0)
Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets->peft==0.17.2.dev0) (0.4.0)
Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from datasets->peft==0.17.2.dev0) (2.3.3)
Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets->peft==0.17.2.dev0) (3.6.0)
Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.11/site-packages (from datasets->peft==0.17.2.dev0) (0.70.16)
Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (3.13.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (25.4.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (1.8.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (6.7.0)
Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (0.4.0)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (1.22.0)
Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (3.10)
Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0) (3.4.3)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0) (2025.10.5)
Requirement already satisfied: importlib_metadata in ./.venv/lib/python3.11/site-packages (from diffusers->peft==0.17.2.dev0) (8.7.0)
Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from diffusers->peft==0.17.2.dev0) (2025.9.18)
Requirement already satisfied: Pillow in ./.venv/lib/python3.11/site-packages (from diffusers->peft==0.17.2.dev0) (11.3.0)
Requirement already satisfied: GitPython in ./.venv/lib/python3.11/site-packages (from hf-doc-builder->peft==0.17.2.dev0) (3.1.45)
Requirement already satisfied: nbformat in ./.venv/lib/python3.11/site-packages (from hf-doc-builder->peft==0.17.2.dev0) (5.10.4)
Requirement already satisfied: gitdb<5,>=4.0.1 in ./.venv/lib/python3.11/site-packages (from GitPython->hf-doc-builder->peft==0.17.2.dev0) (4.0.12)
Requirement already satisfied: smmap<6,>=3.0.1 in ./.venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython->hf-doc-builder->peft==0.17.2.dev0) (5.0.2)
Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.11/site-packages (from importlib_metadata->diffusers->peft==0.17.2.dev0) (3.23.0)
Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft==0.17.2.dev0) (3.0.3)
Requirement already satisfied: fastjsonschema>=2.15 in ./.venv/lib/python3.11/site-packages (from nbformat->hf-doc-builder->peft==0.17.2.dev0) (2.21.2)
Requirement already satisfied: jsonschema>=2.6 in ./.venv/lib/python3.11/site-packages (from nbformat->hf-doc-builder->peft==0.17.2.dev0) (4.25.1)
Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.venv/lib/python3.11/site-packages (from nbformat->hf-doc-builder->peft==0.17.2.dev0) (5.8.1)
Requirement already satisfied: traitlets>=5.1 in ./.venv/lib/python3.11/site-packages (from nbformat->hf-doc-builder->peft==0.17.2.dev0) (5.14.3)
Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat->hf-doc-builder->peft==0.17.2.dev0) (2025.9.1)
Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat->hf-doc-builder->peft==0.17.2.dev0) (0.36.2)
Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat->hf-doc-builder->peft==0.17.2.dev0) (0.27.1)
Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->datasets->peft==0.17.2.dev0) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets->peft==0.17.2.dev0) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->datasets->peft==0.17.2.dev0) (2025.2)
Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->peft==0.17.2.dev0) (1.17.0)
Requirement already satisfied: iniconfig>=1 in ./.venv/lib/python3.11/site-packages (from pytest->peft==0.17.2.dev0) (2.1.0)
Requirement already satisfied: pluggy<2,>=1.5 in ./.venv/lib/python3.11/site-packages (from pytest->peft==0.17.2.dev0) (1.6.0)
Requirement already satisfied: pygments>=2.7.2 in ./.venv/lib/python3.11/site-packages (from pytest->peft==0.17.2.dev0) (2.19.2)
Requirement already satisfied: coverage>=7.10.6 in ./.venv/lib/python3.11/site-packages (from coverage[toml]>=7.10.6->pytest-cov->peft==0.17.2.dev0) (7.10.7)
Requirement already satisfied: execnet>=2.1 in ./.venv/lib/python3.11/site-packages (from pytest-xdist->peft==0.17.2.dev0) (2.1.1)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.11/site-packages (from transformers->peft==0.17.2.dev0) (0.22.1)
Building wheels for collected packages: peft
  Building editable for peft (pyproject.toml): started
  Building editable for peft (pyproject.toml): finished with status 'done'
  Created wheel for peft: filename=peft-0.17.2.dev0-0.editable-py3-none-any.whl size=10763 sha256=4de4fae8f80822e4c3b53dadcbc5d40db767a6b75c4a142bd0f708459d293de7
  Stored in directory: /tmp/pip-ephem-wheel-cache-ir704dt8/wheels/57/0f/98/bb57b2b57b95807699b822a35c022f139d38a02c27922f27ce
Successfully built peft
Installing collected packages: peft
  Attempting uninstall: peft
    Found existing installation: peft 0.17.2.dev0
    Uninstalling peft-0.17.2.dev0:
      Successfully uninstalled peft-0.17.2.dev0
Successfully installed peft-0.17.2.dev0
python -m pytest -n 3 tests/
============================= test session starts ==============================
platform linux -- Python 3.11.0rc1, pytest-8.4.2, pluggy-1.6.0
rootdir: /app
configfile: pyproject.toml
plugins: xdist-3.8.0, cov-7.0.0
created: 3/3 workers
3 workers [19619 items]

sssssssssssssssssssssssss............................................... [  0%]
........................................................................ [  0%]
........................................................................ [  1%]
........................................................................ [  1%]
........................................................................ [  1%]
........................................................................ [  2%]
........................................................................ [  2%]
........................................................................ [  2%]
........................................................................ [  3%]
........................................................................ [  3%]
........................................................................ [  4%]
........................................................................ [  4%]
........................................................................ [  4%]
........................................................................ [  5%]
........................................................................ [  5%]
........................................................................ [  5%]
.........................................ssss.ssssssssss.sssssssss.sssss [  6%]
sssss.ssssssss.sssssssss.ssssssss.sssssssss.sssssssss.ssssssss.sssssssss [  6%]
.ssssssss.sssssssss.ssssssss.sssssssss.ssssssss.sssssssss.sssssssss.ssss [  6%]
ssss.sssssssss.ssssssss.sssssssss.ssssssss.sssssssss.ssssssss........... [  7%]
........................................................................ [  7%]
........................................................................ [  8%]
.............................................................ss......... [  8%]
........................................................................ [  8%]
........................................................................ [  9%]
........................................................................ [  9%]
........................................................................ [  9%]
........................................................................ [ 10%]
..sssssssssss.sssssssssssssss.sssssssssss.sssssssssssss.ssssssssssssssss [ 10%]
sssss.ssssssssssssssss.ssssssssssssssss.sssssssssssssss.sssssssssss..... [ 11%]
........ssssssss.ssssssssssssss.ssssssssssss.ssssss..................... [ 11%]
........................................................................ [ 11%]
........................................................................ [ 12%]
........................................................................ [ 12%]
........................................................................ [ 12%]
........................................................................ [ 13%]
........................................................................ [ 13%]
........................................................................ [ 13%]
........................................................................ [ 14%]
........................................................................ [ 14%]
........................................................................ [ 15%]
........................................................................ [ 15%]
.ss..................................................................... [ 15%]
........................................................................ [ 16%]
........................................................................ [ 16%]
........................................................................ [ 16%]
........................................................................ [ 17%]
........................................................................ [ 17%]
........................................................................ [ 17%]
...................................sss.s................................ [ 18%]
........................................................................ [ 18%]
........................................................................ [ 19%]
..................................................ssssss.sssssssss...... [ 19%]
...........................ssssssssssss................................. [ 19%]
.............................................x.......................... [ 20%]
x.........................sss.s......................................... [ 20%]
...............x................................ssssssssss.sssssssssssss [ 20%]
sssss.sssssssssssssssssss.ssssss.sssssss.ssss.ssss.ss.............ssssss [ 21%]
sssssss.sssssssssssss.ssssssssssss.ss................................... [ 21%]
.........ss.ss.......................................................... [ 22%]
......................................ss............s..s................ [ 22%]
.....s.......................s......................s................... [ 22%]
...s......s.....s....s.....s...s....s..............................s.... [ 23%]
..s.ss.......s...............s...sss.......s.......................s.... [ 23%]
.................................................s...................... [ 23%]
....................................s................................... [ 24%]
..ssss....s...................................................s......... [ 24%]
............................................s........................... [ 24%]
........................................................................ [ 25%]
........................................................................ [ 25%]
........................................................................ [ 26%]
........................................................................ [ 26%]
........................................................................ [ 26%]
........................................................................ [ 27%]
........................................................................ [ 27%]
............................................ss.......................... [ 27%]
........................................................................ [ 28%]
.........................................................ss.sss......... [ 28%]
.............sss.ss..................................sss.sssss.ssssss.ss [ 28%]
sss.sssss.sssss.sssss.ssssss............................................ [ 29%]
.................s.s.................................................... [ 29%]
..................................................................sssss. [ 30%]
.............sssssssssss.............ss.sssssssssssssssssssss.ssssssssss [ 30%]
sssssss................................................................. [ 30%]
........................................................................ [ 31%]
........................................................................ [ 31%]
........................................................................ [ 31%]
.........ssssssssss.ssssssssssssssssssssssss.ssssssssssssssssss.ssssssss [ 32%]
sssssssss.sssssssssssssssss.ssssssssssssss..............ssssssssssssssss [ 32%]
sssssssss.ssssssssssssssssssssssssssss.ssssssssss....................... [ 33%]
........................................................................ [ 33%]
........................................................................ [ 33%]
........................................................................ [ 34%]
........................................................................ [ 34%]
........................................................................ [ 34%]
....................................................ssss................ [ 35%]
...........................ss........................................... [ 35%]
ss.......................................ss............................. [ 35%]
..........................x...x......................................... [ 36%]
........................................................................ [ 36%]
........................................................................ [ 37%]
...sssssssssssssssssssssss.sssssssssssssssssssssssssssss.sssssssssssssss [ 37%]
sssssssssssssssss.sssssssssssssssssssssssssssssss.ssssssssssssssssssssss [ 37%]
sssssssss.sssssssssssssssssssssssssssssss.ssssssssssssssssssssssssssssss [ 38%]
.sssss....s...........s...........s...................s......x.x........ [ 38%]
.................ss.......................s.......s...................s. [ 38%]
.................s....................s.................s..........s.... [ 39%]
.......................................s................................ [ 39%]
.............s.............s..........s............................s.... [ 40%]
.....ssssssssss.sssssssssssss.sssssssss.ssssssssssssssssss.sssssssssssss [ 40%]
sssssss................................................................. [ 40%]
........................................................................ [ 41%]
........................................................................ [ 41%]
........................................................................ [ 41%]
..........................s............................................. [ 42%]
........................................................s............... [ 42%]
............s..............s.s......................................s... [ 42%]
.......s................................................................ [ 43%]
.s...................................................................... [ 43%]
.............................................s..........s............... [ 44%]
........................................................................ [ 44%]
........................................................................ [ 44%]
...........................................................s............ [ 45%]
.............................................................s.......... [ 45%]
.............................................................sssssssss.. [ 45%]
s.........s............................................................. [ 46%]
........................................................................ [ 46%]
........................................................................ [ 46%]
.........................................s.............................. [ 47%]
........................................................................ [ 47%]
...................s.................................................... [ 48%]
........................................................................ [ 48%]
........................................................................ [ 48%]
........................................................................ [ 49%]
........................................................................ [ 49%]
.......................s.............................................s.s [ 49%]
................................................................ssssssss [ 50%]
.ssssssssssssssss.ss.sssssssssssssss.ssssssssssssssss.ssssssssssssssssss [ 50%]
.ssssssss.ssssssss.sssssssssssssss.ssssssssssssssss.sssssssssssss....... [ 51%]
.............................................................s.......... [ 51%]
............sssssssss.s.............s.s...........ssssssssssssssssssssss [ 51%]
ssssssssssssssssssssssssssssssss.s.............s...........sssssssssssss [ 52%]
sssss..............sssssssssssssssssssssssssssssssssssssssssssssssssssss [ 52%]
s..s.........sssssssssssssssssssssssssss.sssssssssssssssssssssssssssssss [ 52%]
ssssssssssssssssssssssssssssssss...............................s........ [ 53%]
..............................................ssssssssssssssssssss.sssss [ 53%]
sssssssssssssssssssssssssssss.........sssssssss.......................s. [ 53%]
....ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 54%]
sssssssssssss.sss.ssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 54%]
sssssssssssssssssssss.ssssssssssssssssssssssssssssss..............ss.sss [ 55%]
ssssssssssssssssssssssssssssssssssssssssssssssssss.ssssss.ssssssssssss.. [ 55%]
................s..s............s..............s..........s.ssssssss.... [ 55%]
.................s.........s...................s........................ [ 56%]
...............................................s.............ssssssss.ss [ 56%]
ssssssssss.sssssss.s..............s..................................... [ 56%]
...........s...............s..............sss.ssssssssssssssssssssssssss [ 57%]
ssssssssssssssssssssssssss.sssssssssssssssss..........s......s.......... [ 57%]
..........................................................s............. [ 57%]
.....s................s.........ssssssssssssssss.sssssssssssssssssssssss [ 58%]
sssssssssssssssssssssssssssssss.ssssssssssssssss.ssss................... [ 58%]
...s..............s.s.............s..................sssssssss.......... [ 59%]
.s................s...s......................s.ssssssssss......s........ [ 59%]
.................s....s....s..................ssssssssss.....s.........s [ 59%]
...........sssssssss..........s......s.................s............s.s. [ 60%]
..s.........................s................s.s.................sssssss [ 60%]
ssssssssssss.............................ssssssssssssssssss.ssssssssssss [ 60%]
sssssss...s..s................s....s.s................s....s.s.......... [ 61%]
........s..s..................s...s................s.....s....s......... [ 61%]
......s............s.......................s.s......................s... [ 62%]
........................................s........................s...... [ 62%]
...........................s...s.................................sssssss [ 62%]
ss...s........s...........................................s........s.... [ 63%]
.......................s......................s..............s.......... [ 63%]
..............................s.............s...s.............s......... [ 63%]
....s.s....................s..................................s.......s. [ 64%]
.................................................................s...... [ 64%]
........s....................sssssssssssssssssssssssssssssssssssssssss.s [ 64%]
ssssssssssssssssssssssssssssss......s............................s...... [ 65%]
.........s....sssssssssssssssssssssssssssssssss.ssssssssssssssssssssssss [ 65%]
sssssssssssssss.........s............................................... [ 66%]
....s.........................ssssssssssssssssssssssssssssssssssssssssss [ 66%]
ss.ssssssssss...................ssssssss.ssssssssssssssssssssssssssss..s [ 66%]
.............s...............s...........sssssssss................s..... [ 67%]
.......................sssssssss..s..................................... [ 67%]
....................................s......s.....sssssssssssss.sssssssss [ 67%]
sssssssssssssssssssssss...........s.........................ssssssssssss [ 68%]
sssssss.ssssssss..s.............s....sss.ss.ssssssssssssssssssssssssssss [ 68%]
sssssssssssssss.sssssssssssss.sssssssssssssssss.ssssssssss.ssssssss.ssss [ 68%]
sssssssssssssssssss.sssssssss.ssssssssssssssssss.sssssssssss.sssssssssss [ 69%]
ssssssssssssssssss.........sssssssss.........s...........s....s......... [ 69%]
...............s....................................................s... [ 70%]
.......................s............ss.sssssssssssssssssssssssssssssssss [ 70%]
ssssssssss....................sssssssssssssssssssssssssss............... [ 70%]
sssssssss...s..........sssssssssssssss.sssssssssssssssssssssssssssssssss [ 71%]
ssssss.........s........................................................ [ 71%]
........ssssssss.sssssssssssssssssss.sssssssssssssssssssssssssssssssssss [ 71%]
sssss.sssssssssssssssssss..............sssssss.sssssssssssssssssssss.sss [ 72%]
sssssssssssssssssssssssssssssssssssssssssssss.ssssssssssssssssssssssssss [ 72%]
sssssssssssssssssssssssssssssssssssssssssssssssss.ssssssssssssssss.sssss [ 73%]
ssssssssssssssssss.................................................sssss [ 73%]
ssssssssssssssssssssssssssssssssssssssssss.sssssss........s........s.... [ 73%]
............s.............ssssssssssssssssssssssssssssssssssssssssssssss [ 74%]
sssssssssssssssss..........s..ss...................s............ss...... [ 74%]
........s.......s......ss..........................s..........ss........ [ 74%]
...........................sssssssssssssssssssssssssssssssssssssssssssss [ 75%]
sssssss.ssssssssssssssssss.sssssssssssssssssssssssssssssssssssssssss..s. [ 75%]
.........s.....................s................s....................... [ 75%]
....................................................s................... [ 76%]
............................s........s..s............................... [ 76%]
....s.s.................................................s..s............ [ 77%]
....................................s...........s..........s............ [ 77%]
.............................s.......................................... [ 77%]
.........................................sssssssss...................... [ 78%]
........................s............................................... [ 78%]
....................s........ss............s...............s............ [ 78%]
.......................................s................................ [ 79%]
.........................sssss.sssss.ss............ssssssssss....ssssss. [ 79%]
ss.....sssssss.sssssssssssssssssssssssssssssssssssssss.....sssssssssssss [ 80%]
sss..ssssssssssssssssssss.....................................ssssss.... [ 80%]
........................ssssssssssssss...............sssssssssssssssssss [ 80%]
s..ssssssssssss..........................ss.....s....................... [ 81%]
.ssssssssssssssssssssssssssssssssssss.ssss........ssssssssssss.......... [ 81%]
........s........................................ssss.........ssssssssss [ 81%]
ssss.......ssssssssssss.....ssssssss.........................ssssssssss. [ 82%]
......ssssss............................................................ [ 82%]
........................................................s............... [ 82%]
........................................................................ [ 83%]
.......................................................ssss............. [ 83%]
........................................................................ [ 84%]
...........s............................................................ [ 84%]
........................................................................ [ 84%]
...................................ssss................ss............... [ 85%]
..............ssss.ssssssssssss........................................s [ 85%]
sssssssssssssssssssssssssssssssssssssss...ss............ssssssssssssssss [ 85%]
ssssssssssssss.ssssssssssssssssssssss.....................ssssssssssssss [ 86%]
sssssssssssss.sssssssssssss.sss...sssssssssss..............s.sssssssssss [ 86%]
ssss................................................ssssss.sssssssssssss [ 86%]
s......ssssssssssss...................................s................. [ 87%]
..ss............................ss..............ss......ss.....ss....ss. [ 87%]
...ss....ssssss...................................s.s...ss..........ssss [ 88%]
ssssssssss.ssssssssss................................................... [ 88%]
........sssssssssssss.ssssssssssssssss.sssssssssssssssssssssssssssssssss [ 88%]
ssssssssssssssssssssss...........sssssssssssssssssssssssssssssssssssssss [ 89%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 89%]
sssssssssssssssssssss.sssssssssssssssssssss.ssssssssssssssssssssssssss.. [ 89%]
................ss...................................................F.. [ 90%]
.........ssssssssssssssssssssssssssss..................sssssssssssssssss [ 90%]
sssssss.......ssssssssssssssss.......................................... [ 91%]
.......ss......ssssssssssssssssssss...........sss....................... [ 91%]
................X................ssssssss............................... [ 91%]
.........ssssssssssssss..................sssssssssssssssssssssssss.sssss [ 92%]
ssss.........................ssssssssssssssssssssss..................... [ 92%]
........................................................................ [ 92%]
........................................................................ [ 93%]
........................................................................ [ 93%]
...............................................sss...................... [ 93%]
..............................ssssssssssssssssssssssssssssssssssss.....s [ 94%]
sssssssssssssssss....................................................... [ 94%]
........................................................................ [ 95%]
........................................................................ [ 95%]
.....sss.........................sss........................F........... [ 95%]
.......F................................................................ [ 96%]
..................................................x.......x............. [ 96%]
...............ss....................................................... [ 96%]
.................................ssssssssssssssss.....................x. [ 97%]
.....F........ss.ssssss................ssssssssssssssss................. [ 97%]
...............................s..................................ssssss [ 97%]
ssssssssss.............................................ssssssssss.ssssss [ 98%]
........................................................................ [ 98%]
..................................................ss.................... [ 99%]
.....................................s.s.....s.......................... [ 99%]
...................................................................F.... [ 99%]
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.......ssssssssss..................                                      [100%]
==================================== ERRORS ====================================
________________ ERROR collecting tests/test_stablediffusion.py ________________
tests/test_stablediffusion.py:222: in <module>
    class TestStableDiffusionModel(PeftCommonTester):
tests/test_stablediffusion.py:228: in TestStableDiffusionModel
    sd_model = StableDiffusionPipeline.from_pretrained("hf-internal-testing/tiny-sd-pipe")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'
------------------------------- Captured stderr --------------------------------
Fetching 12 files:   0%|                                | 0/12 [00:00<?, ?it/s]Fetching 12 files:  17%|████                    | 2/12 [00:00<00:02,  4.31it/s]Fetching 12 files:  33%|████████                | 4/12 [00:00<00:01,  4.66it/s]Fetching 12 files: 100%|███████████████████████| 12/12 [00:00<00:00, 12.48it/s]
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  60%|███████▏    | 3/5 [00:00<00:00, 23.20it/s]Loading pipeline components...:  80%|█████████▌  | 4/5 [00:00<00:00, 29.00it/s]
________________ ERROR collecting tests/test_stablediffusion.py ________________
tests/test_stablediffusion.py:222: in <module>
    class TestStableDiffusionModel(PeftCommonTester):
tests/test_stablediffusion.py:228: in TestStableDiffusionModel
    sd_model = StableDiffusionPipeline.from_pretrained("hf-internal-testing/tiny-sd-pipe")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'
------------------------------- Captured stderr --------------------------------
Fetching 12 files:   0%|                                | 0/12 [00:00<?, ?it/s]Fetching 12 files:  17%|████                    | 2/12 [00:00<00:02,  3.48it/s]Fetching 12 files:  33%|████████                | 4/12 [00:01<00:02,  3.86it/s]Fetching 12 files: 100%|███████████████████████| 12/12 [00:01<00:00, 11.12it/s]
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]
________________ ERROR collecting tests/test_stablediffusion.py ________________
tests/test_stablediffusion.py:222: in <module>
    class TestStableDiffusionModel(PeftCommonTester):
tests/test_stablediffusion.py:228: in TestStableDiffusionModel
    sd_model = StableDiffusionPipeline.from_pretrained("hf-internal-testing/tiny-sd-pipe")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'
------------------------------- Captured stderr --------------------------------
Fetching 12 files:   0%|                                | 0/12 [00:00<?, ?it/s]Fetching 12 files:  17%|████                    | 2/12 [00:00<00:03,  3.16it/s]Fetching 12 files:  33%|████████                | 4/12 [00:01<00:02,  3.79it/s]Fetching 12 files: 100%|███████████████████████| 12/12 [00:01<00:00, 10.70it/s]
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  60%|███████▏    | 3/5 [00:00<00:00, 25.40it/s]Loading pipeline components...:  60%|███████▏    | 3/5 [00:00<00:00, 24.68it/s]
=================================== FAILURES ===================================
_________________ TestScalingAdapters.test_diffusers_pipeline __________________
[gw1] linux -- Python 3.11.0 /app/.venv/bin/python

self = <tests.test_helpers.TestScalingAdapters object at 0x7f1339e55b10>

    def test_diffusers_pipeline(self):
        model_id = "hf-internal-testing/tiny-sd-pipe"
>       pipeline = StableDiffusionPipeline.from_pretrained(model_id)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_helpers.py:185:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'transformers.models.clip.modeling_clip.CLIPTextModel'>
pretrained_model_name_or_path = '/root/.cache/huggingface/hub/models--hf-internal-testing--tiny-sd-pipe/snapshots/39f7e1819d772fbb8dd7f2aa3da8e2fc5a9bd917/text_encoder'
config = CLIPTextConfig {
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dty...en_layers": 5,
  "pad_token_id": 1,
  "projection_dim": 32,
  "transformers_version": "4.57.0",
  "vocab_size": 1000
}

cache_dir = None, ignore_mismatched_sizes = False, force_download = False
local_files_only = False, token = None, revision = 'main'
use_safetensors = None, weights_only = True, model_args = ()
kwargs = {'offload_state_dict': None}, state_dict = None

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        **kwargs,
    ) -> SpecificPreTrainedModelType:
        r"""
        Instantiate a pretrained pytorch model from a pre-trained model configuration.

        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you should first set it back in training mode with `model.train()`.

        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come
        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
        task.

        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those
        weights are discarded.

        Parameters:
            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):
                Can be either:

                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
                    - A path to a *directory* containing model weights saved using
                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In
                      this case, `from_tf` should be set to `True` and a configuration object should be provided as
                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a
                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.
                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,
                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to
                      `True`.
                    - `None` if you are both providing the configuration and state dictionary (resp. with keyword
                      arguments `config` and `state_dict`).
            model_args (sequence of positional arguments, *optional*):
                All remaining positional arguments will be passed to the underlying model's `__init__` method.
            config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):
                Can be either:

                    - an instance of a class derived from [`PretrainedConfig`],
                    - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].

                Configuration for the model to use instead of an automatically loaded configuration. Configuration can
                be automatically loaded when:

                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained
                      model).
                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the
                      save directory.
                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a
                      configuration JSON file named *config.json* is found in the directory.
            state_dict (`dict[str, torch.Tensor]`, *optional*):
                A state dictionary to use instead of a state dictionary loaded from saved weights file.

                This option can be used if you want to create a model from a pretrained configuration but load your own
                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and
                [`~PreTrainedModel.from_pretrained`] is not a simpler option.
            cache_dir (`Union[str, os.PathLike]`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the
                standard cache should not be used.
            from_tf (`bool`, *optional*, defaults to `False`):
                Load the model weights from a TensorFlow checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            from_flax (`bool`, *optional*, defaults to `False`):
                Load the model weights from a Flax checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):
                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size
                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a
                checkpoint with 3 labels).
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
                cached versions if they exist.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
            output_loading_info(`bool`, *optional*, defaults to `False`):
                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.
            local_files_only(`bool`, *optional*, defaults to `False`):
                Whether or not to only look at local files (i.e., do not try to download the model).
            token (`str` or `bool`, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use
                the token generated when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.

                <Tip>

                To test a pull request you made on the Hub, you can pass `revision="refs/pr/<pr_number>"`.

                </Tip>
            attn_implementation (`str`, *optional*):
                The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)), or `"flash_attention_3"` (using [Dao-AILab/flash-attention/hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.

                Accept HF kernel references in the form:
                  <namespace>/<repo_name>[@<revision>][:<kernel_name>]

                - <namespace> and <repo_name> are any non-"/" and non-":" sequences.
                - "@<revision>" is optional (branch, tag, or commit-ish), e.g. "@main", "@v1.2.0", "@abc123".
                - ":<kernel_name>" is optional and selects a function inside the kernel repo.
                - Both options can appear together and in this order only: @revision first, then :kernel_name.
                - We intentionally allow a leading "<wrapper>|" prefix (e.g., "flash|...") because the code
                  strips it before loading; '|' is not excluded in the character classes here.

                Examples that match:
                  "org/model"
                  "org/model@main"
                  "org/model:custom_kernel"
                  "org/model@v1.2.3:custom_kernel"

            > Parameters for big model inference

            dtype (`str` or `torch.dtype`, *optional*):
                Override the default `torch_dtype` and load the model under a specific `dtype`. The different options
                are:

                1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified
                  `dtype`, ignoring the model's `config.dtype` if one exists. If not specified
                  - the model will get loaded in `torch.float` (fp32).

                2. `"auto"` - A `dtype` or `torch_dtype` entry in the `config.json` file of the model will be
                  attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in
                  the checkpoint that's of a floating point type and use that as `dtype`. This will load the model
                  using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how
                  the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.

                3. A string that is a valid `torch.dtype`. E.g. "float32" loads the model in `torch.float32`, "float16" loads in `torch.float16` etc.

                <Tip>

                For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or
                reach out to the authors and ask them to add this information to the model's card and to insert the
                `dtype` or `torch_dtype` entry in `config.json` on the hub.

                </Tip>

            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):
                A map that specifies where each submodule should go. It doesn't need to be refined to each
                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
                same device. If we only pass the device (*e.g.*, `"cpu"`, `"cuda:1"`, `"mps"`, or a GPU ordinal rank
                like `1`) on which the model will be allocated, the device map will map the entire model to this
                device. Passing `device_map = 0` means put the whole model on GPU 0.

                To have Accelerate compute the most optimized `device_map` automatically, set `device_map="auto"`. For
                more information about each option see [designing a device
                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).
            max_memory (`Dict`, *optional*):
                A dictionary device identifier to maximum memory if using `device_map`. Will default to the maximum memory available for each
                GPU and the available CPU RAM if unset.
            tp_plan (`str`, *optional*):
                A torch tensor parallel plan, see [here](https://pytorch.org/tutorials/intermediate/TP_tutorial.html). Currently, it only accepts
                `tp_plan="auto"` to use predefined plan based on the model. Note that if you use it, you should launch your script accordingly with
                `torchrun [args] script.py`. This will be much faster than using a `device_map`, but has limitations.
            tp_size (`str`, *optional*):
                A torch tensor parallel degree. If not provided would default to world size.
            device_mesh (`torch.distributed.DeviceMesh`, *optional*):
                A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.
                If provided, it has to contain dimension named `"tp"` in case it's > 1 dimensional, this dimension will be used for tensor parallelism
            offload_folder (`str` or `os.PathLike`, *optional*):
                If the `device_map` contains any value `"disk"`, the folder where we will offload weights.
            offload_buffers (`bool`, *optional*):
                Whether or not to offload the buffers with the model parameters.
            quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):
                A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g
                bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and
                `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes
                quantizations and not preferred. consider inserting all such arguments into quantization_config
                instead.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            variant (`str`, *optional*):
                If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is
                ignored when using `from_tf` or `from_flax`.
            use_safetensors (`bool`, *optional*, defaults to `None`):
                Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`
                is not installed, it will be set to `False`.
            weights_only (`bool`, *optional*, defaults to `True`):
                Indicates whether unpickler should be restricted to loading only tensors, primitive types,
                dictionaries and any types added via torch.serialization.add_safe_globals().
                When set to False, we can load wrapper tensor subclass weights.
            key_mapping (`dict[str, str], *optional*):
                A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers
                architecture, but was not converted accordingly.
            kwargs (remaining dictionary of keyword arguments, *optional*):
                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or
                automatically loaded:

                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the
                      underlying model's `__init__` method (we assume all relevant updates to the configuration have
                      already been done)
                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class
                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that
                      corresponds to a configuration attribute will be used to override said attribute with the
                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute
                      will be passed to the underlying model's `__init__` function.

        <Tip>

        Activate the special ["offline-mode"](https://huggingface.co/transformers/installation.html#offline-mode) to
        use this method in a firewalled environment.

        </Tip>

        Examples:

        ```python
        >>> from transformers import BertConfig, BertModel

        >>> # Download model and configuration from huggingface.co and cache.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased")
        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
        >>> model = BertModel.from_pretrained("./test/saved_model/")
        >>> # Update configuration during loading.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", output_attentions=True)
        >>> assert model.config.output_attentions == True
        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).
        >>> config = BertConfig.from_json_file("./tf_model/my_tf_model_config.json")
        >>> model = BertModel.from_pretrained("./tf_model/my_tf_checkpoint.ckpt.index", from_tf=True, config=config)
        >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", from_flax=True)
        ```
        """
        state_dict = kwargs.pop("state_dict", None)
        from_tf = kwargs.pop("from_tf", False)
        from_flax = kwargs.pop("from_flax", False)
        proxies = kwargs.pop("proxies", None)
        output_loading_info = kwargs.pop("output_loading_info", False)
        use_auth_token = kwargs.pop("use_auth_token", None)
        from_pipeline = kwargs.pop("_from_pipeline", None)
        from_auto_class = kwargs.pop("_from_auto", False)
        dtype = kwargs.pop("dtype", None)
        torch_dtype = kwargs.pop("torch_dtype", None)  # kept for BC
        device_map = kwargs.pop("device_map", None)
        max_memory = kwargs.pop("max_memory", None)
        offload_folder = kwargs.pop("offload_folder", None)
        offload_buffers = kwargs.pop("offload_buffers", False)
        load_in_8bit = kwargs.pop("load_in_8bit", False)
        load_in_4bit = kwargs.pop("load_in_4bit", False)
        quantization_config = kwargs.pop("quantization_config", None)
        subfolder = kwargs.pop("subfolder", "")
        commit_hash = kwargs.pop("_commit_hash", None)
        variant = kwargs.pop("variant", None)
        adapter_kwargs = kwargs.pop("adapter_kwargs", {})
        adapter_name = kwargs.pop("adapter_name", "default")
        generation_config = kwargs.pop("generation_config", None)
        gguf_file = kwargs.pop("gguf_file", None)
        tp_plan = kwargs.pop("tp_plan", None)
        tp_size = kwargs.pop("tp_size", None)
        distributed_config: DistributedConfig = kwargs.pop("distributed_config", None)
        device_mesh = kwargs.pop("device_mesh", None)
        trust_remote_code = kwargs.pop("trust_remote_code", None)
        use_kernels = kwargs.pop("use_kernels", False)

        key_mapping = kwargs.pop("key_mapping", None)
        # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model
        if key_mapping is None and any(
            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS
        ):
            key_mapping = cls._checkpoint_conversion_mapping

        if distributed_config is not None:
            tp_plan = "auto"

        # Not used anymore -- remove them from the kwargs
        _ = kwargs.pop("resume_download", None)
        _ = kwargs.pop("mirror", None)
        _ = kwargs.pop("_fast_init", True)
        _ = kwargs.pop("low_cpu_mem_usage", None)

        # For BC on torch_dtype argument
        if torch_dtype is not None:
            logger.warning_once("`torch_dtype` is deprecated! Use `dtype` instead!")
            # If both kwargs are provided, use `dtype`
            dtype = dtype if dtype is not None else torch_dtype

        if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):
            raise ValueError(
                "`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies."
            )
        if tp_size is not None and tp_plan is None:
            raise ValueError("tp_plan has to be set when tp_size is passed.")
        if tp_plan is not None and tp_plan != "auto":
            # TODO: we can relax this check when we support taking tp_plan from a json file, for example.
            raise ValueError(f"tp_plan supports 'auto' only for now but got {tp_plan}.")
        if tp_plan is not None and device_map is not None:
            raise ValueError(
                "`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization."
            )

        if device_map == "auto" and int(os.environ.get("WORLD_SIZE", "0")):
            logger.info(
                "You've set device_map=`auto` while triggering a distributed run with torchrun. This might lead to unexpected behavior. "
                "If your plan is to load the model on each device, you should set device_map={"
                ": PartialState().process_index} where PartialState comes from accelerate library"
            )

        # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple
        # `device_map` pointing to the correct device
        if tp_plan is not None:
            if device_mesh is None:
                tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(tp_plan, tp_size=tp_size)
            else:
                if device_mesh.ndim > 1:
                    if "tp" not in device_mesh.mesh_dim_names:
                        raise ValueError(
                            "When using `tp_plan` and n-d `device_mesh`, it must contain a 'tp' dimension. "
                            "Please provide a valid `device_mesh`."
                        )
                    device_mesh = device_mesh["tp"]
                tp_size = device_mesh.size()
                device_map = torch.device(f"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}")

            if tp_size is None:
                tp_size = torch.distributed.get_world_size()

        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError(
                    "`token` and `use_auth_token` are both specified. Please set only the argument `token`."
                )
            token = use_auth_token

        if token is not None and adapter_kwargs is not None and "token" not in adapter_kwargs:
            adapter_kwargs["token"] = token

        if gguf_file is not None and not is_accelerate_available():
            raise ValueError("accelerate is required when loading a GGUF file `pip install accelerate`.")

        if commit_hash is None:
            if not isinstance(config, PretrainedConfig):
                # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible
                resolved_config_file = cached_file(
                    pretrained_model_name_or_path,
                    CONFIG_NAME,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    token=token,
                    revision=revision,
                    subfolder=subfolder,
                    _raise_exceptions_for_gated_repo=False,
                    _raise_exceptions_for_missing_entries=False,
                    _raise_exceptions_for_connection_errors=False,
                )
                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)
            else:
                commit_hash = getattr(config, "_commit_hash", None)

        if is_peft_available():
            _adapter_model_path = adapter_kwargs.pop("_adapter_model_path", None)

            if _adapter_model_path is None:
                _adapter_model_path = find_adapter_config_file(
                    pretrained_model_name_or_path,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    _commit_hash=commit_hash,
                    **adapter_kwargs,
                )
            if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):
                with open(_adapter_model_path, "r", encoding="utf-8") as f:
                    _adapter_model_path = pretrained_model_name_or_path
                    pretrained_model_name_or_path = json.load(f)["base_model_name_or_path"]
        else:
            _adapter_model_path = None

        # Potentially detect context manager or global device, and use it (only if no device_map was provided)
        if device_map is None and not is_deepspeed_zero3_enabled():
            device_in_context = get_torch_context_manager_or_global_device()
            if device_in_context == torch.device("meta"):
                raise RuntimeError(
                    "You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\n"
                    "This is an anti-pattern as `from_pretrained` wants to load existing weights.\nIf you want to initialize an "
                    "empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`"
                )
            device_map = device_in_context

        # change device_map into a map if we passed an int, a str or a torch.device
        if isinstance(device_map, torch.device):
            device_map = {"": device_map}
        elif isinstance(device_map, str) and device_map not in ["auto", "balanced", "balanced_low_0", "sequential"]:
            try:
                device_map = {"": torch.device(device_map)}
            except RuntimeError:
                raise ValueError(
                    "When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or "
                    f"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}."
                )
        elif isinstance(device_map, int):
            if device_map < 0:
                raise ValueError(
                    "You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' "
                )
            else:
                device_map = {"": device_map}

        if device_map is not None:
            if is_deepspeed_zero3_enabled():
                raise ValueError("DeepSpeed Zero-3 is not compatible with passing a `device_map`.")
            if not is_accelerate_available():
                raise ValueError(
                    "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` "
                    "requires `accelerate`. You can install it with `pip install accelerate`"
                )

        # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.
        if load_in_4bit or load_in_8bit:
            if quantization_config is not None:
                raise ValueError(
                    "You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing "
                    "`quantization_config` argument at the same time."
                )

            # preparing BitsAndBytesConfig from kwargs
            config_dict = {k: v for k, v in kwargs.items() if k in inspect.signature(BitsAndBytesConfig).parameters}
            config_dict = {**config_dict, "load_in_4bit": load_in_4bit, "load_in_8bit": load_in_8bit}
            quantization_config, kwargs = BitsAndBytesConfig.from_dict(
                config_dict=config_dict, return_unused_kwargs=True, **kwargs
            )
            logger.warning(
                "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. "
                "Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead."
            )

        from_pt = not (from_tf | from_flax)

        user_agent = {"file_type": "model", "framework": "pytorch", "from_auto_class": from_auto_class}
        if from_pipeline is not None:
            user_agent["using_pipeline"] = from_pipeline

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True

        # Load config if we don't provide a configuration
        if not isinstance(config, PretrainedConfig):
            config_path = config if config is not None else pretrained_model_name_or_path
            config, model_kwargs = cls.config_class.from_pretrained(
                config_path,
                cache_dir=cache_dir,
                return_unused_kwargs=True,
                force_download=force_download,
                proxies=proxies,
                local_files_only=local_files_only,
                token=token,
                revision=revision,
                subfolder=subfolder,
                gguf_file=gguf_file,
                _from_auto=from_auto_class,
                _from_pipeline=from_pipeline,
                **kwargs,
            )
            if "gguf_file" in model_kwargs:
                model_kwargs.pop("gguf_file")
        else:
            config = copy.deepcopy(config)
            model_kwargs = kwargs

        # Because some composite configs call super().__init__ before instantiating the sub-configs, we need this call
        # to correctly redispatch recursively if the kwarg is provided
        if "attn_implementation" in kwargs:
            config._attn_implementation = kwargs.pop("attn_implementation")

        transformers_explicit_filename = getattr(config, "transformers_weights", None)

        if transformers_explicit_filename is not None:
            if not transformers_explicit_filename.endswith(
                ".safetensors"
            ) and not transformers_explicit_filename.endswith(".safetensors.index.json"):
                raise ValueError(
                    "The transformers file in the config seems to be incorrect: it is neither a safetensors file "
                    "(*.safetensors) nor a safetensors index file (*.safetensors.index.json): "
                    f"{transformers_explicit_filename}"
                )

        hf_quantizer, config, dtype, device_map = get_hf_quantizer(
            config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent
        )

        if gguf_file is not None and hf_quantizer is not None:
            raise ValueError(
                "You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub."
            )

        if (
            gguf_file
            and device_map is not None
            and ((isinstance(device_map, dict) and "disk" in device_map.values()) or "disk" in device_map)
        ):
            raise RuntimeError(
                "One or more modules is configured to be mapped to disk. Disk offload is not supported for models "
                "loaded from GGUF files."
            )

        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
            pretrained_model_name_or_path=pretrained_model_name_or_path,
            subfolder=subfolder,
            variant=variant,
            gguf_file=gguf_file,
            from_tf=from_tf,
            from_flax=from_flax,
            use_safetensors=use_safetensors,
            cache_dir=cache_dir,
            force_download=force_download,
            proxies=proxies,
            local_files_only=local_files_only,
            token=token,
            user_agent=user_agent,
            revision=revision,
            commit_hash=commit_hash,
            is_remote_code=cls._auto_class is not None,
            transformers_explicit_filename=transformers_explicit_filename,
        )

        is_sharded = sharded_metadata is not None
        is_quantized = hf_quantizer is not None
        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None

        if is_from_file and not is_sharded and checkpoint_files[0].endswith(".safetensors"):
            with safe_open(checkpoint_files[0], framework="pt") as f:
                metadata = f.metadata()

            if metadata is None:
                # Assume it's a pytorch checkpoint (introduced for timm checkpoints)
                pass
            elif metadata.get("format") == "pt":
                pass
            elif metadata.get("format") == "tf":
                from_tf = True
                logger.info("A TensorFlow safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "flax":
                from_flax = True
                logger.info("A Flax safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "mlx":
                # This is a mlx file, we assume weights are compatible with pt
                pass
            else:
                raise ValueError(
                    f"Incompatible safetensors file. File metadata is not ['pt', 'tf', 'flax', 'mlx'] but {metadata.get('format')}"
                )

        from_pt = not (from_tf | from_flax)

        if from_pt:
            if gguf_file:
                from .modeling_gguf_pytorch_utils import load_gguf_checkpoint

                # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was
                # passed directly as a kwarg from now on
                with torch.device("meta"):
                    dummy_model = cls(config)
                state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[
                    "tensors"
                ]

            # Find the correct dtype based on current state
            config, dtype, dtype_orig = _get_dtype(
                cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only
            )

        config.name_or_path = pretrained_model_name_or_path
        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)
        config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.
        with ContextManagers(model_init_context):
            # Let's make sure we don't run the init function of buffer modules
>           model = cls(config, *model_args, **model_kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'

.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: TypeError
----------------------------- Captured stderr call -----------------------------
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]
_______________ TestHotSwapping.test_hotswap_works[True-config0] _______________
[gw2] linux -- Python 3.11.0 /app/.venv/bin/python

self = <tests.test_initialization.TestHotSwapping object at 0x7f062a2fc250>
config = LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.17.2.dev0@UNKNOWN', b...time_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None)
do_compile = True
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/popen-gw2/test_hotswap_works_True_config0')

    @pytest.mark.parametrize(
        "config",
        [
            LoraConfig(init_lora_weights=0, target_modules=["lin0"]),
            LoraConfig(init_lora_weights=0, target_modules=["lin0", "lin1"]),
        ],
    )
    @pytest.mark.parametrize("do_compile", [False, True])
    def test_hotswap_works(self, config, do_compile, tmp_path):
        # Load 2 different adapters and check that we can hotswap between them, with the model optionally being
        # compiled.
        atol, rtol = 1e-4, 1e-4
        inputs = torch.rand(3, 10).to(self.torch_device)

        # create adapter 0
        model = self.get_model()
        torch.manual_seed(0)
        model = get_peft_model(model, config)
        model = self.compile(model, do_compile=do_compile)
        model.eval()
        with torch.inference_mode():
>           output0 = model(inputs)
                      ^^^^^^^^^^^^^

tests/test_initialization.py:3494:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:375: in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:749: in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:923: in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:907: in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1578: in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1456: in codegen_and_compile
    compiled_module = graph.compile_to_module()
                      ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2293: in compile_to_module
    return self._compile_to_module()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2299: in _compile_to_module
    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
                                                             ^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2238: in codegen
    self.scheduler.codegen()
.venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py:4598: in codegen
    else self._codegen(self.nodes)
         ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py:4750: in _codegen
    self.get_backend(device).codegen_node(node)
.venv/lib/python3.11/site-packages/torch/_inductor/codegen/cpp.py:5076: in codegen_node
    cpp_kernel_proxy = self.kernel_proxy_cls(kernel_group)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/codegen/cpp.py:3816: in __init__
    self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:432: in pick_vec_isa
    _valid_vec_isa_list: list[VecISA] = valid_vec_isa_list()
                                        ^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:419: in valid_vec_isa_list
    isa_list.extend(
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:419: in <genexpr>
    isa_list.extend(
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:147: in __bool__
    return self.__bool__impl(config.cpp.vec_isa_ok)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:157: in __bool__impl
    return self.check_build(VecISA._avx_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:102: in check_build
    extra=_get_isa_dry_compile_fingerprint(self._arch_flags),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:28: in _get_isa_dry_compile_fingerprint
    compiler_info = get_compiler_version_info(get_cpp_compiler())
                                              ^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:155: in get_cpp_compiler
    compiler = cpp_compiler_search(search)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

search = (None, 'g++')

    @functools.lru_cache(1)
    def cpp_compiler_search(search: str) -> str:
        from torch._inductor.codecache import get_lock_dir, LOCK_TIMEOUT

        for cxx in search:
            try:
                if cxx is None:
                    # gxx package is only available for Linux
                    # according to https://anaconda.org/conda-forge/gxx/
                    if sys.platform != "linux":
                        continue
                    # Do not install GXX by default
                    if not os.getenv("TORCH_INDUCTOR_INSTALL_GXX"):
                        continue
                    from torch.utils._filelock import FileLock

                    lock_dir = get_lock_dir()
                    lock = FileLock(
                        os.path.join(lock_dir, "g++.lock"), timeout=LOCK_TIMEOUT
                    )
                    with lock:
                        cxx = install_gcc_via_conda()
                subprocess.check_output([cxx, "--version"])
                return cxx
            except (subprocess.SubprocessError, FileNotFoundError, ImportError):
                continue
>       raise exc.InvalidCxxCompiler
E       torch._inductor.exc.InductorError: InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')
E
E       Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

.venv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:101: InductorError
_______________ TestHotSwapping.test_hotswap_works[True-config1] _______________
[gw2] linux -- Python 3.11.0 /app/.venv/bin/python

self = <tests.test_initialization.TestHotSwapping object at 0x7f062a2fc950>
config = LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.17.2.dev0@UNKNOWN', b...time_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None)
do_compile = True
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/popen-gw2/test_hotswap_works_True_config1')

    @pytest.mark.parametrize(
        "config",
        [
            LoraConfig(init_lora_weights=0, target_modules=["lin0"]),
            LoraConfig(init_lora_weights=0, target_modules=["lin0", "lin1"]),
        ],
    )
    @pytest.mark.parametrize("do_compile", [False, True])
    def test_hotswap_works(self, config, do_compile, tmp_path):
        # Load 2 different adapters and check that we can hotswap between them, with the model optionally being
        # compiled.
        atol, rtol = 1e-4, 1e-4
        inputs = torch.rand(3, 10).to(self.torch_device)

        # create adapter 0
        model = self.get_model()
        torch.manual_seed(0)
        model = get_peft_model(model, config)
        model = self.compile(model, do_compile=do_compile)
        model.eval()
        with torch.inference_mode():
>           output0 = model(inputs)
                      ^^^^^^^^^^^^^

tests/test_initialization.py:3494:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:375: in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:749: in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:923: in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:907: in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1578: in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1456: in codegen_and_compile
    compiled_module = graph.compile_to_module()
                      ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2293: in compile_to_module
    return self._compile_to_module()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2299: in _compile_to_module
    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
                                                             ^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2238: in codegen
    self.scheduler.codegen()
.venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py:4598: in codegen
    else self._codegen(self.nodes)
         ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py:4750: in _codegen
    self.get_backend(device).codegen_node(node)
.venv/lib/python3.11/site-packages/torch/_inductor/codegen/cpp.py:5076: in codegen_node
    cpp_kernel_proxy = self.kernel_proxy_cls(kernel_group)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/codegen/cpp.py:3816: in __init__
    self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:432: in pick_vec_isa
    _valid_vec_isa_list: list[VecISA] = valid_vec_isa_list()
                                        ^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:419: in valid_vec_isa_list
    isa_list.extend(
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:419: in <genexpr>
    isa_list.extend(
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:147: in __bool__
    return self.__bool__impl(config.cpp.vec_isa_ok)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:157: in __bool__impl
    return self.check_build(VecISA._avx_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:102: in check_build
    extra=_get_isa_dry_compile_fingerprint(self._arch_flags),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:28: in _get_isa_dry_compile_fingerprint
    compiler_info = get_compiler_version_info(get_cpp_compiler())
                                              ^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:155: in get_cpp_compiler
    compiler = cpp_compiler_search(search)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

search = (None, 'g++')

    @functools.lru_cache(1)
    def cpp_compiler_search(search: str) -> str:
        from torch._inductor.codecache import get_lock_dir, LOCK_TIMEOUT

        for cxx in search:
            try:
                if cxx is None:
                    # gxx package is only available for Linux
                    # according to https://anaconda.org/conda-forge/gxx/
                    if sys.platform != "linux":
                        continue
                    # Do not install GXX by default
                    if not os.getenv("TORCH_INDUCTOR_INSTALL_GXX"):
                        continue
                    from torch.utils._filelock import FileLock

                    lock_dir = get_lock_dir()
                    lock = FileLock(
                        os.path.join(lock_dir, "g++.lock"), timeout=LOCK_TIMEOUT
                    )
                    with lock:
                        cxx = install_gcc_via_conda()
                subprocess.check_output([cxx, "--version"])
                return cxx
            except (subprocess.SubprocessError, FileNotFoundError, ImportError):
                continue
>       raise exc.InvalidCxxCompiler
E       torch._inductor.exc.InductorError: InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')
E
E       Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

.venv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:101: InductorError
_ TestInjectAdapterFromStateDict.test_inject_from_state_dict_stable_diffusion __
[gw2] linux -- Python 3.11.0 /app/.venv/bin/python

self = <tests.test_low_level_api.TestInjectAdapterFromStateDict object at 0x7f062ae0cc50>

    def test_inject_from_state_dict_stable_diffusion(self):
        # same test as above, but with stable diffusion model and only testing LoRA
        model_id = "hf-internal-testing/tiny-sd-pipe"
        config_text_encoder = LoraConfig(target_modules=["k_proj", "q_proj", "v_proj", "out_proj", "fc1", "fc2"])
        config_unet = LoraConfig(
            target_modules=[
                "proj_in",
                "proj_out",
                "to_k",
                "to_q",
                "to_v",
                "to_out.0",
                "ff.net.0.proj",
                "ff.net.2",
            ]
        )
        with hub_online_once(model_id):
>           pipe = StableDiffusionPipeline.from_pretrained(model_id)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_low_level_api.py:286:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'transformers.models.clip.modeling_clip.CLIPTextModel'>
pretrained_model_name_or_path = '/root/.cache/huggingface/hub/models--hf-internal-testing--tiny-sd-pipe/snapshots/39f7e1819d772fbb8dd7f2aa3da8e2fc5a9bd917/text_encoder'
config = CLIPTextConfig {
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dty...en_layers": 5,
  "pad_token_id": 1,
  "projection_dim": 32,
  "transformers_version": "4.57.0",
  "vocab_size": 1000
}

cache_dir = None, ignore_mismatched_sizes = False, force_download = False
local_files_only = False, token = None, revision = 'main'
use_safetensors = None, weights_only = True, model_args = ()
kwargs = {'offload_state_dict': None}, state_dict = None

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        **kwargs,
    ) -> SpecificPreTrainedModelType:
        r"""
        Instantiate a pretrained pytorch model from a pre-trained model configuration.

        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you should first set it back in training mode with `model.train()`.

        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come
        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
        task.

        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those
        weights are discarded.

        Parameters:
            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):
                Can be either:

                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
                    - A path to a *directory* containing model weights saved using
                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In
                      this case, `from_tf` should be set to `True` and a configuration object should be provided as
                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a
                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.
                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,
                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to
                      `True`.
                    - `None` if you are both providing the configuration and state dictionary (resp. with keyword
                      arguments `config` and `state_dict`).
            model_args (sequence of positional arguments, *optional*):
                All remaining positional arguments will be passed to the underlying model's `__init__` method.
            config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):
                Can be either:

                    - an instance of a class derived from [`PretrainedConfig`],
                    - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].

                Configuration for the model to use instead of an automatically loaded configuration. Configuration can
                be automatically loaded when:

                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained
                      model).
                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the
                      save directory.
                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a
                      configuration JSON file named *config.json* is found in the directory.
            state_dict (`dict[str, torch.Tensor]`, *optional*):
                A state dictionary to use instead of a state dictionary loaded from saved weights file.

                This option can be used if you want to create a model from a pretrained configuration but load your own
                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and
                [`~PreTrainedModel.from_pretrained`] is not a simpler option.
            cache_dir (`Union[str, os.PathLike]`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the
                standard cache should not be used.
            from_tf (`bool`, *optional*, defaults to `False`):
                Load the model weights from a TensorFlow checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            from_flax (`bool`, *optional*, defaults to `False`):
                Load the model weights from a Flax checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):
                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size
                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a
                checkpoint with 3 labels).
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
                cached versions if they exist.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
            output_loading_info(`bool`, *optional*, defaults to `False`):
                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.
            local_files_only(`bool`, *optional*, defaults to `False`):
                Whether or not to only look at local files (i.e., do not try to download the model).
            token (`str` or `bool`, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use
                the token generated when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.

                <Tip>

                To test a pull request you made on the Hub, you can pass `revision="refs/pr/<pr_number>"`.

                </Tip>
            attn_implementation (`str`, *optional*):
                The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)), or `"flash_attention_3"` (using [Dao-AILab/flash-attention/hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.

                Accept HF kernel references in the form:
                  <namespace>/<repo_name>[@<revision>][:<kernel_name>]

                - <namespace> and <repo_name> are any non-"/" and non-":" sequences.
                - "@<revision>" is optional (branch, tag, or commit-ish), e.g. "@main", "@v1.2.0", "@abc123".
                - ":<kernel_name>" is optional and selects a function inside the kernel repo.
                - Both options can appear together and in this order only: @revision first, then :kernel_name.
                - We intentionally allow a leading "<wrapper>|" prefix (e.g., "flash|...") because the code
                  strips it before loading; '|' is not excluded in the character classes here.

                Examples that match:
                  "org/model"
                  "org/model@main"
                  "org/model:custom_kernel"
                  "org/model@v1.2.3:custom_kernel"

            > Parameters for big model inference

            dtype (`str` or `torch.dtype`, *optional*):
                Override the default `torch_dtype` and load the model under a specific `dtype`. The different options
                are:

                1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified
                  `dtype`, ignoring the model's `config.dtype` if one exists. If not specified
                  - the model will get loaded in `torch.float` (fp32).

                2. `"auto"` - A `dtype` or `torch_dtype` entry in the `config.json` file of the model will be
                  attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in
                  the checkpoint that's of a floating point type and use that as `dtype`. This will load the model
                  using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how
                  the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.

                3. A string that is a valid `torch.dtype`. E.g. "float32" loads the model in `torch.float32`, "float16" loads in `torch.float16` etc.

                <Tip>

                For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or
                reach out to the authors and ask them to add this information to the model's card and to insert the
                `dtype` or `torch_dtype` entry in `config.json` on the hub.

                </Tip>

            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):
                A map that specifies where each submodule should go. It doesn't need to be refined to each
                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
                same device. If we only pass the device (*e.g.*, `"cpu"`, `"cuda:1"`, `"mps"`, or a GPU ordinal rank
                like `1`) on which the model will be allocated, the device map will map the entire model to this
                device. Passing `device_map = 0` means put the whole model on GPU 0.

                To have Accelerate compute the most optimized `device_map` automatically, set `device_map="auto"`. For
                more information about each option see [designing a device
                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).
            max_memory (`Dict`, *optional*):
                A dictionary device identifier to maximum memory if using `device_map`. Will default to the maximum memory available for each
                GPU and the available CPU RAM if unset.
            tp_plan (`str`, *optional*):
                A torch tensor parallel plan, see [here](https://pytorch.org/tutorials/intermediate/TP_tutorial.html). Currently, it only accepts
                `tp_plan="auto"` to use predefined plan based on the model. Note that if you use it, you should launch your script accordingly with
                `torchrun [args] script.py`. This will be much faster than using a `device_map`, but has limitations.
            tp_size (`str`, *optional*):
                A torch tensor parallel degree. If not provided would default to world size.
            device_mesh (`torch.distributed.DeviceMesh`, *optional*):
                A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.
                If provided, it has to contain dimension named `"tp"` in case it's > 1 dimensional, this dimension will be used for tensor parallelism
            offload_folder (`str` or `os.PathLike`, *optional*):
                If the `device_map` contains any value `"disk"`, the folder where we will offload weights.
            offload_buffers (`bool`, *optional*):
                Whether or not to offload the buffers with the model parameters.
            quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):
                A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g
                bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and
                `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes
                quantizations and not preferred. consider inserting all such arguments into quantization_config
                instead.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            variant (`str`, *optional*):
                If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is
                ignored when using `from_tf` or `from_flax`.
            use_safetensors (`bool`, *optional*, defaults to `None`):
                Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`
                is not installed, it will be set to `False`.
            weights_only (`bool`, *optional*, defaults to `True`):
                Indicates whether unpickler should be restricted to loading only tensors, primitive types,
                dictionaries and any types added via torch.serialization.add_safe_globals().
                When set to False, we can load wrapper tensor subclass weights.
            key_mapping (`dict[str, str], *optional*):
                A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers
                architecture, but was not converted accordingly.
            kwargs (remaining dictionary of keyword arguments, *optional*):
                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or
                automatically loaded:

                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the
                      underlying model's `__init__` method (we assume all relevant updates to the configuration have
                      already been done)
                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class
                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that
                      corresponds to a configuration attribute will be used to override said attribute with the
                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute
                      will be passed to the underlying model's `__init__` function.

        <Tip>

        Activate the special ["offline-mode"](https://huggingface.co/transformers/installation.html#offline-mode) to
        use this method in a firewalled environment.

        </Tip>

        Examples:

        ```python
        >>> from transformers import BertConfig, BertModel

        >>> # Download model and configuration from huggingface.co and cache.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased")
        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
        >>> model = BertModel.from_pretrained("./test/saved_model/")
        >>> # Update configuration during loading.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", output_attentions=True)
        >>> assert model.config.output_attentions == True
        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).
        >>> config = BertConfig.from_json_file("./tf_model/my_tf_model_config.json")
        >>> model = BertModel.from_pretrained("./tf_model/my_tf_checkpoint.ckpt.index", from_tf=True, config=config)
        >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", from_flax=True)
        ```
        """
        state_dict = kwargs.pop("state_dict", None)
        from_tf = kwargs.pop("from_tf", False)
        from_flax = kwargs.pop("from_flax", False)
        proxies = kwargs.pop("proxies", None)
        output_loading_info = kwargs.pop("output_loading_info", False)
        use_auth_token = kwargs.pop("use_auth_token", None)
        from_pipeline = kwargs.pop("_from_pipeline", None)
        from_auto_class = kwargs.pop("_from_auto", False)
        dtype = kwargs.pop("dtype", None)
        torch_dtype = kwargs.pop("torch_dtype", None)  # kept for BC
        device_map = kwargs.pop("device_map", None)
        max_memory = kwargs.pop("max_memory", None)
        offload_folder = kwargs.pop("offload_folder", None)
        offload_buffers = kwargs.pop("offload_buffers", False)
        load_in_8bit = kwargs.pop("load_in_8bit", False)
        load_in_4bit = kwargs.pop("load_in_4bit", False)
        quantization_config = kwargs.pop("quantization_config", None)
        subfolder = kwargs.pop("subfolder", "")
        commit_hash = kwargs.pop("_commit_hash", None)
        variant = kwargs.pop("variant", None)
        adapter_kwargs = kwargs.pop("adapter_kwargs", {})
        adapter_name = kwargs.pop("adapter_name", "default")
        generation_config = kwargs.pop("generation_config", None)
        gguf_file = kwargs.pop("gguf_file", None)
        tp_plan = kwargs.pop("tp_plan", None)
        tp_size = kwargs.pop("tp_size", None)
        distributed_config: DistributedConfig = kwargs.pop("distributed_config", None)
        device_mesh = kwargs.pop("device_mesh", None)
        trust_remote_code = kwargs.pop("trust_remote_code", None)
        use_kernels = kwargs.pop("use_kernels", False)

        key_mapping = kwargs.pop("key_mapping", None)
        # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model
        if key_mapping is None and any(
            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS
        ):
            key_mapping = cls._checkpoint_conversion_mapping

        if distributed_config is not None:
            tp_plan = "auto"

        # Not used anymore -- remove them from the kwargs
        _ = kwargs.pop("resume_download", None)
        _ = kwargs.pop("mirror", None)
        _ = kwargs.pop("_fast_init", True)
        _ = kwargs.pop("low_cpu_mem_usage", None)

        # For BC on torch_dtype argument
        if torch_dtype is not None:
            logger.warning_once("`torch_dtype` is deprecated! Use `dtype` instead!")
            # If both kwargs are provided, use `dtype`
            dtype = dtype if dtype is not None else torch_dtype

        if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):
            raise ValueError(
                "`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies."
            )
        if tp_size is not None and tp_plan is None:
            raise ValueError("tp_plan has to be set when tp_size is passed.")
        if tp_plan is not None and tp_plan != "auto":
            # TODO: we can relax this check when we support taking tp_plan from a json file, for example.
            raise ValueError(f"tp_plan supports 'auto' only for now but got {tp_plan}.")
        if tp_plan is not None and device_map is not None:
            raise ValueError(
                "`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization."
            )

        if device_map == "auto" and int(os.environ.get("WORLD_SIZE", "0")):
            logger.info(
                "You've set device_map=`auto` while triggering a distributed run with torchrun. This might lead to unexpected behavior. "
                "If your plan is to load the model on each device, you should set device_map={"
                ": PartialState().process_index} where PartialState comes from accelerate library"
            )

        # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple
        # `device_map` pointing to the correct device
        if tp_plan is not None:
            if device_mesh is None:
                tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(tp_plan, tp_size=tp_size)
            else:
                if device_mesh.ndim > 1:
                    if "tp" not in device_mesh.mesh_dim_names:
                        raise ValueError(
                            "When using `tp_plan` and n-d `device_mesh`, it must contain a 'tp' dimension. "
                            "Please provide a valid `device_mesh`."
                        )
                    device_mesh = device_mesh["tp"]
                tp_size = device_mesh.size()
                device_map = torch.device(f"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}")

            if tp_size is None:
                tp_size = torch.distributed.get_world_size()

        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError(
                    "`token` and `use_auth_token` are both specified. Please set only the argument `token`."
                )
            token = use_auth_token

        if token is not None and adapter_kwargs is not None and "token" not in adapter_kwargs:
            adapter_kwargs["token"] = token

        if gguf_file is not None and not is_accelerate_available():
            raise ValueError("accelerate is required when loading a GGUF file `pip install accelerate`.")

        if commit_hash is None:
            if not isinstance(config, PretrainedConfig):
                # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible
                resolved_config_file = cached_file(
                    pretrained_model_name_or_path,
                    CONFIG_NAME,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    token=token,
                    revision=revision,
                    subfolder=subfolder,
                    _raise_exceptions_for_gated_repo=False,
                    _raise_exceptions_for_missing_entries=False,
                    _raise_exceptions_for_connection_errors=False,
                )
                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)
            else:
                commit_hash = getattr(config, "_commit_hash", None)

        if is_peft_available():
            _adapter_model_path = adapter_kwargs.pop("_adapter_model_path", None)

            if _adapter_model_path is None:
                _adapter_model_path = find_adapter_config_file(
                    pretrained_model_name_or_path,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    _commit_hash=commit_hash,
                    **adapter_kwargs,
                )
            if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):
                with open(_adapter_model_path, "r", encoding="utf-8") as f:
                    _adapter_model_path = pretrained_model_name_or_path
                    pretrained_model_name_or_path = json.load(f)["base_model_name_or_path"]
        else:
            _adapter_model_path = None

        # Potentially detect context manager or global device, and use it (only if no device_map was provided)
        if device_map is None and not is_deepspeed_zero3_enabled():
            device_in_context = get_torch_context_manager_or_global_device()
            if device_in_context == torch.device("meta"):
                raise RuntimeError(
                    "You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\n"
                    "This is an anti-pattern as `from_pretrained` wants to load existing weights.\nIf you want to initialize an "
                    "empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`"
                )
            device_map = device_in_context

        # change device_map into a map if we passed an int, a str or a torch.device
        if isinstance(device_map, torch.device):
            device_map = {"": device_map}
        elif isinstance(device_map, str) and device_map not in ["auto", "balanced", "balanced_low_0", "sequential"]:
            try:
                device_map = {"": torch.device(device_map)}
            except RuntimeError:
                raise ValueError(
                    "When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or "
                    f"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}."
                )
        elif isinstance(device_map, int):
            if device_map < 0:
                raise ValueError(
                    "You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' "
                )
            else:
                device_map = {"": device_map}

        if device_map is not None:
            if is_deepspeed_zero3_enabled():
                raise ValueError("DeepSpeed Zero-3 is not compatible with passing a `device_map`.")
            if not is_accelerate_available():
                raise ValueError(
                    "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` "
                    "requires `accelerate`. You can install it with `pip install accelerate`"
                )

        # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.
        if load_in_4bit or load_in_8bit:
            if quantization_config is not None:
                raise ValueError(
                    "You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing "
                    "`quantization_config` argument at the same time."
                )

            # preparing BitsAndBytesConfig from kwargs
            config_dict = {k: v for k, v in kwargs.items() if k in inspect.signature(BitsAndBytesConfig).parameters}
            config_dict = {**config_dict, "load_in_4bit": load_in_4bit, "load_in_8bit": load_in_8bit}
            quantization_config, kwargs = BitsAndBytesConfig.from_dict(
                config_dict=config_dict, return_unused_kwargs=True, **kwargs
            )
            logger.warning(
                "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. "
                "Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead."
            )

        from_pt = not (from_tf | from_flax)

        user_agent = {"file_type": "model", "framework": "pytorch", "from_auto_class": from_auto_class}
        if from_pipeline is not None:
            user_agent["using_pipeline"] = from_pipeline

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True

        # Load config if we don't provide a configuration
        if not isinstance(config, PretrainedConfig):
            config_path = config if config is not None else pretrained_model_name_or_path
            config, model_kwargs = cls.config_class.from_pretrained(
                config_path,
                cache_dir=cache_dir,
                return_unused_kwargs=True,
                force_download=force_download,
                proxies=proxies,
                local_files_only=local_files_only,
                token=token,
                revision=revision,
                subfolder=subfolder,
                gguf_file=gguf_file,
                _from_auto=from_auto_class,
                _from_pipeline=from_pipeline,
                **kwargs,
            )
            if "gguf_file" in model_kwargs:
                model_kwargs.pop("gguf_file")
        else:
            config = copy.deepcopy(config)
            model_kwargs = kwargs

        # Because some composite configs call super().__init__ before instantiating the sub-configs, we need this call
        # to correctly redispatch recursively if the kwarg is provided
        if "attn_implementation" in kwargs:
            config._attn_implementation = kwargs.pop("attn_implementation")

        transformers_explicit_filename = getattr(config, "transformers_weights", None)

        if transformers_explicit_filename is not None:
            if not transformers_explicit_filename.endswith(
                ".safetensors"
            ) and not transformers_explicit_filename.endswith(".safetensors.index.json"):
                raise ValueError(
                    "The transformers file in the config seems to be incorrect: it is neither a safetensors file "
                    "(*.safetensors) nor a safetensors index file (*.safetensors.index.json): "
                    f"{transformers_explicit_filename}"
                )

        hf_quantizer, config, dtype, device_map = get_hf_quantizer(
            config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent
        )

        if gguf_file is not None and hf_quantizer is not None:
            raise ValueError(
                "You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub."
            )

        if (
            gguf_file
            and device_map is not None
            and ((isinstance(device_map, dict) and "disk" in device_map.values()) or "disk" in device_map)
        ):
            raise RuntimeError(
                "One or more modules is configured to be mapped to disk. Disk offload is not supported for models "
                "loaded from GGUF files."
            )

        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
            pretrained_model_name_or_path=pretrained_model_name_or_path,
            subfolder=subfolder,
            variant=variant,
            gguf_file=gguf_file,
            from_tf=from_tf,
            from_flax=from_flax,
            use_safetensors=use_safetensors,
            cache_dir=cache_dir,
            force_download=force_download,
            proxies=proxies,
            local_files_only=local_files_only,
            token=token,
            user_agent=user_agent,
            revision=revision,
            commit_hash=commit_hash,
            is_remote_code=cls._auto_class is not None,
            transformers_explicit_filename=transformers_explicit_filename,
        )

        is_sharded = sharded_metadata is not None
        is_quantized = hf_quantizer is not None
        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None

        if is_from_file and not is_sharded and checkpoint_files[0].endswith(".safetensors"):
            with safe_open(checkpoint_files[0], framework="pt") as f:
                metadata = f.metadata()

            if metadata is None:
                # Assume it's a pytorch checkpoint (introduced for timm checkpoints)
                pass
            elif metadata.get("format") == "pt":
                pass
            elif metadata.get("format") == "tf":
                from_tf = True
                logger.info("A TensorFlow safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "flax":
                from_flax = True
                logger.info("A Flax safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "mlx":
                # This is a mlx file, we assume weights are compatible with pt
                pass
            else:
                raise ValueError(
                    f"Incompatible safetensors file. File metadata is not ['pt', 'tf', 'flax', 'mlx'] but {metadata.get('format')}"
                )

        from_pt = not (from_tf | from_flax)

        if from_pt:
            if gguf_file:
                from .modeling_gguf_pytorch_utils import load_gguf_checkpoint

                # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was
                # passed directly as a kwarg from now on
                with torch.device("meta"):
                    dummy_model = cls(config)
                state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[
                    "tensors"
                ]

            # Find the correct dtype based on current state
            config, dtype, dtype_orig = _get_dtype(
                cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only
            )

        config.name_or_path = pretrained_model_name_or_path
        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)
        config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.
        with ContextManagers(model_init_context):
            # Let's make sure we don't run the init function of buffer modules
>           model = cls(config, *model_args, **model_kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'

.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: TypeError
----------------------------- Captured stderr call -----------------------------
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  40%|████▊       | 2/5 [00:00<00:00, 16.63it/s]Loading pipeline components...:  80%|█████████▌  | 4/5 [00:00<00:00, 22.90it/s]
____ PeftCustomKwargsTester.test_maybe_include_all_linear_layers_diffusion _____
[gw2] linux -- Python 3.11.0 /app/.venv/bin/python

self = <tests.test_tuners_utils.PeftCustomKwargsTester testMethod=test_maybe_include_all_linear_layers_diffusion>

    def test_maybe_include_all_linear_layers_diffusion(self):
        model_id = "hf-internal-testing/tiny-sd-pipe"
        with hub_online_once(model_id):
>           model = StableDiffusionPipeline.from_pretrained(model_id)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_tuners_utils.py:347:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'transformers.models.clip.modeling_clip.CLIPTextModel'>
pretrained_model_name_or_path = '/root/.cache/huggingface/hub/models--hf-internal-testing--tiny-sd-pipe/snapshots/39f7e1819d772fbb8dd7f2aa3da8e2fc5a9bd917/text_encoder'
config = CLIPTextConfig {
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dty...en_layers": 5,
  "pad_token_id": 1,
  "projection_dim": 32,
  "transformers_version": "4.57.0",
  "vocab_size": 1000
}

cache_dir = None, ignore_mismatched_sizes = False, force_download = False
local_files_only = False, token = None, revision = 'main'
use_safetensors = None, weights_only = True, model_args = ()
kwargs = {'offload_state_dict': None}, state_dict = None

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        **kwargs,
    ) -> SpecificPreTrainedModelType:
        r"""
        Instantiate a pretrained pytorch model from a pre-trained model configuration.

        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you should first set it back in training mode with `model.train()`.

        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come
        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
        task.

        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those
        weights are discarded.

        Parameters:
            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):
                Can be either:

                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
                    - A path to a *directory* containing model weights saved using
                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In
                      this case, `from_tf` should be set to `True` and a configuration object should be provided as
                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a
                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.
                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,
                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to
                      `True`.
                    - `None` if you are both providing the configuration and state dictionary (resp. with keyword
                      arguments `config` and `state_dict`).
            model_args (sequence of positional arguments, *optional*):
                All remaining positional arguments will be passed to the underlying model's `__init__` method.
            config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):
                Can be either:

                    - an instance of a class derived from [`PretrainedConfig`],
                    - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].

                Configuration for the model to use instead of an automatically loaded configuration. Configuration can
                be automatically loaded when:

                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained
                      model).
                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the
                      save directory.
                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a
                      configuration JSON file named *config.json* is found in the directory.
            state_dict (`dict[str, torch.Tensor]`, *optional*):
                A state dictionary to use instead of a state dictionary loaded from saved weights file.

                This option can be used if you want to create a model from a pretrained configuration but load your own
                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and
                [`~PreTrainedModel.from_pretrained`] is not a simpler option.
            cache_dir (`Union[str, os.PathLike]`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the
                standard cache should not be used.
            from_tf (`bool`, *optional*, defaults to `False`):
                Load the model weights from a TensorFlow checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            from_flax (`bool`, *optional*, defaults to `False`):
                Load the model weights from a Flax checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):
                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size
                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a
                checkpoint with 3 labels).
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
                cached versions if they exist.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
            output_loading_info(`bool`, *optional*, defaults to `False`):
                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.
            local_files_only(`bool`, *optional*, defaults to `False`):
                Whether or not to only look at local files (i.e., do not try to download the model).
            token (`str` or `bool`, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use
                the token generated when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.

                <Tip>

                To test a pull request you made on the Hub, you can pass `revision="refs/pr/<pr_number>"`.

                </Tip>
            attn_implementation (`str`, *optional*):
                The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)), or `"flash_attention_3"` (using [Dao-AILab/flash-attention/hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.

                Accept HF kernel references in the form:
                  <namespace>/<repo_name>[@<revision>][:<kernel_name>]

                - <namespace> and <repo_name> are any non-"/" and non-":" sequences.
                - "@<revision>" is optional (branch, tag, or commit-ish), e.g. "@main", "@v1.2.0", "@abc123".
                - ":<kernel_name>" is optional and selects a function inside the kernel repo.
                - Both options can appear together and in this order only: @revision first, then :kernel_name.
                - We intentionally allow a leading "<wrapper>|" prefix (e.g., "flash|...") because the code
                  strips it before loading; '|' is not excluded in the character classes here.

                Examples that match:
                  "org/model"
                  "org/model@main"
                  "org/model:custom_kernel"
                  "org/model@v1.2.3:custom_kernel"

            > Parameters for big model inference

            dtype (`str` or `torch.dtype`, *optional*):
                Override the default `torch_dtype` and load the model under a specific `dtype`. The different options
                are:

                1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified
                  `dtype`, ignoring the model's `config.dtype` if one exists. If not specified
                  - the model will get loaded in `torch.float` (fp32).

                2. `"auto"` - A `dtype` or `torch_dtype` entry in the `config.json` file of the model will be
                  attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in
                  the checkpoint that's of a floating point type and use that as `dtype`. This will load the model
                  using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how
                  the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.

                3. A string that is a valid `torch.dtype`. E.g. "float32" loads the model in `torch.float32`, "float16" loads in `torch.float16` etc.

                <Tip>

                For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or
                reach out to the authors and ask them to add this information to the model's card and to insert the
                `dtype` or `torch_dtype` entry in `config.json` on the hub.

                </Tip>

            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):
                A map that specifies where each submodule should go. It doesn't need to be refined to each
                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
                same device. If we only pass the device (*e.g.*, `"cpu"`, `"cuda:1"`, `"mps"`, or a GPU ordinal rank
                like `1`) on which the model will be allocated, the device map will map the entire model to this
                device. Passing `device_map = 0` means put the whole model on GPU 0.

                To have Accelerate compute the most optimized `device_map` automatically, set `device_map="auto"`. For
                more information about each option see [designing a device
                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).
            max_memory (`Dict`, *optional*):
                A dictionary device identifier to maximum memory if using `device_map`. Will default to the maximum memory available for each
                GPU and the available CPU RAM if unset.
            tp_plan (`str`, *optional*):
                A torch tensor parallel plan, see [here](https://pytorch.org/tutorials/intermediate/TP_tutorial.html). Currently, it only accepts
                `tp_plan="auto"` to use predefined plan based on the model. Note that if you use it, you should launch your script accordingly with
                `torchrun [args] script.py`. This will be much faster than using a `device_map`, but has limitations.
            tp_size (`str`, *optional*):
                A torch tensor parallel degree. If not provided would default to world size.
            device_mesh (`torch.distributed.DeviceMesh`, *optional*):
                A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.
                If provided, it has to contain dimension named `"tp"` in case it's > 1 dimensional, this dimension will be used for tensor parallelism
            offload_folder (`str` or `os.PathLike`, *optional*):
                If the `device_map` contains any value `"disk"`, the folder where we will offload weights.
            offload_buffers (`bool`, *optional*):
                Whether or not to offload the buffers with the model parameters.
            quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):
                A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g
                bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and
                `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes
                quantizations and not preferred. consider inserting all such arguments into quantization_config
                instead.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            variant (`str`, *optional*):
                If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is
                ignored when using `from_tf` or `from_flax`.
            use_safetensors (`bool`, *optional*, defaults to `None`):
                Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`
                is not installed, it will be set to `False`.
            weights_only (`bool`, *optional*, defaults to `True`):
                Indicates whether unpickler should be restricted to loading only tensors, primitive types,
                dictionaries and any types added via torch.serialization.add_safe_globals().
                When set to False, we can load wrapper tensor subclass weights.
            key_mapping (`dict[str, str], *optional*):
                A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers
                architecture, but was not converted accordingly.
            kwargs (remaining dictionary of keyword arguments, *optional*):
                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or
                automatically loaded:

                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the
                      underlying model's `__init__` method (we assume all relevant updates to the configuration have
                      already been done)
                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class
                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that
                      corresponds to a configuration attribute will be used to override said attribute with the
                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute
                      will be passed to the underlying model's `__init__` function.

        <Tip>

        Activate the special ["offline-mode"](https://huggingface.co/transformers/installation.html#offline-mode) to
        use this method in a firewalled environment.

        </Tip>

        Examples:

        ```python
        >>> from transformers import BertConfig, BertModel

        >>> # Download model and configuration from huggingface.co and cache.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased")
        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
        >>> model = BertModel.from_pretrained("./test/saved_model/")
        >>> # Update configuration during loading.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", output_attentions=True)
        >>> assert model.config.output_attentions == True
        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).
        >>> config = BertConfig.from_json_file("./tf_model/my_tf_model_config.json")
        >>> model = BertModel.from_pretrained("./tf_model/my_tf_checkpoint.ckpt.index", from_tf=True, config=config)
        >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", from_flax=True)
        ```
        """
        state_dict = kwargs.pop("state_dict", None)
        from_tf = kwargs.pop("from_tf", False)
        from_flax = kwargs.pop("from_flax", False)
        proxies = kwargs.pop("proxies", None)
        output_loading_info = kwargs.pop("output_loading_info", False)
        use_auth_token = kwargs.pop("use_auth_token", None)
        from_pipeline = kwargs.pop("_from_pipeline", None)
        from_auto_class = kwargs.pop("_from_auto", False)
        dtype = kwargs.pop("dtype", None)
        torch_dtype = kwargs.pop("torch_dtype", None)  # kept for BC
        device_map = kwargs.pop("device_map", None)
        max_memory = kwargs.pop("max_memory", None)
        offload_folder = kwargs.pop("offload_folder", None)
        offload_buffers = kwargs.pop("offload_buffers", False)
        load_in_8bit = kwargs.pop("load_in_8bit", False)
        load_in_4bit = kwargs.pop("load_in_4bit", False)
        quantization_config = kwargs.pop("quantization_config", None)
        subfolder = kwargs.pop("subfolder", "")
        commit_hash = kwargs.pop("_commit_hash", None)
        variant = kwargs.pop("variant", None)
        adapter_kwargs = kwargs.pop("adapter_kwargs", {})
        adapter_name = kwargs.pop("adapter_name", "default")
        generation_config = kwargs.pop("generation_config", None)
        gguf_file = kwargs.pop("gguf_file", None)
        tp_plan = kwargs.pop("tp_plan", None)
        tp_size = kwargs.pop("tp_size", None)
        distributed_config: DistributedConfig = kwargs.pop("distributed_config", None)
        device_mesh = kwargs.pop("device_mesh", None)
        trust_remote_code = kwargs.pop("trust_remote_code", None)
        use_kernels = kwargs.pop("use_kernels", False)

        key_mapping = kwargs.pop("key_mapping", None)
        # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model
        if key_mapping is None and any(
            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS
        ):
            key_mapping = cls._checkpoint_conversion_mapping

        if distributed_config is not None:
            tp_plan = "auto"

        # Not used anymore -- remove them from the kwargs
        _ = kwargs.pop("resume_download", None)
        _ = kwargs.pop("mirror", None)
        _ = kwargs.pop("_fast_init", True)
        _ = kwargs.pop("low_cpu_mem_usage", None)

        # For BC on torch_dtype argument
        if torch_dtype is not None:
            logger.warning_once("`torch_dtype` is deprecated! Use `dtype` instead!")
            # If both kwargs are provided, use `dtype`
            dtype = dtype if dtype is not None else torch_dtype

        if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):
            raise ValueError(
                "`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies."
            )
        if tp_size is not None and tp_plan is None:
            raise ValueError("tp_plan has to be set when tp_size is passed.")
        if tp_plan is not None and tp_plan != "auto":
            # TODO: we can relax this check when we support taking tp_plan from a json file, for example.
            raise ValueError(f"tp_plan supports 'auto' only for now but got {tp_plan}.")
        if tp_plan is not None and device_map is not None:
            raise ValueError(
                "`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization."
            )

        if device_map == "auto" and int(os.environ.get("WORLD_SIZE", "0")):
            logger.info(
                "You've set device_map=`auto` while triggering a distributed run with torchrun. This might lead to unexpected behavior. "
                "If your plan is to load the model on each device, you should set device_map={"
                ": PartialState().process_index} where PartialState comes from accelerate library"
            )

        # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple
        # `device_map` pointing to the correct device
        if tp_plan is not None:
            if device_mesh is None:
                tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(tp_plan, tp_size=tp_size)
            else:
                if device_mesh.ndim > 1:
                    if "tp" not in device_mesh.mesh_dim_names:
                        raise ValueError(
                            "When using `tp_plan` and n-d `device_mesh`, it must contain a 'tp' dimension. "
                            "Please provide a valid `device_mesh`."
                        )
                    device_mesh = device_mesh["tp"]
                tp_size = device_mesh.size()
                device_map = torch.device(f"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}")

            if tp_size is None:
                tp_size = torch.distributed.get_world_size()

        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError(
                    "`token` and `use_auth_token` are both specified. Please set only the argument `token`."
                )
            token = use_auth_token

        if token is not None and adapter_kwargs is not None and "token" not in adapter_kwargs:
            adapter_kwargs["token"] = token

        if gguf_file is not None and not is_accelerate_available():
            raise ValueError("accelerate is required when loading a GGUF file `pip install accelerate`.")

        if commit_hash is None:
            if not isinstance(config, PretrainedConfig):
                # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible
                resolved_config_file = cached_file(
                    pretrained_model_name_or_path,
                    CONFIG_NAME,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    token=token,
                    revision=revision,
                    subfolder=subfolder,
                    _raise_exceptions_for_gated_repo=False,
                    _raise_exceptions_for_missing_entries=False,
                    _raise_exceptions_for_connection_errors=False,
                )
                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)
            else:
                commit_hash = getattr(config, "_commit_hash", None)

        if is_peft_available():
            _adapter_model_path = adapter_kwargs.pop("_adapter_model_path", None)

            if _adapter_model_path is None:
                _adapter_model_path = find_adapter_config_file(
                    pretrained_model_name_or_path,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    _commit_hash=commit_hash,
                    **adapter_kwargs,
                )
            if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):
                with open(_adapter_model_path, "r", encoding="utf-8") as f:
                    _adapter_model_path = pretrained_model_name_or_path
                    pretrained_model_name_or_path = json.load(f)["base_model_name_or_path"]
        else:
            _adapter_model_path = None

        # Potentially detect context manager or global device, and use it (only if no device_map was provided)
        if device_map is None and not is_deepspeed_zero3_enabled():
            device_in_context = get_torch_context_manager_or_global_device()
            if device_in_context == torch.device("meta"):
                raise RuntimeError(
                    "You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\n"
                    "This is an anti-pattern as `from_pretrained` wants to load existing weights.\nIf you want to initialize an "
                    "empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`"
                )
            device_map = device_in_context

        # change device_map into a map if we passed an int, a str or a torch.device
        if isinstance(device_map, torch.device):
            device_map = {"": device_map}
        elif isinstance(device_map, str) and device_map not in ["auto", "balanced", "balanced_low_0", "sequential"]:
            try:
                device_map = {"": torch.device(device_map)}
            except RuntimeError:
                raise ValueError(
                    "When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or "
                    f"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}."
                )
        elif isinstance(device_map, int):
            if device_map < 0:
                raise ValueError(
                    "You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' "
                )
            else:
                device_map = {"": device_map}

        if device_map is not None:
            if is_deepspeed_zero3_enabled():
                raise ValueError("DeepSpeed Zero-3 is not compatible with passing a `device_map`.")
            if not is_accelerate_available():
                raise ValueError(
                    "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` "
                    "requires `accelerate`. You can install it with `pip install accelerate`"
                )

        # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.
        if load_in_4bit or load_in_8bit:
            if quantization_config is not None:
                raise ValueError(
                    "You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing "
                    "`quantization_config` argument at the same time."
                )

            # preparing BitsAndBytesConfig from kwargs
            config_dict = {k: v for k, v in kwargs.items() if k in inspect.signature(BitsAndBytesConfig).parameters}
            config_dict = {**config_dict, "load_in_4bit": load_in_4bit, "load_in_8bit": load_in_8bit}
            quantization_config, kwargs = BitsAndBytesConfig.from_dict(
                config_dict=config_dict, return_unused_kwargs=True, **kwargs
            )
            logger.warning(
                "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. "
                "Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead."
            )

        from_pt = not (from_tf | from_flax)

        user_agent = {"file_type": "model", "framework": "pytorch", "from_auto_class": from_auto_class}
        if from_pipeline is not None:
            user_agent["using_pipeline"] = from_pipeline

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True

        # Load config if we don't provide a configuration
        if not isinstance(config, PretrainedConfig):
            config_path = config if config is not None else pretrained_model_name_or_path
            config, model_kwargs = cls.config_class.from_pretrained(
                config_path,
                cache_dir=cache_dir,
                return_unused_kwargs=True,
                force_download=force_download,
                proxies=proxies,
                local_files_only=local_files_only,
                token=token,
                revision=revision,
                subfolder=subfolder,
                gguf_file=gguf_file,
                _from_auto=from_auto_class,
                _from_pipeline=from_pipeline,
                **kwargs,
            )
            if "gguf_file" in model_kwargs:
                model_kwargs.pop("gguf_file")
        else:
            config = copy.deepcopy(config)
            model_kwargs = kwargs

        # Because some composite configs call super().__init__ before instantiating the sub-configs, we need this call
        # to correctly redispatch recursively if the kwarg is provided
        if "attn_implementation" in kwargs:
            config._attn_implementation = kwargs.pop("attn_implementation")

        transformers_explicit_filename = getattr(config, "transformers_weights", None)

        if transformers_explicit_filename is not None:
            if not transformers_explicit_filename.endswith(
                ".safetensors"
            ) and not transformers_explicit_filename.endswith(".safetensors.index.json"):
                raise ValueError(
                    "The transformers file in the config seems to be incorrect: it is neither a safetensors file "
                    "(*.safetensors) nor a safetensors index file (*.safetensors.index.json): "
                    f"{transformers_explicit_filename}"
                )

        hf_quantizer, config, dtype, device_map = get_hf_quantizer(
            config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent
        )

        if gguf_file is not None and hf_quantizer is not None:
            raise ValueError(
                "You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub."
            )

        if (
            gguf_file
            and device_map is not None
            and ((isinstance(device_map, dict) and "disk" in device_map.values()) or "disk" in device_map)
        ):
            raise RuntimeError(
                "One or more modules is configured to be mapped to disk. Disk offload is not supported for models "
                "loaded from GGUF files."
            )

        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
            pretrained_model_name_or_path=pretrained_model_name_or_path,
            subfolder=subfolder,
            variant=variant,
            gguf_file=gguf_file,
            from_tf=from_tf,
            from_flax=from_flax,
            use_safetensors=use_safetensors,
            cache_dir=cache_dir,
            force_download=force_download,
            proxies=proxies,
            local_files_only=local_files_only,
            token=token,
            user_agent=user_agent,
            revision=revision,
            commit_hash=commit_hash,
            is_remote_code=cls._auto_class is not None,
            transformers_explicit_filename=transformers_explicit_filename,
        )

        is_sharded = sharded_metadata is not None
        is_quantized = hf_quantizer is not None
        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None

        if is_from_file and not is_sharded and checkpoint_files[0].endswith(".safetensors"):
            with safe_open(checkpoint_files[0], framework="pt") as f:
                metadata = f.metadata()

            if metadata is None:
                # Assume it's a pytorch checkpoint (introduced for timm checkpoints)
                pass
            elif metadata.get("format") == "pt":
                pass
            elif metadata.get("format") == "tf":
                from_tf = True
                logger.info("A TensorFlow safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "flax":
                from_flax = True
                logger.info("A Flax safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "mlx":
                # This is a mlx file, we assume weights are compatible with pt
                pass
            else:
                raise ValueError(
                    f"Incompatible safetensors file. File metadata is not ['pt', 'tf', 'flax', 'mlx'] but {metadata.get('format')}"
                )

        from_pt = not (from_tf | from_flax)

        if from_pt:
            if gguf_file:
                from .modeling_gguf_pytorch_utils import load_gguf_checkpoint

                # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was
                # passed directly as a kwarg from now on
                with torch.device("meta"):
                    dummy_model = cls(config)
                state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[
                    "tensors"
                ]

            # Find the correct dtype based on current state
            config, dtype, dtype_orig = _get_dtype(
                cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only
            )

        config.name_or_path = pretrained_model_name_or_path
        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)
        config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.
        with ContextManagers(model_init_context):
            # Let's make sure we don't run the init function of buffer modules
>           model = cls(config, *model_args, **model_kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'

.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: TypeError
----------------------------- Captured stderr call -----------------------------
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  40%|████▊       | 2/5 [00:00<00:00, 19.92it/s]Loading pipeline components...:  80%|█████████▌  | 4/5 [00:00<00:00, 26.33it/s]
=============================== warnings summary ===============================
src/peft/tuners/bone/config.py:126: 3 warnings
tests/test_custom_models.py: 241 warnings
tests/test_decoder_models.py: 166 warnings
tests/test_config.py: 22 warnings
tests/test_encoder_decoder_models.py: 38 warnings
tests/test_feature_extraction_models.py: 54 warnings
tests/test_seq_classifier.py: 27 warnings
  /app/src/peft/tuners/bone/config.py:126: UserWarning: Bone will be removed in v0.19.0 of PEFT, use `MissConfig` instead. If you already have a Bone checkpoint, you can use `/scripts/convert-bone-to-miss.py` to convert it into
    warnings.warn(

tests/test_custom_models.py: 207 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv1d' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 129 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MLP_LayerNorm' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 79 warnings
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'torch.nn.modules.normalization.LayerNorm'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_custom_models.py: 116 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv1dBigger' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 58 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2d1x1' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 3632 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MLP' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 811 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2d' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_save_pretrained[Vanilla MLP 1 BOFT-MLP-BOFTConfig-config_kwargs137]
tests/test_custom_models.py::TestPeftCustomModel::test_safe_merge[Vanilla MLP 1 BOFT-MLP-BOFTConfig-config_kwargs137]
tests/test_boft.py::TestBoft::test_boft_state_dict
  /app/.venv/lib/python3.11/site-packages/pkg_resources/_vendor/pyparsing.py:87: DeprecationWarning: module 'sre_constants' is deprecated
    import sre_constants

tests/test_custom_models.py::TestPeftCustomModel::test_save_pretrained[Vanilla MLP 1 BOFT-MLP-BOFTConfig-config_kwargs137]
tests/test_custom_models.py::TestPeftCustomModel::test_safe_merge[Vanilla MLP 1 BOFT-MLP-BOFTConfig-config_kwargs137]
tests/test_boft.py::TestBoft::test_boft_state_dict
  /app/src/peft/tuners/boft/layer.py:95: UserWarning: Failed to load the CUDA extension: Ninja is required to load C++ extensions (pip install ninja to get it), check if ninja is available.
    warnings.warn(f"Failed to load the CUDA extension: {e}, check if ninja is available.")

tests/test_custom_models.py: 353 warnings
tests/test_decoder_models.py: 160 warnings
tests/test_boft.py: 2 warnings
tests/test_encoder_decoder_models.py: 36 warnings
tests/test_feature_extraction_models.py: 46 warnings
tests/test_seq_classifier.py: 27 warnings
  /app/src/peft/tuners/boft/layer.py:96: UserWarning: Setting boft_n_butterfly_factor to 1 to speed up the finetuning process.
    warnings.warn("Setting boft_n_butterfly_factor to 1 to speed up the finetuning process.")

tests/test_custom_models.py: 351 warnings
tests/test_decoder_models.py: 160 warnings
tests/test_boft.py: 2 warnings
tests/test_encoder_decoder_models.py: 36 warnings
tests/test_feature_extraction_models.py: 46 warnings
tests/test_seq_classifier.py: 27 warnings
  /app/src/peft/tuners/boft/layer.py:95: UserWarning: Failed to load the CUDA extension: /root/.cache/torch_extensions/py311_cu128/fbd_cuda/fbd_cuda.so: cannot open shared object file: No such file or directory, check if ninja is available.
    warnings.warn(f"Failed to load the CUDA extension: {e}, check if ninja is available.")

tests/test_custom_models.py: 56 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MLP2' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 56 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2d2' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 385 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'EmbConv1D' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 27 warnings
tests/test_decoder_models.py: 17 warnings
  /app/src/peft/tuners/vera/model.py:275: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py: 30 warnings
tests/test_decoder_models.py: 19 warnings
  /app/src/peft/tuners/vblora/model.py:141: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py: 160 warnings
tests/test_decoder_models.py: 100 warnings
tests/test_helpers.py: 2 warnings
tests/test_initialization.py: 4 warnings
tests/test_tuners_utils.py: 2 warnings
  /app/src/peft/tuners/lora/layer.py:2264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py: 52 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2dGroups' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 104 warnings
tests/test_initialization.py: 6 warnings
  /app/src/peft/tuners/lora/layer.py:1138: UserWarning: LoRA adapter added to ConvNd layer with groups > 1. Merging is not supported.
    warnings.warn("LoRA adapter added to ConvNd layer with groups > 1. Merging is not supported.")

tests/test_custom_models.py: 52 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2dGroups2' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 300 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv3d' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 63 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MHA' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 26 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MlpUsingParameters' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 26 warnings
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'tests.test_custom_models._LinearUsingParameter'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_custom_models.py: 87 warnings
tests/test_decoder_models.py: 20 warnings
  /app/src/peft/tuners/ia3/model.py:130: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py: 29 warnings
  /app/src/peft/tuners/ia3/model.py:122: UserWarning: fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. Setting fan_in_fan_out to False.
    warnings.warn(

tests/test_custom_models.py: 14 warnings
  /app/src/peft/tuners/tuners_utils.py:1683: UserWarning: All adapters are already merged, nothing to do.
    warnings.warn("All adapters are already merged, nothing to do.")

tests/test_custom_models.py: 10 warnings
tests/test_tuners_utils.py: 2 warnings
  /app/src/peft/tuners/lora/layer.py:728: UserWarning: Already unmerged. Nothing to do.
    warnings.warn("Already unmerged. Nothing to do.")

tests/test_custom_models.py: 110 warnings
tests/test_decoder_models.py: 7 warnings
tests/test_feature_extraction_models.py: 4 warnings
tests/test_encoder_decoder_models.py: 2 warnings
  /app/src/peft/tuners/ia3/layer.py:142: UserWarning: Unmerge result can be inaccurate for (IA)^3.
    warnings.warn("Unmerge result can be inaccurate for (IA)^3.")

tests/test_custom_models.py: 60 warnings
  /app/src/peft/tuners/ia3/layer.py:268: UserWarning: Unmerge result can be inaccurate for (IA)^3.
    warnings.warn("Unmerge result can be inaccurate for (IA)^3.")

tests/test_custom_models.py: 160 warnings
tests/test_decoder_models.py: 112 warnings
tests/test_encoder_decoder_models.py: 22 warnings
tests/test_feature_extraction_models.py: 44 warnings
  /app/src/peft/tuners/tuners_utils.py:1350: UserWarning: Adapter delete_me was active which is now deleted. Setting active adapter to default.
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter[MHA 1 LoRA-MHA-LoraConfig-config_kwargs33]
tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter[MHA 2 LoRA-MHA-LoraConfig-config_kwargs34]
tests/test_custom_models.py::TestPeftCustomModel::test_delete_inactive_adapter[MHA 1 LoRA-MHA-LoraConfig-config_kwargs33]
tests/test_custom_models.py::TestPeftCustomModel::test_delete_inactive_adapter[MHA 2 LoRA-MHA-LoraConfig-config_kwargs34]
  /app/src/peft/tuners/lora/layer.py:1679: UserWarning: Already unmerged. Nothing to do.
    warnings.warn("Already unmerged. Nothing to do.")

tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_with_multiple_adapters_works
tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_modules_to_save
tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_trainable_token_indices
tests/test_mixed.py::TestMixedAdapterTypes::test_delete_adapter
  /app/src/peft/tuners/tuners_utils.py:1350: UserWarning: Adapter adapter0 was active which is now deleted. Setting active adapter to adapter1.
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_modules_to_save
tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_trainable_token_indices
  /app/src/peft/tuners/tuners_utils.py:1350: UserWarning: Adapter adapter1 was active which is now deleted. Setting active adapter to adapter2.
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_trainable_token_indices
  /app/src/peft/tuners/tuners_utils.py:1350: UserWarning: Adapter adapter0 was active which is now deleted. Setting active adapter to adapter2.
    warnings.warn(

tests/test_custom_models.py: 1 warning
tests/test_mapping.py: 2 warnings
tests/test_tuners_utils.py: 9 warnings
  /app/src/peft/tuners/tuners_utils.py:279: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_load_resized_embedding_ignore_mismatched_sizes
  /app/src/peft/utils/save_and_load.py:587: UserWarning: Some weights of PeftModel were not initialized from the model checkpoint and are being ignored because you passed `ignore_mismatched_sizes=True`: - base_model.model.emb.lora_embedding_A.default: found shape torch.Size([8, 100]) in the checkpoint and torch.Size([8, 105]) in the model instantiated.
    warnings.warn(msg)

tests/test_custom_models.py::TestPeftCustomModel::test_load_resized_embedding_ignore_mismatched_sizes
  /app/src/peft/peft_model.py:584: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.emb.lora_embedding_A.default'].
    warnings.warn(warn_message)

tests/test_custom_models.py::TestDynamicDispatch::test_custom_lora_layer_used
tests/test_custom_models.py::TestDynamicDispatch::test_training_works
tests/test_custom_models.py::TestDynamicDispatch::test_saving_and_loading
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'tests.test_custom_models.TestDynamicDispatch.custom_module_cls.<locals>.MyModule'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_decoder_models.py: 14 warnings
  /app/src/peft/tuners/adalora/model.py:211: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_decoder_models.py: 57 warnings
tests/test_encoder_decoder_models.py: 17 warnings
tests/test_seq_classifier.py: 9 warnings
  /app/src/peft/tuners/oft/layer.py:446: UserWarning: Invalid `oft_block_size` (32)! Adjusted `oft_block_size` to (16).
    warnings.warn(

tests/test_decoder_models.py: 19 warnings
  /app/src/peft/tuners/oft/layer.py:446: UserWarning: Invalid `oft_block_size` (32)! Adjusted `oft_block_size` to (8).
    warnings.warn(

tests/test_decoder_models.py: 18 warnings
  /app/src/peft/tuners/fourierft/model.py:116: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_decoder_models.py: 16 warnings
  /app/src/peft/tuners/waveft/model.py:169: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_decoder_models.py: 141 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-OPTForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 93 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-GPT2LMHeadModel - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 141 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-BloomForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 141 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-gpt_neo - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 141 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-GPTJForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 141 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-GPTBigCodeForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 141 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in trl-internal-testing/tiny-random-LlamaForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 141 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in peft-internal-testing/tiny-dummy-qwen2 - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 141 warnings
tests/test_hub_features.py: 7 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-Gemma3ForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_config.py::TestPeftConfig::test_from_peft_type
  /app/src/peft/tuners/xlora/config.py:83: UserWarning: No value was provided for `hidden_size`. This will be set to 4096 by default, please ensure that this is correct.
    warnings.warn(

tests/test_config.py::TestPeftConfig::test_from_peft_type
  /app/src/peft/tuners/xlora/config.py:88: UserWarning: No value was provided for for `adapters`. This will be set to empty, please ensure that this is correct.
    warnings.warn(

tests/test_config.py::TestPeftConfig::test_from_peft_type
tests/test_cpt.py::test_model_initialization_text
tests/test_cpt.py::test_model_initialization_random
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-hf-internal-testing/tiny-random-gpt2]
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-hf-internal-testing/tiny-random-OPTForCausalLM]
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-hf-internal-testing/tiny-random-MistralForCausalLM]
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-peft-internal-testing/tiny-dummy-qwen2]
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-trl-internal-testing/tiny-random-LlamaForCausalLM]
  /app/src/peft/tuners/cpt/config.py:85: FutureWarning: CPTConfig only supports task_type = CAUSAL_LM, setting it automatically. This will raise an error starting from PEFT v0.18.0.
    warnings.warn(

tests/test_config.py: 24 warnings
  /app/src/peft/config.py:280: UserWarning: The configuration file contains a `runtime_config` key. This is ignored. Runtime configurations are only valid at runtime.
    warnings.warn(

tests/test_cpt.py: 2 warnings
tests/test_decoder_models.py: 16 warnings
  /app/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
    warnings.warn(warn_msg)

tests/test_decoder_models.py::TestDecoderModels::test_prompt_tuning_text_prepare_for_training[PromptTuningConfig-config_kwargs15-trl-internal-testing/tiny-random-LlamaForCausalLM]
tests/test_trainable_tokens.py::TestTrainableTokens::test_stand_alone_usage
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute

tests/test_decoder_models.py::TestDecoderModels::test_prompt_tuning_text_prepare_for_training[PromptTuningConfig-config_kwargs15-trl-internal-testing/tiny-random-LlamaForCausalLM]
tests/test_trainable_tokens.py::TestTrainableTokens::test_stand_alone_usage
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute

tests/test_decoder_models.py: 104 warnings
tests/test_multitask_prompt_tuning.py: 7 warnings
  /app/src/peft/peft_model.py:2101: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.
    warnings.warn("Position ids are not supported for parameter efficient tuning. Ignoring position ids.")

tests/test_decoder_models.py: 9 warnings
tests/test_encoder_decoder_models.py: 2 warnings
  /app/src/peft/tuners/adalora/config.py:96: UserWarning: Note that `r` is not used in AdaLora and will be ignored.If you intended to set the initial rank, use `init_r` instead.
    warnings.warn(

tests/test_decoder_models.py: 18 warnings
  /app/src/peft/tuners/lora/variants.py:714: UserWarning: Cannot calculate aLoRA offsets when only inputs_embeds are provided. Disabling aLoRA for this forward pass.
    warnings.warn(

tests/test_decoder_models.py::TestDecoderModels::test_lora_layer_replication
  /app/tests/test_decoder_models.py:608: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
    layers[0].mlp.up_proj.base_layer.weight.data.storage().data_ptr()

tests/test_encoder_decoder_models.py: 119 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in ybelkada/tiny-random-T5ForConditionalGeneration-calibrated - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_encoder_decoder_models.py: 118 warnings
tests/test_hub_features.py: 2 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-BartForConditionalGeneration - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_feature_extraction_models.py: 80 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-RobertaModel - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_feature_extraction_models.py: 79 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-DebertaModel - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_feature_extraction_models.py: 79 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-DebertaV2Model - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_feature_extraction_models.py: 81 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-BertModel - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_initialization.py::TestVeraInitialization::test_vera_mixing_save_projection_raises
tests/test_vera.py::TestVera::test_multiple_adapters_save_projection_false_contains_no_vera_A_vera_B
tests/test_vera.py::TestVera::test_multiple_adapters_save_load_save_projection_false
  /app/src/peft/tuners/vera/config.py:158: UserWarning: Specified to not save vera_A and vera_B within the state dictionary, instead they will be restored using the PRNG key store in `config.projection_prng_key`. Consider setting `config.save_projection` to `True` to guarantee restoring the checkpoint correctly on all system configurations.
    warnings.warn(

tests/test_initialization.py::TestLoraInitialization::test_lora_pissa_conversion_same_output_after_loading
tests/test_initialization.py::TestLoraInitialization::test_lora_pissa_conversion_same_output_after_loading_with_rank_pattern
tests/test_initialization.py::TestLoraInitialization::test_lora_pissa_conversion_same_output_after_loading_with_alpha_pattern
tests/test_initialization.py::TestLoraInitialization::test_lora_pissa_conversion_same_output_after_loading_with_rslora
  /app/src/peft/peft_model.py:1254: UserWarning: PiSSA changes the base weights of the model and should thus not be used with other adapters. Consider converting the PiSSA adapter into a normal LoRA adapter: https://github.com/huggingface/peft/tree/main/examples/pissa_finetuning#convert-pissa-to-lora
    warnings.warn(msg)

tests/test_initialization.py::TestLoraInitialization::test_pissa_rank_pattern_and_rslora_raises
tests/test_initialization.py::TestLoraInitialization::test_pissa_alpha_pattern_and_rslora_raises
tests/test_initialization.py::TestLoraInitialization::test_olora_rank_pattern_and_rslora_raises
tests/test_initialization.py::TestLoraInitialization::test_olora_alpha_pattern_and_rslora_raises
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_rank_pattern_and_rslora_raises[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_rank_pattern_and_rslora_raises[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_alpha_pattern_and_rslora_raises[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_alpha_pattern_and_rslora_raises[kpm]
  /app/src/peft/tuners/lora/config.py:762: UserWarning: Using Rank-Stabilized LoRA with rank_pattern/alpha_pattern and post-training conversion of modified base weights PiSSA/CorDA/OLoRA means that you won't be able to pass `path_initial_model_for_weight_conversion` to `save_pretrained` to restore the initial values of the base weights; if you intend to do this, please ensure not to use rslora or rank_pattern/alpha_pattern.
    warnings.warn(msg)

tests/test_initialization.py::TestLoraInitialization::test_olora_conversion_same_output_after_loading
tests/test_initialization.py::TestLoraInitialization::test_olora_conversion_same_output_after_loading_with_rank_pattern
tests/test_initialization.py::TestLoraInitialization::test_olora_conversion_same_output_after_loading_with_alpha_pattern
tests/test_initialization.py::TestLoraInitialization::test_olora_conversion_same_output_after_loading_with_rslora
  /app/src/peft/peft_model.py:1268: UserWarning: OLoRA changes the base weights of the model and should thus not be used with other adapters. Consider converting the OLoRA adapter into a normal LoRA adapter: https://github.com/huggingface/peft/tree/main/examples/olora_finetuning#olora-and-lora
    warnings.warn(msg)

tests/test_initialization.py::TestLoraInitialization::test_lora_with_bias_incompatible_arguments[extra_kwargs1]
  /app/src/peft/tuners/lora/config.py:718: UserWarning: `init_lora_weights` is 'eva' but `eva_config` is not specified. Using default EVA config.
    warnings.warn("`init_lora_weights` is 'eva' but `eva_config` is not specified. Using default EVA config.")

tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rank_pattern[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rank_pattern[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_alpha_pattern[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_alpha_pattern[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rslora[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rslora[kpm]
  /app/src/peft/tuners/lora/config.py:729: UserWarning: `corda_config` specified but will be ignored when `init_lora_weights` is not 'corda'.
    warnings.warn("`corda_config` specified but will be ignored when `init_lora_weights` is not 'corda'.")

tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rank_pattern[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rank_pattern[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_alpha_pattern[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_alpha_pattern[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rslora[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rslora[kpm]
  /app/src/peft/peft_model.py:1261: UserWarning: CorDA changes the base weights of the model and should thus not be used with other adapters. Consider converting the CorDA adapter into a normal LoRA adapter: https://github.com/huggingface/peft/tree/main/examples/corda_finetuning#convert-corda-to-lora
    warnings.warn(msg)

tests/test_other.py::TestTargetingAuxiliaryTrainingWrapper::test_targeting_trainable_tokens_raises
tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/tuners/tuners_utils.py:899: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
    warnings.warn(

tests/test_initialization.py::TestEvaInitialization::test_eva_state_dict_adjust_scaling_factors[eva_config0]
tests/test_initialization.py::TestEvaInitialization::test_load_eva_state_dict[True]
tests/test_initialization.py::TestEvaInitialization::test_load_eva_state_dict[False]
tests/test_initialization.py::TestEvaInitialization::test_missing_eva_inits
tests/test_initialization.py::TestEvaInitialization::test_load_eva_model
  /app/src/peft/mapping_func.py:96: UserWarning: lora with eva initialization used with low_cpu_mem_usage=False. Setting low_cpu_mem_usage=True can improve the maximum batch size possible for eva initialization.
    warnings.warn(

tests/test_seq_classifier.py: 95 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-BertForSequenceClassification - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_seq_classifier.py: 95 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-RobertaForSequenceClassification - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_seq_classifier.py: 95 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in trl-internal-testing/tiny-LlamaForSequenceClassification-3.2 - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_randlora.py::TestRandLora::test_multiple_adapters_save_load_save_projection_false
tests/test_randlora.py::TestRandLora::test_multiple_adapters_save_projection_false_contains_no_randlora_A_randlora_B
  /app/src/peft/tuners/randlora/config.py:195: UserWarning: Specified to not save basis_A and basis_B within the state dictionary, instead they will be restored using the PRNG key store in `config.projection_prng_key`. Consider setting `config.save_projection` to `True` to guarantee restoring the checkpoint correctly on all system configurations.
    warnings.warn(

tests/test_initialization.py::TestHotSwapping::test_prepare_model_for_compiled_hotswap_lora_bias
  /app/src/peft/tuners/lora/layer.py:170: PeftWarning: `lora_bias=True` was passed but the targeted layer of type Linear has no bias. This means that merging LoRA weights won't be possible.
    warnings.warn(

tests/test_lora_variants.py::TestLoraVariants::test_variant_is_applied_to_layers[alora-LoraConfig-config_kwargs1]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets[input_ids0-alora_invocation_tokens0-expected_offsets0]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets[input_ids1-alora_invocation_tokens1-expected_offsets1]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets[input_ids2-alora_invocation_tokens2-expected_offsets2]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets_with_adapter_names[input_ids0-alora_invocations0-expected_offsets0]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets_with_adapter_names[input_ids1-alora_invocations1-expected_offsets1]
tests/test_lora_variants.py::TestActivatedLora::test_alora_activation_matches_base_until_invocation
tests/test_lora_variants.py::TestActivatedLora::test_input_embeds_warning
tests/test_lora_variants.py::TestActivatedLora::test_num_beams_error
  /app/src/peft/tuners/lora/config.py:741: UserWarning: aLoRA is currently only supported for CAUSAL_LM task.
    warnings.warn("aLoRA is currently only supported for CAUSAL_LM task.")

tests/test_shira.py::TestShira::test_mlp_single_adapter_shapes
  /app/.venv/lib/python3.11/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_shira.py::TestShira::test_mlp_single_adapter_shapes returned <class 'peft.peft_model.PeftModel'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_shira.py::TestShira::test_multiple_adapters_save_load
  /app/.venv/lib/python3.11/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_shira.py::TestShira::test_multiple_adapters_save_load returned <class 'peft.peft_model.PeftModel'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_shira.py::TestShira::test_save_load_custom_mask_function
  /app/src/peft/tuners/shira/config.py:126: UserWarning: Argument self.mask_type='custom' is not recognized, please supply your own masking function by calling `config.mask_fn = my_mask_fn`.
    warnings.warn(

tests/test_shira.py::TestShira::test_save_load_custom_mask_function
  /app/.venv/lib/python3.11/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_shira.py::TestShira::test_save_load_custom_mask_function returned <class 'peft.peft_model.PeftModel'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_shira.py::TestShira::test_save_load_default_random_mask_with_seed_function
  /app/.venv/lib/python3.11/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_shira.py::TestShira::test_save_load_default_random_mask_with_seed_function returned <class 'peft.peft_model.PeftModel'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_target_parameters.py: 81 warnings
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'transformers.models.llama4.modeling_llama4.Llama4TextExperts'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_target_parameters.py: 80 warnings
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssExperts'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_target_parameters.py: 20 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in trl-internal-testing/tiny-Llama4ForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_target_parameters.py: 20 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in trl-internal-testing/tiny-GptOssForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_transformers_irregular_targets
tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_transformers_target_parameters_raises
tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_transformers_target_parameters_fails
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in facebook/opt-125m - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_extra_keys_warning
  /app/src/peft/tuners/tuners_utils.py:877: UserWarning: You have passed exclude_modules=['model.decoder.layers.5.self_attn.q_proj'] but no modules were excluded. Please check that exclude_modules was set correctly.
    warnings.warn(

tests/test_trainable_tokens.py::TestTrainableTokens::test_save_pretrained_auto[peft_config0-True]
tests/test_trainable_tokens.py::TestTrainableTokens::test_save_pretrained_auto[peft_config1-True]
  /app/src/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
    warnings.warn(

tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_0_HuggingFaceH4_tiny_random_LlamaForCausalLM
tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_1_HuggingFaceH4_tiny_random_LlamaForCausalLM
tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_2_hf_internal_testing_tiny_random_gpt2
tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_3_hf_internal_testing_tiny_random_t5
tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_4_hf_internal_testing_tiny_random_GPTNeoXForCausalLM
  /app/src/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
    warnings.warn(

tests/test_tuners_utils.py::TestModelAndLayerStatus::test_merge_adapters_large
  /app/src/peft/tuners/tuners_utils.py:1678: UserWarning: Already following adapters were merged other. You are now additionally merging default.
    warnings.warn(

tests/test_tuners_utils.py::TestModelAndLayerStatus::test_model_merged_adapters_large
  /app/src/peft/tuners/lora/layer.py:984: UserWarning: Already unmerged. Nothing to do.
    warnings.warn("Already unmerged. Nothing to do.")

tests/test_tuners_utils.py::TestModelAndLayerStatus::test_model_merged_adapters_large
  /app/src/peft/tuners/lora/layer.py:1317: UserWarning: Already unmerged. Nothing to do.
    warnings.warn("Already unmerged. Nothing to do.")

tests/test_vera.py::TestVera::test_multiple_adapters_save_load_save_projection_false
  /app/src/peft/utils/save_and_load.py:531: UserWarning: Specified to not load vera_A and vera_B from state dictionary. This means we will be relying on PRNG initialisation to restore these projections using `config.projection_prng_key`, which may not be accurate on all system configurations.
    warnings.warn(

tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/utils/save_and_load.py:279: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.
    warnings.warn("Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.")

tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/tuners/tuners_utils.py:899: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
    warnings.warn(

tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/tuners/tuners_utils.py:899: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
    warnings.warn(

tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/tuners/tuners_utils.py:899: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens', 'model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
_____________ coverage: platform linux, python 3.11.0-candidate-1 ______________

Name                                                  Stmts   Miss  Cover   Missing
-----------------------------------------------------------------------------------
src/peft/__init__.py                                     10      0   100%
src/peft/auto.py                                         71      4    94%   61, 92, 99, 111
src/peft/config.py                                      168      8    95%   63-64, 72, 144, 211, 297, 322-323
src/peft/functional.py                                    4      4     0%   21-26
src/peft/helpers.py                                      72     21    71%   48-58, 84-98, 124-132, 235
src/peft/import_utils.py                                 94     42    55%   33-35, 41-46, 55-73, 87-98, 131-147, 158, 161-166
src/peft/mapping.py                                      18      3    83%   44, 78, 81
src/peft/mapping_func.py                                 36      0   100%
src/peft/mixed_model.py                                 154     25    84%   48, 51-54, 112, 120, 137, 143, 163-165, 231-234, 272, 289, 370, 379, 432, 436-439, 443, 448, 452
src/peft/optimizers/__init__.py                           3      0   100%
src/peft/optimizers/lorafa.py                           105     25    76%   67, 69, 71, 73, 93, 113, 127-131, 157, 176, 182-210
src/peft/optimizers/loraplus.py                          36     27    25%   61-121
src/peft/peft_model.py                                 1345    426    68%   163-166, 172, 219, 228, 245, 260, 300, 324-327, 444, 451, 471, 474-501, 506, 509, 513-535, 579, 632, 656, 681, 700-701, 748-749, 757, 767-769, 793, 808, 842, 848-854, 873-875, 910-912, 920, 1005, 1171-1238, 1243, 1373-1420, 1452, 1459, 1476, 1529, 1534-1536, 1540-1543, 1549, 1675-1727, 1740-1793, 1866-1868, 1879, 1901-1902, 1904-1905, 2041, 2049, 2059-2064, 2073-2095, 2105-2108, 2119, 2236-2240, 2243-2244, 2246-2247, 2290-2320, 2337, 2339-2342, 2344-2347, 2359-2360, 2381-2387, 2399, 2477-2478, 2510-2517, 2531-2584, 2597-2633, 2698-2699, 2731-2738, 2755-2811, 2825-2876, 2942, 2963-2964, 2966-2967, 3068, 3213
src/peft/tuners/__init__.py                              30      0   100%
src/peft/tuners/_buffer_dict.py                          62      8    87%   83, 92-94, 123, 137, 141, 159
src/peft/tuners/adalora/__init__.py                      16      7    56%   33-43
src/peft/tuners/adalora/bnb.py                           74     74     0%   15-143
src/peft/tuners/adalora/config.py                        36      2    94%   88, 92
src/peft/tuners/adalora/gptq.py                          31     26    16%   30-37, 40-67
src/peft/tuners/adalora/layer.py                        219     94    57%   31, 53, 58, 126, 142, 152-153, 169, 217, 236-252, 256-273, 278, 281-283, 286-335, 339-347, 351-360
src/peft/tuners/adalora/model.py                        152     79    48%   78, 104, 129, 136, 159-161, 163, 168, 181-188, 190-198, 200, 204-208, 217, 230-252, 256-284, 287-300, 323-342, 346
src/peft/tuners/adaption_prompt/__init__.py               6      0   100%
src/peft/tuners/adaption_prompt/config.py                25      1    96%   81
src/peft/tuners/adaption_prompt/layer.py                 86      5    94%   39, 54, 171, 185, 192
src/peft/tuners/adaption_prompt/model.py                 85      5    94%   64, 72, 98, 100, 168
src/peft/tuners/adaption_prompt/utils.py                 68     34    50%   47-58, 73, 84-89, 100-128, 141-146
src/peft/tuners/boft/__init__.py                          6      0   100%
src/peft/tuners/boft/config.py                           30      3    90%   152, 154, 158
src/peft/tuners/boft/fbd/__init__.py                      0      0   100%
src/peft/tuners/boft/layer.py                           495     97    80%   65-71, 78, 130-132, 136-138, 168, 234, 240-244, 247-254, 257-261, 283, 288, 301, 306-311, 319, 324-329, 336, 342-346, 388, 422-446, 528, 550-551, 583, 595, 625, 639, 713, 718, 724, 736, 741-746, 754, 759-764, 771, 777-781, 872-873, 916, 928, 951, 962, 975
src/peft/tuners/boft/model.py                            35      5    86%   78, 111, 117-121, 126
src/peft/tuners/bone/__init__.py                          6      0   100%
src/peft/tuners/bone/config.py                           26      2    92%   120, 124
src/peft/tuners/bone/layer.py                           186     29    84%   46, 64, 74, 79, 99-106, 109-113, 169, 188-189, 273-295, 327, 342-343
src/peft/tuners/bone/model.py                            29      3    90%   89, 115, 122
src/peft/tuners/c3a/__init__.py                           6      0   100%
src/peft/tuners/c3a/config.py                            23      2    91%   133, 137
src/peft/tuners/c3a/layer.py                            112     13    88%   47, 51, 61, 97, 103-108, 155, 171-172, 192
src/peft/tuners/c3a/model.py                             32      2    94%   63, 90
src/peft/tuners/c3a/utils.py                             30      0   100%
src/peft/tuners/cpt/__init__.py                           5      0   100%
src/peft/tuners/cpt/config.py                            35      1    97%   106
src/peft/tuners/cpt/model.py                             84      0   100%
src/peft/tuners/fourierft/__init__.py                     6      0   100%
src/peft/tuners/fourierft/config.py                      30      3    90%   199, 203, 206
src/peft/tuners/fourierft/layer.py                      104      5    95%   52, 58, 60, 159-160
src/peft/tuners/fourierft/model.py                       47      5    89%   67, 102, 108-112, 121
src/peft/tuners/hra/__init__.py                           6      0   100%
src/peft/tuners/hra/config.py                            27      3    89%   125, 129, 133
src/peft/tuners/hra/layer.py                            248     40    84%   50, 70, 85, 99-100, 111-118, 121-125, 177, 193-194, 213-220, 262, 330, 358-359, 385-392, 424, 447
src/peft/tuners/hra/model.py                             31      3    90%   89, 117, 126
src/peft/tuners/ia3/__init__.py                          15      7    53%   29-39
src/peft/tuners/ia3/bnb.py                               67     67     0%   15-129
src/peft/tuners/ia3/config.py                            22      0   100%
src/peft/tuners/ia3/layer.py                            194     10    95%   44, 50, 139-140, 246, 265-266, 298, 322, 330
src/peft/tuners/ia3/model.py                            120     23    81%   80-82, 85, 92, 97-105, 107-115, 138, 192, 198, 219, 222, 234, 241, 247, 251, 256, 284, 307, 314
src/peft/tuners/ln_tuning/__init__.py                     5      0   100%
src/peft/tuners/ln_tuning/config.py                      13      0   100%
src/peft/tuners/ln_tuning/layer.py                       64      7    89%   76, 82-83, 93-94, 106, 112
src/peft/tuners/ln_tuning/model.py                       44      1    98%   102
src/peft/tuners/loha/__init__.py                          6      0   100%
src/peft/tuners/loha/config.py                           25      1    96%   143
src/peft/tuners/loha/layer.py                           207      2    99%   126, 166
src/peft/tuners/loha/model.py                            22      0   100%
src/peft/tuners/lokr/__init__.py                          6      0   100%
src/peft/tuners/lokr/config.py                           28      1    96%   155
src/peft/tuners/lokr/layer.py                           230     15    93%   82-88, 146-147, 156, 186, 239, 286, 479-481
src/peft/tuners/lokr/model.py                            23      0   100%
src/peft/tuners/lora/__init__.py                         21      6    71%   50-52, 55-57, 60-62
src/peft/tuners/lora/aqlm.py                             48     31    35%   25, 42-49, 62-85, 88-89, 111-112
src/peft/tuners/lora/arrow.py                           210     16    92%   177, 212, 217, 316, 318, 339, 346-347, 351, 389, 400, 411, 420, 434, 444, 450
src/peft/tuners/lora/awq.py                              55     37    33%   39-50, 62-85, 88-89, 105-119
src/peft/tuners/lora/bnb.py                             316    316     0%   14-611
src/peft/tuners/lora/config.py                          134      8    94%   117, 119, 707, 709, 714-715, 724-727
src/peft/tuners/lora/corda.py                           173     17    90%   51, 111, 168, 181, 186, 246, 262, 284, 293, 298, 303, 333, 337, 342, 347, 352, 357
src/peft/tuners/lora/dora.py                            107      4    96%   48, 63, 97, 164
src/peft/tuners/lora/eetq.py                             55     42    24%   24-96, 112-116
src/peft/tuners/lora/eva.py                             333     55    83%   63, 67, 82-102, 132-135, 156-157, 164-165, 221-222, 234-238, 250, 253, 273, 277, 310-313, 322, 332-334, 383-385, 392, 434-436, 454, 461, 470, 544, 632, 713, 718, 722, 727
src/peft/tuners/lora/gptq.py                             65     43    34%   42-52, 66-80, 84-114, 117-118, 142-146, 151-152
src/peft/tuners/lora/hqq.py                             132    115    13%   30-237, 249
src/peft/tuners/lora/inc.py                              23     10    57%   31-59, 71-76
src/peft/tuners/lora/layer.py                          1182    104    91%   58, 63, 91, 167, 220-221, 250, 260, 271, 281, 285, 299-306, 308-313, 322, 341, 358, 368, 380, 384, 388, 394, 400, 406, 428-446, 479, 492, 505, 701, 892, 901, 923, 967, 1047, 1091, 1136, 1181, 1197, 1225, 1265, 1278, 1291, 1360, 1367, 1399, 1431, 1448, 1465, 1516, 1534, 1635, 1643, 1789-1790, 1870-1873, 1913, 1920, 1922, 1924, 1926, 1928, 1965, 1971, 1978, 1991, 2001-2002, 2004-2005, 2007-2008, 2010-2011, 2013, 2015-2016, 2023, 2042, 2047, 2075-2076, 2081, 2085, 2104, 2126, 2166-2167, 2179, 2203, 2255-2259
src/peft/tuners/lora/model.py                           346     49    86%   170, 221, 243, 270, 272, 295, 310-312, 315-317, 341, 393, 428, 430, 437, 449, 453, 497, 501, 503, 509, 515, 573, 601-605, 616-617, 624, 680, 694, 698-702, 713-717, 719-720, 740-744, 763, 779
src/peft/tuners/lora/torchao.py                          80     57    29%   35-40, 44-54, 57-91, 94-124, 127-128, 150-156
src/peft/tuners/lora/tp_layer.py                        169    140    17%   56-99, 117-186, 189-216, 231-256, 262-270, 280-304, 307-308, 325, 333-346
src/peft/tuners/lora/variants.py                        396     66    83%   55, 121, 129, 144-152, 229-230, 248, 338, 407-408, 454-486, 490, 494, 498, 502, 512-541, 551, 555, 559, 580, 621, 637-639, 694-695, 726, 750
src/peft/tuners/lycoris_utils.py                        104     26    75%   98-101, 140, 153-156, 159-166, 173-174, 181-188, 241-242, 257-258
src/peft/tuners/miss/__init__.py                          6      0   100%
src/peft/tuners/miss/config.py                           26      2    92%   136, 140
src/peft/tuners/miss/layer.py                           214     41    81%   50, 70, 75, 86, 91, 94-100, 116, 122-129, 132-136, 190-191, 197, 207-208, 219-220, 231, 307, 310-332, 364, 378, 383-384
src/peft/tuners/miss/model.py                            29      3    90%   89, 119, 126
src/peft/tuners/mixed/__init__.py                         2      0   100%
src/peft/tuners/mixed/model.py                          166     40    76%   82, 102-107, 119, 127-131, 150-160, 167, 172, 182-187, 195-196, 204, 220, 249-255, 260, 270, 276
src/peft/tuners/multitask_prompt_tuning/__init__.py       5      0   100%
src/peft/tuners/multitask_prompt_tuning/config.py        21      0   100%
src/peft/tuners/multitask_prompt_tuning/model.py         48      4    92%   38, 65, 71-73
src/peft/tuners/oft/__init__.py                          19     10    47%   37-52
src/peft/tuners/oft/aqlm.py                              41     25    39%   25, 45-49, 64-82, 85-86, 97, 102-103
src/peft/tuners/oft/awq.py                               49     32    35%   42-50, 64-83, 86-87, 98, 103-117
src/peft/tuners/oft/bnb.py                              188    188     0%   14-388
src/peft/tuners/oft/config.py                            39      4    90%   171, 173, 177, 191
src/peft/tuners/oft/eetq.py                              48     36    25%   24-94, 105, 110-114
src/peft/tuners/oft/gptq.py                              49     30    39%   41-48, 63-84, 87-88, 99, 106-110, 115-116
src/peft/tuners/oft/hqq.py                               94     79    16%   29-172, 179, 184
src/peft/tuners/oft/inc.py                               23     11    52%   31-59, 66, 71-76
src/peft/tuners/oft/layer.py                            425     80    81%   52-69, 193, 218, 249, 344-368, 380-384, 387-394, 397-401, 452-454, 457, 499, 506-508, 512-519, 593, 614-615, 654, 729, 737, 742-748, 751-753, 756, 848-849, 898, 923, 931-935
src/peft/tuners/oft/model.py                             54      9    83%   101, 122, 154-156, 159-161, 183, 197, 199
src/peft/tuners/p_tuning/__init__.py                      5      0   100%
src/peft/tuners/p_tuning/config.py                       17      0   100%
src/peft/tuners/p_tuning/model.py                        34      7    79%   84-96, 119, 124, 128
src/peft/tuners/poly/__init__.py                          6      0   100%
src/peft/tuners/poly/config.py                           21      0   100%
src/peft/tuners/poly/layer.py                            89      9    90%   49, 56, 103-108, 137
src/peft/tuners/poly/model.py                            52      5    90%   43, 52, 58, 65, 73
src/peft/tuners/poly/router.py                           34      3    91%   31, 64, 66
src/peft/tuners/prefix_tuning/__init__.py                 5      0   100%
src/peft/tuners/prefix_tuning/config.py                  10      0   100%
src/peft/tuners/prefix_tuning/model.py                   19      4    79%   65-66, 76-77
src/peft/tuners/prompt_tuning/__init__.py                 5      0   100%
src/peft/tuners/prompt_tuning/config.py                  23      0   100%
src/peft/tuners/prompt_tuning/model.py                   30      0   100%
src/peft/tuners/randlora/__init__.py                     15      7    53%   30-40
src/peft/tuners/randlora/bnb.py                         209    209     0%   14-456
src/peft/tuners/randlora/config.py                       26      0   100%
src/peft/tuners/randlora/layer.py                       176     13    93%   80-81, 106, 109, 133, 147, 150, 159, 162, 235, 251-252, 339
src/peft/tuners/randlora/model.py                       144     27    81%   60, 119-123, 133-134, 239, 255, 292-294, 297, 304, 309-317, 319-327, 330-343
src/peft/tuners/road/__init__.py                         15      7    53%   37-47
src/peft/tuners/road/bnb.py                             195    195     0%   14-407
src/peft/tuners/road/config.py                           21      1    95%   124
src/peft/tuners/road/layer.py                           199      8    96%   70, 102, 265, 276, 298-299, 380, 411
src/peft/tuners/road/model.py                            77      5    94%   55, 97-99, 102-104
src/peft/tuners/shira/__init__.py                         6      0   100%
src/peft/tuners/shira/config.py                          25      0   100%
src/peft/tuners/shira/layer.py                          102     12    88%   48, 64, 72, 95, 103, 106-109, 128, 163, 174-175
src/peft/tuners/shira/mask_functions.py                  14      0   100%
src/peft/tuners/shira/model.py                           42      6    86%   72, 81, 109, 115-121
src/peft/tuners/trainable_tokens/__init__.py              6      0   100%
src/peft/tuners/trainable_tokens/config.py               13      0   100%
src/peft/tuners/trainable_tokens/layer.py               115     13    89%   88-105, 127, 180, 187-188, 242
src/peft/tuners/trainable_tokens/model.py                41      0   100%
src/peft/tuners/tuners_utils.py                         775     77    90%   78-79, 88-105, 110, 114-123, 160, 177, 181, 184, 187, 190, 193, 196, 199, 284, 346, 456-460, 478, 499, 648, 857, 859, 870, 872, 987, 1035-1039, 1046, 1048, 1051-1054, 1214, 1226, 1229, 1251, 1373, 1393, 1497, 1530, 1575, 1603, 1671, 1718, 1725-1730, 1732, 1749-1754, 1789-1790
src/peft/tuners/vblora/__init__.py                        6      0   100%
src/peft/tuners/vblora/config.py                         29      1    97%   196
src/peft/tuners/vblora/layer.py                         130      5    96%   75, 77, 165, 175-176
src/peft/tuners/vblora/model.py                          77     12    84%   90, 127, 133-137, 146, 184-189, 205-206
src/peft/tuners/vera/__init__.py                         15      7    53%   30-40
src/peft/tuners/vera/bnb.py                             208    208     0%   14-411
src/peft/tuners/vera/config.py                           28      1    96%   156
src/peft/tuners/vera/layer.py                           149      7    95%   81, 99, 115, 125, 208-209, 268
src/peft/tuners/vera/model.py                           119     18    85%   57, 123, 133-134, 194, 229-231, 234, 241, 246-254, 256-264, 267-271, 280
src/peft/tuners/waveft/__init__.py                        6      0   100%
src/peft/tuners/waveft/config.py                         36      2    94%   253, 257
src/peft/tuners/waveft/constants.py                       1      0   100%
src/peft/tuners/waveft/layer.py                         145      5    97%   64, 132, 243, 257-258
src/peft/tuners/waveft/model.py                          97     17    82%   46-49, 52-55, 59, 84, 108, 118, 155, 161-165, 174, 192-195
src/peft/tuners/waveft/wavelet.py                        46      8    83%   65, 69, 113, 118, 123, 127, 130, 513
src/peft/tuners/waveft/waverec2d.py                     201     59    71%   45, 68, 85, 90-91, 96, 102, 120-126, 130, 147-150, 154-156, 160-163, 167-168, 172, 177, 180, 182, 184-188, 191, 194-195, 197, 203, 205, 207, 210-212, 214-218, 233, 251-255, 283, 288, 299-302
src/peft/tuners/xlora/__init__.py                         5      0   100%
src/peft/tuners/xlora/classifier.py                      88     11    88%   74-77, 80, 118-120, 142-143, 185-186
src/peft/tuners/xlora/config.py                          36      3    92%   94, 97, 102
src/peft/tuners/xlora/layer.py                          110     29    74%   114, 158, 161, 170-171, 186, 194-223
src/peft/tuners/xlora/model.py                          209     17    92%   75-85, 148, 241, 257, 261, 266-267, 350, 399, 405, 437, 442, 445
src/peft/utils/__init__.py                                7      0   100%
src/peft/utils/constants.py                              64     16    75%   23-32, 37-43, 50, 59
src/peft/utils/hotswap.py                               204     29    86%   46, 50, 75-76, 130, 136, 172, 175, 210, 216, 356, 360, 423, 457, 474-500, 542, 607
src/peft/utils/incremental_pca.py                       148     10    93%   74, 76-77, 103, 121, 142, 146, 148, 199-200
src/peft/utils/integrations.py                          162     87    46%   33, 45-49, 61-62, 65-66, 70-73, 79-86, 94-127, 133, 135, 145-166, 178-194, 217, 231-233, 237, 245-249, 254, 256, 261, 263
src/peft/utils/loftq_utils.py                           234    202    14%   37-49, 53-61, 65-87, 90-103, 106-113, 116-154, 158-170, 177-187, 192-237, 242-258, 270-308, 311-327, 366-409
src/peft/utils/merge_utils.py                            79      7    91%   91-92, 94, 100, 120-123
src/peft/utils/other.py                                 664    146    78%   115, 117, 119, 121, 123, 172, 183-211, 225-234, 265, 269, 276, 318, 324, 335, 362-370, 379-383, 386, 391, 395, 458, 475, 481, 485, 494, 502, 546-549, 556-559, 593, 624, 634, 655-656, 693, 713-715, 772-775, 835, 868-869, 900, 912, 926-928, 973, 1049, 1060, 1073, 1116, 1131-1135, 1138-1144, 1170-1174, 1184, 1202, 1213-1239, 1246-1276, 1295-1297, 1319-1323, 1339, 1358-1359, 1467-1509
src/peft/utils/peft_types.py                             61      6    90%   143, 146, 149, 162, 165, 169
src/peft/utils/save_and_load.py                         348     39    89%   54, 110-112, 117-119, 138-149, 171, 193, 208-211, 228, 236, 366, 373, 450, 502, 505, 521, 525, 549-553, 626, 651, 683-684, 691, 709
src/peft/utils/warning.py                                 1      0   100%
-----------------------------------------------------------------------------------
TOTAL                                                 17510   4566    74%
============================= slowest 10 durations =============================
86.62s call     tests/test_xlora.py::TestXlora::test_save_load_functional_pt
83.64s call     tests/test_xlora.py::TestXlora::test_save_load_functional
66.38s call     tests/test_xlora.py::TestXlora::test_scalings_logging_methods
64.20s call     tests/test_xlora.py::TestXlora::test_misc_methods
46.67s call     tests/test_xlora.py::TestXlora::test_softmax_topk
46.53s call     tests/test_xlora.py::TestXlora::test_functional_layerwise
36.66s call     tests/test_xlora.py::TestXlora::test_topk_lora
26.72s call     tests/test_poly.py::TestPoly::test_poly
22.02s call     tests/test_xlora.py::TestXlora::test_functional
20.06s call     tests/test_adaption_prompt.py::TestAdaptionPrompt::test_attributes[hf-internal-testing/tiny-random-MistralForCausalLM]
=========================== short test summary info ============================
FAILED tests/test_helpers.py::TestScalingAdapters::test_diffusers_pipeline - ...
FAILED tests/test_initialization.py::TestHotSwapping::test_hotswap_works[True-config0]
FAILED tests/test_initialization.py::TestHotSwapping::test_hotswap_works[True-config1]
FAILED tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_stable_diffusion
FAILED tests/test_tuners_utils.py::PeftCustomKwargsTester::test_maybe_include_all_linear_layers_diffusion
ERROR tests/test_stablediffusion.py - TypeError: CLIPTextModel.__init__() got...
ERROR tests/test_stablediffusion.py - TypeError: CLIPTextModel.__init__() got...
ERROR tests/test_stablediffusion.py - TypeError: CLIPTextModel.__init__() got...
= 5 failed, 14957 passed, 4648 skipped, 10 xfailed, 1 xpassed, 11731 warnings, 3 errors in 1079.59s (0:17:59) =
make: *** [Makefile:20: test] Error 1
