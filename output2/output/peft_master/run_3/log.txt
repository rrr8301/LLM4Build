Requirement already satisfied: pip in ./.venv/lib/python3.11/site-packages (25.2)
Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (from -r examples/arrow_multitask/requirements.txt (line 1)) (2.8.0)
Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (from -r examples/arrow_multitask/requirements.txt (line 2)) (4.57.0)
Requirement already satisfied: accelerate in ./.venv/lib/python3.11/site-packages (from -r examples/arrow_multitask/requirements.txt (line 3)) (1.10.1)
Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (from -r examples/arrow_multitask/requirements.txt (line 4)) (4.1.1)
Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (from -r examples/arrow_multitask/requirements.txt (line 5)) (1.7.2)
Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from -r examples/arrow_multitask/requirements.txt (line 6)) (4.67.1)
Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from -r examples/arrow_multitask/requirements.txt (line 7)) (2.3.3)
Requirement already satisfied: bitsandbytes in ./.venv/lib/python3.11/site-packages (from -r examples/arrow_multitask/requirements.txt (line 8)) (0.48.1)
Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (3.19.1)
Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (4.15.0)
Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (1.14.0)
Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (3.5)
Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (3.1.6)
Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (2025.9.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (12.8.93)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (12.8.90)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (12.8.90)
Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (9.10.2.21)
Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (12.8.4.1)
Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (11.3.3.83)
Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (10.3.9.90)
Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (11.7.3.90)
Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (12.5.8.93)
Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (0.7.1)
Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (2.27.3)
Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (12.8.90)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (12.8.93)
Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (1.13.1.3)
Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.11/site-packages (from torch->-r examples/arrow_multitask/requirements.txt (line 1)) (3.4.0)
Requirement already satisfied: setuptools>=40.8.0 in ./.venv/lib/python3.11/site-packages (from triton==3.4.0->torch->-r examples/arrow_multitask/requirements.txt (line 1)) (59.6.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.11/site-packages (from transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (0.35.3)
Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (25.0)
Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (6.0.3)
Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (2025.9.18)
Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (2.32.5)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.11/site-packages (from transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (0.22.1)
Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (0.6.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (1.1.10)
Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from accelerate->-r examples/arrow_multitask/requirements.txt (line 3)) (7.1.0)
Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.11/site-packages (from datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (21.0.0)
Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (0.4.0)
Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (2.3.3)
Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (3.6.0)
Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.11/site-packages (from datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (0.70.16)
Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (3.13.0)
Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->-r examples/arrow_multitask/requirements.txt (line 5)) (1.16.2)
Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->-r examples/arrow_multitask/requirements.txt (line 5)) (1.5.2)
Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->-r examples/arrow_multitask/requirements.txt (line 5)) (3.6.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (25.4.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (1.8.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (6.7.0)
Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (0.4.0)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (1.22.0)
Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (3.10)
Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (3.4.3)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->transformers->-r examples/arrow_multitask/requirements.txt (line 2)) (2025.10.5)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch->-r examples/arrow_multitask/requirements.txt (line 1)) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch->-r examples/arrow_multitask/requirements.txt (line 1)) (3.0.3)
Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (2025.2)
Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r examples/arrow_multitask/requirements.txt (line 4)) (1.17.0)
Obtaining file:///app
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (2.3.3)
Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (25.0)
Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (7.1.0)
Requirement already satisfied: pyyaml in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (6.0.3)
Requirement already satisfied: torch>=1.13.0 in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (2.8.0)
Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (4.57.0)
Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (4.67.1)
Requirement already satisfied: accelerate>=0.21.0 in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (1.10.1)
Requirement already satisfied: safetensors in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (0.6.2)
Requirement already satisfied: huggingface_hub>=0.25.0 in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (0.35.3)
Requirement already satisfied: black in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (25.9.0)
Requirement already satisfied: hf-doc-builder in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (0.5.0)
Requirement already satisfied: ruff~=0.12.8 in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (0.12.12)
Requirement already satisfied: pytest in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (8.4.2)
Requirement already satisfied: pytest-cov in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (7.0.0)
Requirement already satisfied: pytest-xdist in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (3.8.0)
Requirement already satisfied: parameterized in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (0.9.0)
Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (4.1.1)
Requirement already satisfied: diffusers in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (0.35.1)
Requirement already satisfied: scipy in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (1.16.2)
Requirement already satisfied: protobuf in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (6.32.1)
Requirement already satisfied: sentencepiece in ./.venv/lib/python3.11/site-packages (from peft==0.17.2.dev0) (0.2.1)
Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (3.19.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (2025.9.0)
Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (2.32.5)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (4.15.0)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (1.1.10)
Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (1.14.0)
Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (3.5)
Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (3.1.6)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.8.93)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.8.90)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.8.90)
Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (9.10.2.21)
Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.8.4.1)
Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (11.3.3.83)
Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (10.3.9.90)
Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (11.7.3.90)
Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.5.8.93)
Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (0.7.1)
Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (2.27.3)
Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.8.90)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.8.93)
Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (1.13.1.3)
Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.2.dev0) (3.4.0)
Requirement already satisfied: setuptools>=40.8.0 in ./.venv/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.13.0->peft==0.17.2.dev0) (59.6.0)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.17.2.dev0) (1.3.0)
Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.11/site-packages (from black->peft==0.17.2.dev0) (8.3.0)
Requirement already satisfied: mypy-extensions>=0.4.3 in ./.venv/lib/python3.11/site-packages (from black->peft==0.17.2.dev0) (1.1.0)
Requirement already satisfied: pathspec>=0.9.0 in ./.venv/lib/python3.11/site-packages (from black->peft==0.17.2.dev0) (0.12.1)
Requirement already satisfied: platformdirs>=2 in ./.venv/lib/python3.11/site-packages (from black->peft==0.17.2.dev0) (4.4.0)
Requirement already satisfied: pytokens>=0.1.10 in ./.venv/lib/python3.11/site-packages (from black->peft==0.17.2.dev0) (0.1.10)
Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.11/site-packages (from datasets->peft==0.17.2.dev0) (21.0.0)
Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets->peft==0.17.2.dev0) (0.4.0)
Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from datasets->peft==0.17.2.dev0) (2.3.3)
Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets->peft==0.17.2.dev0) (3.6.0)
Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.11/site-packages (from datasets->peft==0.17.2.dev0) (0.70.16)
Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (3.13.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (25.4.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (1.8.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (6.7.0)
Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (0.4.0)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (1.22.0)
Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->peft==0.17.2.dev0) (3.10)
Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0) (3.4.3)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0) (2025.10.5)
Requirement already satisfied: importlib_metadata in ./.venv/lib/python3.11/site-packages (from diffusers->peft==0.17.2.dev0) (8.7.0)
Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from diffusers->peft==0.17.2.dev0) (2025.9.18)
Requirement already satisfied: Pillow in ./.venv/lib/python3.11/site-packages (from diffusers->peft==0.17.2.dev0) (11.3.0)
Requirement already satisfied: GitPython in ./.venv/lib/python3.11/site-packages (from hf-doc-builder->peft==0.17.2.dev0) (3.1.45)
Requirement already satisfied: nbformat in ./.venv/lib/python3.11/site-packages (from hf-doc-builder->peft==0.17.2.dev0) (5.10.4)
Requirement already satisfied: gitdb<5,>=4.0.1 in ./.venv/lib/python3.11/site-packages (from GitPython->hf-doc-builder->peft==0.17.2.dev0) (4.0.12)
Requirement already satisfied: smmap<6,>=3.0.1 in ./.venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython->hf-doc-builder->peft==0.17.2.dev0) (5.0.2)
Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.11/site-packages (from importlib_metadata->diffusers->peft==0.17.2.dev0) (3.23.0)
Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft==0.17.2.dev0) (3.0.3)
Requirement already satisfied: fastjsonschema>=2.15 in ./.venv/lib/python3.11/site-packages (from nbformat->hf-doc-builder->peft==0.17.2.dev0) (2.21.2)
Requirement already satisfied: jsonschema>=2.6 in ./.venv/lib/python3.11/site-packages (from nbformat->hf-doc-builder->peft==0.17.2.dev0) (4.25.1)
Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.venv/lib/python3.11/site-packages (from nbformat->hf-doc-builder->peft==0.17.2.dev0) (5.8.1)
Requirement already satisfied: traitlets>=5.1 in ./.venv/lib/python3.11/site-packages (from nbformat->hf-doc-builder->peft==0.17.2.dev0) (5.14.3)
Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat->hf-doc-builder->peft==0.17.2.dev0) (2025.9.1)
Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat->hf-doc-builder->peft==0.17.2.dev0) (0.36.2)
Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat->hf-doc-builder->peft==0.17.2.dev0) (0.27.1)
Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->datasets->peft==0.17.2.dev0) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets->peft==0.17.2.dev0) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->datasets->peft==0.17.2.dev0) (2025.2)
Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->peft==0.17.2.dev0) (1.17.0)
Requirement already satisfied: iniconfig>=1 in ./.venv/lib/python3.11/site-packages (from pytest->peft==0.17.2.dev0) (2.1.0)
Requirement already satisfied: pluggy<2,>=1.5 in ./.venv/lib/python3.11/site-packages (from pytest->peft==0.17.2.dev0) (1.6.0)
Requirement already satisfied: pygments>=2.7.2 in ./.venv/lib/python3.11/site-packages (from pytest->peft==0.17.2.dev0) (2.19.2)
Requirement already satisfied: coverage>=7.10.6 in ./.venv/lib/python3.11/site-packages (from coverage[toml]>=7.10.6->pytest-cov->peft==0.17.2.dev0) (7.10.7)
Requirement already satisfied: execnet>=2.1 in ./.venv/lib/python3.11/site-packages (from pytest-xdist->peft==0.17.2.dev0) (2.1.1)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.11/site-packages (from transformers->peft==0.17.2.dev0) (0.22.1)
Building wheels for collected packages: peft
  Building editable for peft (pyproject.toml): started
  Building editable for peft (pyproject.toml): finished with status 'done'
  Created wheel for peft: filename=peft-0.17.2.dev0-0.editable-py3-none-any.whl size=10763 sha256=0a35e69d66cd81bcb3ea076da04bf98020ea42b3df49d81ed2dc172127c7395f
  Stored in directory: /tmp/pip-ephem-wheel-cache-g2v97p_5/wheels/57/0f/98/bb57b2b57b95807699b822a35c022f139d38a02c27922f27ce
Successfully built peft
Installing collected packages: peft
  Attempting uninstall: peft
    Found existing installation: peft 0.17.2.dev0
    Uninstalling peft-0.17.2.dev0:
      Successfully uninstalled peft-0.17.2.dev0
Successfully installed peft-0.17.2.dev0
python -m pytest -n 3 tests/
============================= test session starts ==============================
platform linux -- Python 3.11.0rc1, pytest-8.4.2, pluggy-1.6.0
rootdir: /app
configfile: pyproject.toml
plugins: xdist-3.8.0, cov-7.0.0
created: 3/3 workers
3 workers [19628 items]

ssssssssssssssssssssssssssss.ssssss..................................... [  0%]
........................................................................ [  0%]
........................................................................ [  1%]
........................................................................ [  1%]
........................................................................ [  1%]
........................................................................ [  2%]
........................................................................ [  2%]
........................................................................ [  2%]
........................................................................ [  3%]
........................................................................ [  3%]
........................................................................ [  4%]
........................................................................ [  4%]
........................................................................ [  4%]
........................................................................ [  5%]
........................................................................ [  5%]
........................................................................ [  5%]
...................................................................sssss [  6%]
sss.ssssssss.sssssssss.sssssssss.sssssssss.sssssssss.ssssssss.sssssssss. [  6%]
sssssssss.sssssssss.ssssssss.ssssssss.sssssssss.ssssssss.ssssssss.ssssss [  6%]
ss.ssssssss.ssssssss.ssssssss.ssssssss.ssssssss.sssssssss.ssssssss.sssss [  7%]
sss.ssssssss.sss........................................................ [  7%]
........................................................................ [  8%]
........................................................................ [  8%]
........................ss.............................................. [  8%]
........................................................................ [  9%]
........................................................................ [  9%]
........................................................................ [  9%]
........................................................................ [ 10%]
.............................ssssssssssss.sssssssssssssss.ssssssssssssss [ 10%]
s.sssssssssssssss.sssssssssssssss.ssssssssssssssss.sssssssssssssss.sssss [ 11%]
ssssssssssssssss.sssss............ssssssssss.ssssssssssssssss.ssssssssss [ 11%]
ssss.................................................................... [ 11%]
........................................................................ [ 12%]
........................................................................ [ 12%]
........................................................................ [ 12%]
........................................................................ [ 13%]
........................................................................ [ 13%]
........................................................................ [ 13%]
........................................................................ [ 14%]
....x....................x.............................................. [ 14%]
x....................................................................... [ 15%]
........................................................................ [ 15%]
........................................................................ [ 15%]
...........................ss........................................... [ 16%]
........................................................................ [ 16%]
........................................................................ [ 16%]
........................................................................ [ 17%]
........................................................................ [ 17%]
........................................................................ [ 17%]
........................................................................ [ 18%]
........................................................................ [ 18%]
...............................ssss..................................... [ 19%]
........................................................................ [ 19%]
......................................................s.ssssssssssssss.. [ 19%]
.....................................ssssssssssss....................... [ 20%]
........................................................................ [ 20%]
............................ss.ss....................................... [ 20%]
................................................................sssss.ss [ 21%]
sssss.sssssss.ssssssssssssssssss.ssssssssssssssssss.sssssss.sssssss.s... [ 21%]
....ssssssssss.sssss.ssss.sssss.sssss.ssss.ssssss.s..................... [ 22%]
........................ss..........ss.sss..s...................s....... [ 22%]
.................s...........................s....................s..... [ 22%]
s....s.....s....s...s......s...........................s.....sss......s. [ 23%]
............s....sss.......s....................s....................... [ 23%]
..................s....................................................s [ 23%]
............................................................s........... [ 24%]
.........................................s.............................. [ 24%]
..ssss.....s............................................................ [ 24%]
........................................................................ [ 25%]
........................................................................ [ 25%]
........................................................................ [ 26%]
........................................................................ [ 26%]
........................................................................ [ 26%]
........................................................................ [ 27%]
........................................................................ [ 27%]
........................ss.............................................. [ 27%]
........................................................................ [ 28%]
...........................sssss....................s.ssss.............. [ 28%]
.............sssssss.ssssssssssss.sssss.ssssss.sssssss.sss.............. [ 28%]
....................................................ss.................. [ 29%]
........................................................................ [ 29%]
........................................................................ [ 30%]
...sssss...............ssssssss.sss..............ss.sssssssssssssss.ssss [ 30%]
sssssss.sssssssssss.s................................................... [ 30%]
........................................................................ [ 31%]
........................................................................ [ 31%]
........................................................................ [ 31%]
..............ssssssssssssss.sssssssssssssss.ssssssssssssssss.ssssssssss [ 32%]
ssssss.ssssssssssssssss.ssssssssssssssss.sssssss..............ssssssssss [ 32%]
s.sssssssssssssssssssssssssssss.sssssssssssssssssssssss................. [ 33%]
........................................................................ [ 33%]
........................................................................ [ 33%]
........................................................................ [ 34%]
........................................................................ [ 34%]
........................................................................ [ 34%]
........................................................................ [ 35%]
....ssss................................................................ [ 35%]
........................................................................ [ 35%]
.................................ss..................................... [ 36%]
...ss.....................................................sss.ssssssssss [ 36%]
s.sssssssssss.ssssssss.ssssssssss.ssssssssssss.sssssss.sssssssss.sssssss [ 37%]
s.ssssssss.sssssssss.ssssssssss.sssssssss.sssssssss.sssssss.ssssssss.sss [ 37%]
sssss.ssssssssss.sssssssssssssssssssssssssssssssssss.sssssssssssssssss.s [ 37%]
ssss.....s...............s...............xx............................. [ 38%]
..........................................................s............. [ 38%]
.......s......xx............................ss.ssss.ssssssss.sssssssss.s [ 38%]
sssssssssssssssssssss.ssssssssssssssssssssssssss........................ [ 39%]
........................................................................ [ 39%]
........................................................................ [ 39%]
....................................................................s... [ 40%]
........................................................................ [ 40%]
.....................................................s............s..... [ 41%]
....................s.....................s.s...................s....... [ 41%]
......................................s..............s....s............. [ 41%]
......s...........................................................s..... [ 42%]
........................................................................ [ 42%]
........................................................................ [ 42%]
........................................................................ [ 43%]
........................................................................ [ 43%]
...................s.................................................... [ 44%]
..............................................................s......... [ 44%]
......................................s...........s..................... [ 44%]
.............s.........s.....................s.........s..........s...s. [ 45%]
...........s............................................................ [ 45%]
........................................................................ [ 45%]
..........................s............................................. [ 46%]
....................s................................................... [ 46%]
........................................................................ [ 46%]
........................................................................ [ 47%]
........................................................................ [ 47%]
........................................................................ [ 48%]
........................................................................ [ 48%]
........................................................................ [ 48%]
........................................................................ [ 49%]
..............................sssssssss.......s......................... [ 49%]
........................................................................ [ 49%]
.........................s..........s....s.s.sssssssssssssssssssssssssss [ 50%]
ssssssssssssssssssssssssss.sssssssssssssssssssssssssssssssssssssssssss.s [ 50%]
sssssss.ssssssssssssssssssssssssssssssssssssssssssssssss.sssssssss...... [ 50%]
...................................s...............................ssss. [ 51%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 51%]
sssssssssssssssssss.sssssssssssssssssssssssssssssssssssssssss.........s. [ 52%]
...............................s..................sssssssss.s..........s [ 52%]
...sssssssssssssssssssssssssss.sssssssssssssssssssssssssssssssssssssssss [ 52%]
sssssssssssssssssssssssssssssssssssssss.ssssssssss........s............. [ 53%]
.s.................s..............s.........s.................ssssssssss [ 53%]
sssssssssssssssssssssssssss.sssssssssssssssss..........................s [ 53%]
..............s................s...s.................................... [ 54%]
................s....................................................... [ 54%]
.....s................................................................s. [ 55%]
.....s.............................................s.................... [ 55%]
..........s..s.................s............................s........... [ 55%]
...........s....s..............................s..s...................ss [ 56%]
ssssss.ss.s.................s....s.....sssssssss....s....ssssssssss.s..s [ 56%]
.............s..s.s.............s...s....s..........s................... [ 56%]
....sssssssssssssssssss......................s.............sssssssssssss [ 57%]
ssss.sssss.sssssssssssssss...s.s..s...............s...........s..s...... [ 57%]
..................s...s..........s..s....................s..s....s...... [ 57%]
...................s....s.....................s.............s........... [ 58%]
........s...........s............................s......s............... [ 58%]
.........................s.........................s.................... [ 59%]
.......s...........s...................................ssssssssss..s..s. [ 59%]
...................................................s........s........... [ 59%]
................s....................s...............s.................. [ 60%]
......................s...............s..........s.....s.....s.......... [ 60%]
.s.....s.....................................s.......................... [ 60%]
.........................................s.................s............ [ 61%]
......sssssssssssssssssssssssssssssssssssssssssssss.sssssssssss.ssssssss [ 61%]
ssssssss....s...........ss.sssssssssssssssssssssssssssssssssssssssssssss [ 61%]
sssssss..s...........s..............ssssssssssssssssss..........s....... [ 62%]
sssssssssssssssssssssssssss.sssssssssssssssssssssssssss...s......s...... [ 62%]
sssssss.ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 63%]
sssss.ssssssssssssss.................................s.................. [ 63%]
...........................................................s............ [ 63%]
..................sssssssss...................s.............s........... [ 64%]
.......................s.........sssssssss..s..........s............ssss [ 64%]
sssss........................s.....s........sssssssssssssss.sssssssss.ss [ 64%]
sssssssssssssssssssssssssssssssssssss.ssssssssss.ssssssssssssssssssss.ss [ 65%]
ssssss.ssssssssssssssssssssss.ssss.sssssssssssssssssssssssssssss.ssssss. [ 65%]
sssssssssssssssssssssssssss.......................ssssssssssss.sssssssss [ 66%]
sssssssss.ssssssss.ssssssssssssssssssssssssssssssssss................... [ 66%]
..............................s....................s.s.................s [ 66%]
sssssssssssssssss.ssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 67%]
.s.s...............s...................s..................sss.ssssssssss [ 67%]
ssssssssssssss.....s.................s...s..................s........... [ 67%]
......sssssssss....ssssssssss.ssssssssssssssssssssssssssssss.sssssssssss [ 68%]
sssssssssssssssssssss..........s..............................s......... [ 68%]
...............................................................s........ [ 68%]
.......sssssssssssssss.ssssssssssssssssssssssssssssss.................ss [ 69%]
sssssssssssssssssssssssss...............s...........ssssssssssssssssssss [ 69%]
ssssssssssssssssssssssssss.ssssssss...sss.ssssssssssssssssss.sssssssssss [ 70%]
sssssssss.ssssssssssssssssssssssssssss.sssssssssssssssssssss............ [ 70%]
.................s....................s................s....ssssssssssss [ 70%]
.ssssssssssssssssssssssssssssssssssssssssss.sssss.ssssssssssssssssssssss [ 71%]
.sssssssss....................ssssssssss.sss.ssssssssssssss.ssssssssssss [ 71%]
sssssssss.ssssssssssssssssssssss.ss........s........................s... [ 71%]
...................ssssssssssssssssssssssssssssssssssssssss.ssss.sssssss [ 72%]
sss.................................................................s... [ 72%]
.......sssssssss...........................s..s............s............ [ 72%]
.s...ssssssssssssssssssssssssssss.ssssssss..s.......s.......s........... [ 73%]
..s............s........................................s........s...... [ 73%]
...........s............................sssssssss............ss.ssssssss [ 74%]
sssssssssssssssssssss.sssssssssssssssssssssssssssssssssssssssss..s...... [ 74%]
.............................................s..............s..s........ [ 74%]
..........................ssssssssssssssssss.sssssssssssssssssssssssssss [ 75%]
sssssssss...................s..ssss.ssssssssssssssssssssssssssssssss..s. [ 75%]
................s.............sssssssssssssssssssss.ssssssssssssssssssss [ 75%]
sss.ssssssssssss.ssssssss..........sssssssss.............s.ss........... [ 76%]
.................................ss..............ss..................... [ 76%]
....ss..s..................................sssssssssssssssss.sssssssssss [ 77%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssss.ssssss....... [ 77%]
...........................ssssssssssss.ss............ss....ssssssssssss [ 77%]
ssssssssssssssssssssssss...ssssssssssssssss............................. [ 78%]
...ssssss..................ssssssssss.ssss..........ssssssssssssssssssss [ 78%]
................................ss..........................ss.......sss [ 78%]
ssssss...........................................s...................... [ 79%]
..............ssssssssssssss........ssssssssssss....ssssssss............ [ 79%]
....................ssssssssss.........ssssss........................... [ 79%]
.............ssssssssss.....ssssss........ssssssssss.......sssssssssssss [ 80%]
sssssss.....................ss.......................................... [ 80%]
..ssssssssssss......ssssssss...s...................ssssssssssssss....sss [ 81%]
sssssssssssssssssssssssssssssssssssss.......sssssssssssssssssssssssss.ss [ 81%]
sssssssssssssssssss....ssssssssssssssssssssss.s......................... [ 81%]
.......................................sss.s............................ [ 82%]
........................................................................ [ 82%]
........................................................................ [ 82%]
..........ssss.......................................................... [ 83%]
.....................s.......ssss....................................... [ 83%]
........................................................................ [ 84%]
........................................................................ [ 84%]
s.............s............s............................................ [ 84%]
..s.s................................................................... [ 85%]
.............................................s.ssssssssssss............. [ 85%]
.....................................................s...............sss [ 85%]
s.ssssssssssss.sssssssssssssssssssssssss......ss....................ssss [ 86%]
sssssssssssssssssssssss.ssssssssssssss.ssss.sssssss..................s.. [ 86%]
............................ssssssssssssssssssssssssssssssssssssssss..ss [ 86%]
s...ss..ss........ssss..ss...ss...ss...ss..ss...ssssss.................. [ 87%]
.......ss..ss..........................................ssssssssssssssss. [ 87%]
.........................................sssssssssssssssssssssssssssssss [ 88%]
ssssssssssssssssssss.............sssssssssssssssssss.sssssssssssssssssss [ 88%]
sssssssssssssssssssssssssss.ssssssssssssssssssssssssssssssssssssssssssss [ 88%]
sssssssssssssssssssssssssssssssssssss.ssssss...ss.................F.ssss [ 89%]
Fatal Python error: Segmentation fault

Thread 0x00007f44297ba640 (most recent call first):
  File "/usr/lib/python3.11/ssl.py", line 1134 in read
  File "/usr/lib/python3.11/ssl.py", line 1278 in recv_into
  File "/usr/lib/python3.11/socket.py", line 705 in readinto
  File "/usr/lib/python3.11/http/client.py", line 279 in _read_status
  File "/usr/lib/python3.11/http/client.py", line 318 in begin
  File "/usr/lib/python3.11/http/client.py", line 1374 in getresponse
  File "/app/.venv/lib/python3.11/site-packages/urllib3/connection.py", line 565 in getresponse
  File "/app/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py", line 534 in _make_request
  File "/app/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py", line 787 in urlopen
  File "/app/.venv/lib/python3.11/site-packages/requests/adapters.py", line 644 in send
  File "/app/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 95 in send
  File "/app/.venv/lib/python3.11/site-packages/requests/sessions.py", line 703 in send
  File "/app/.venv/lib/python3.11/site-packages/requests/sessions.py", line 589 in request
  File "/app/.venv/lib/python3.11/site-packages/requests/sessions.py", line 602 in get
  File "/app/.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py", line 2648 in model_info
  File "/app/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114 in _inner_fn
  File "/app/.venv/lib/python3.11/site-packages/transformers/safetensors_conversion.py", line 59 in get_conversion_pr_reference
  File "/app/.venv/lib/python3.11/site-packages/transformers/safetensors_conversion.py", line 84 in auto_conversion
  File "/usr/lib/python3.11/threading.py", line 975 in run
  File "/usr/lib/python3.11/threading.py", line 1038 in _bootstrap_inner
  File "/usr/lib/python3.11/threading.py", line 995 in _bootstrap

Thread 0x00007f4598fe9640 (most recent call first):
  File "/usr/lib/python3.11/threading.py", line 324 in wait
  File "/usr/lib/python3.11/threading.py", line 622 in wait
  File "/app/.venv/lib/python3.11/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.11/threading.py", line 1038 in _bootstrap_inner
  File "/usr/lib/python3.11/threading.py", line 995 in _bootstrap

Thread 0x00007f45bbfff640 (most recent call first):
  File "/usr/lib/python3.11/threading.py", line 324 in wait
  File "/usr/lib/python3.11/threading.py", line 622 in wait
  File "/app/.venv/lib/python3.11/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.11/threading.py", line 1038 in _bootstrap_inner
  File "/usr/lib/python3.11/threading.py", line 995 in _bootstrap

Thread 0x00007f46873ff640 (most recent call first):
  File "/usr/lib/python3.11/threading.py", line 324 in wait
  File "/usr/lib/python3.11/threading.py", line 622 in wait
  File "/app/.venv/lib/python3.11/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.11/threading.py", line 1038 in _bootstrap_inner
  File "/usr/lib/python3.11/threading.py", line 995 in _bootstrap

Thread 0x00007f4686bfe640 (most recent call first):
  File "/usr/lib/python3.11/threading.py", line 324 in wait
  File "/usr/lib/python3.11/threading.py", line 622 in wait
  File "/app/.venv/lib/python3.11/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.11/threading.py", line 1038 in _bootstrap_inner
  File "/usr/lib/python3.11/threading.py", line 995 in _bootstrap

Thread 0x00007f4693aff640 (most recent call first):
  File "/app/.venv/lib/python3.11/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 61 in _recv_msg
  File "/app/.venv/lib/python3.11/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 195 in _read_thread
  File "/usr/lib/python3.11/threading.py", line 975 in run
  File "/usr/lib/python3.11/threading.py", line 1038 in _bootstrap_inner
  File "/usr/lib/python3.11/threading.py", line 995 in _bootstrap

Thread 0x00007f48fa5b1640 (most recent call first):
  File "/app/.venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 534 in read
  File "/app/.venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 567 in from_io
  File "/app/.venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 1160 in _thread_receiver
  File "/app/.venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 341 in run
  File "/app/.venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 411 in _perform_spawn

Thread 0x00007f48faf0b280 (most recent call first):
  File "/app/.venv/lib/python3.11/site-packages/bitsandbytes/backends/cpu/ops.py", line 43 in _
  File "/app/.venv/lib/python3.11/site-packages/torch/library.py", line 752 in func_no_dynamo
  File "/app/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 929 in _fn
  File "/app/.venv/lib/python3.11/site-packages/torch/_compile.py", line 53 in inner
  File "/app/.venv/lib/python3.11/site-packages/torch/_ops.py", line 829 in __call__
  File "/app/.venv/lib/python3.11/site-packages/bitsandbytes/functional.py", line 610 in quantize_blockwise
  File "/app/.venv/lib/python3.11/site-packages/bitsandbytes/functional.py", line 875 in quantize_4bit
  File "/app/.venv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py", line 296 in _quantize
  File "/app/.venv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py", line 337 in to
  File "/app/.venv/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py", line 222 in create_quantized_param
  File "/app/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 774 in _load_state_dict_into_meta_model
  File "/app/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/app/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 847 in load_shard_file
  File "/app/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5471 in _load_pretrained_model
  File "/app/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5051 in from_pretrained
  File "/app/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 277 in _wrapper
  File "/app/.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 604 in from_pretrained
  File "/app/tests/test_gpu_examples.py", line 4744 in test_low_cpu_mem_usage_with_quantization
  File "/app/.venv/lib/python3.11/site-packages/_pytest/python.py", line 157 in pytest_pyfunc_call
  File "/app/.venv/lib/python3.11/site-packages/pluggy/_callers.py", line 121 in _multicall
  File "/app/.venv/lib/python3.11/site-packages/pluggy/_manager.py", line 120 in _hookexec
  File "/app/.venv/lib/python3.11/site-packages/pluggy/_hooks.py", line 512 in __call__
  File "/app/.venv/lib/python3.11/site-packages/_pytest/python.py", line 1671 in runtest
  File "/app/.venv/lib/python3.11/site-packages/_pytest/runner.py", line 178 in pytest_runtest_call
  File "/app/.venv/lib/python3.11/site-packages/pluggy/_callers.py", line 121 in _multicall
  File "/app/.venv/lib/python3.11/site-packages/pluggy/_manager.py", line 120 in _hookexec
  File "/app/.venv/lib/python3.11/site-packages/pluggy/_hooks.py", line 512 in __call__
  File "/app/.venv/lib/python3.11/site-packages/_pytest/runner.py", line 246 in <lambda>
  File "/app/.venv/lib/python3.11/site-packages/_pytest/runner.py", line 344 in from_call
  File "/app/.venv/lib/python3.11/site-packages/_pytest/runner.py", line 245 in call_and_report
  File "/app/.venv/lib/python3.11/site-packages/_pytest/runner.py", line 136 in runtestprotocol
  File "/app/.venv/lib/python3.11/site-packages/_pytest/runner.py", line 117 in pytest_runtest_protocol
  File "/app/.venv/lib/python3.11/site-packages/pluggy/_callers.py", line 121 in _multicall
  File "/app/.venv/lib/python3.11/site-packages/pluggy/_manager.py", line 120 in _hookexec
  File "/app/.venv/lib/python3.11/site-packages/pluggy/_hooks.py", line 512 in __call__
  File "/app/.venv/lib/python3.11/site-packages/xdist/remote.py", line 227 in run_one_test
  File "/app/.venv/lib/python3.11/site-packages/xdist/remote.py", line 206 in pytest_runtestloop
  File "/app/.venv/lib/python3.11/site-packages/pluggy/_callers.py", line 121 in _multicall
  File "/app/.venv/lib/python3.11/site-packages/pluggy/_manager.py", line 120 in _hookexec
  File "/app/.venv/lib/python3.11/site-packages/pluggy/_hooks.py", line 512 in __call__
  File "/app/.venv/lib/python3.11/site-packages/_pytest/main.py", line 343 in _main
  File "/app/.venv/lib/python3.11/site-packages/_pytest/main.py", line 289 in wrap_session
  File "/app/.venv/lib/python3.11/site-packages/_pytest/main.py", line 336 in pytest_cmdline_main
  File "/app/.venv/lib/python3.11/site-packages/pluggy/_callers.py", line 121 in _multicall
  File "/app/.venv/lib/python3.11/site-packages/pluggy/_manager.py", line 120 in _hookexec
  File "/app/.venv/lib/python3.11/site-packages/pluggy/_hooks.py", line 512 in __call__
  File "/app/.venv/lib/python3.11/site-packages/xdist/remote.py", line 427 in <module>
  File "/app/.venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 1291 in executetask
  File "/app/.venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 341 in run
  File "/app/.venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 411 in _perform_spawn
  File "/app/.venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 389 in integrate_as_primary_thread
  File "/app/.venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 1273 in serve
  File "/app/.venv/lib/python3.11/site-packages/execnet/gateway_base.py", line 1806 in serve
  File "<string>", line 8 in <module>
  File "<string>", line 1 in <module>

Extension modules: numpy._core._multiarray_umath, numpy.linalg._umath_linalg, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, yaml._yaml, regex._regex, markupsafe._speedups, PIL._imaging, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._pcg64, numpy.random._mt19937, numpy.random._generator, numpy.random._philox, numpy.random._sfc64, numpy.random.mtrand, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, psutil._psutil_linux, psutil._psutil_posix, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._acero, pyarrow._csv, pyarrow._json, pyarrow._substrait, pyarrow._dataset, pyarrow._dataset_orc, pyarrow._parquet_encryption, pyarrow._dataset_parquet_encryption, pyarrow._dataset_parquet, scipy.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils, PIL._imagingmath (total: 208)
s...[gw2] node down: Not properly terminated
F
replacing crashed worker gw2
.................ssssssssssssssssssssssssssssss.....ssssssssssss........ [ 89%]
........................................................................ [ 89%]
........ssssssssssssssssssssssss........................................ [ 90%]
............................................................ssssssssssss [ 90%]
sssssssssssssssssssss................................................... [ 90%]
........................................x.......x....................... [ 91%]
F.........ss............................................................ [ 91%]
.............x....F......ss................................sssssssssssss [ 92%]
sssssssssss.......s.sssssssssssssss...........s......................... [ 92%]
..............................ssssssssssssssssssss.................sssss [ 92%]
ssssss.s...................................................sssssssssssss [ 93%]
sss.........................ssssssssssssssssssssssssssssssssssss........ [ 93%]
.....................ssssssssssssssssss................................. [ 93%]
........................................................................ [ 94%]
........................................................................ [ 94%]
.................sss.................................................... [ 95%]
.F...........F.......................................................... [ 95%]
........................................................................ [ 95%]
.............sss.................s.ss................................... [ 96%]
........................................................................ [ 96%]
....................................sssssssssssssss..................... [ 96%]
.........ssssssssssssssss.ssssssssssssssss......................ssssssss [ 97%]
........................ssssssssssssssss........................s....... [ 97%]
.......................ssss............................................. [ 97%]
..............ss.......................................ss.....s......... [ 98%]
........................................................................ [ 98%]
............F....................ssssss...............................ss [ 99%]
sssssssssss.....s............................FF...........E........E.... [ 99%]
................................................F....................... [ 99%]
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
...............F..................sssss                                  [100%]
==================================== ERRORS ====================================
________________ ERROR collecting tests/test_stablediffusion.py ________________
tests/test_stablediffusion.py:222: in <module>
    class TestStableDiffusionModel(PeftCommonTester):
tests/test_stablediffusion.py:228: in TestStableDiffusionModel
    sd_model = StableDiffusionPipeline.from_pretrained("hf-internal-testing/tiny-sd-pipe")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'
------------------------------- Captured stderr --------------------------------
Fetching 12 files:   0%|                                | 0/12 [00:00<?, ?it/s]Fetching 12 files:  17%|                    | 2/12 [00:00<00:03,  3.15it/s]Fetching 12 files:  33%|                | 4/12 [00:01<00:02,  3.84it/s]Fetching 12 files: 100%|| 12/12 [00:01<00:00, 14.40it/s]Fetching 12 files: 100%|| 12/12 [00:01<00:00, 10.19it/s]
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  40%|       | 2/5 [00:00<00:00, 35.67it/s]
________________ ERROR collecting tests/test_stablediffusion.py ________________
tests/test_stablediffusion.py:222: in <module>
    class TestStableDiffusionModel(PeftCommonTester):
tests/test_stablediffusion.py:228: in TestStableDiffusionModel
    sd_model = StableDiffusionPipeline.from_pretrained("hf-internal-testing/tiny-sd-pipe")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'
------------------------------- Captured stderr --------------------------------
Fetching 12 files:   0%|                                | 0/12 [00:00<?, ?it/s]Fetching 12 files:  17%|                    | 2/12 [00:00<00:03,  3.21it/s]Fetching 12 files:  33%|                | 4/12 [00:01<00:02,  3.91it/s]Fetching 12 files: 100%|| 12/12 [00:01<00:00, 14.59it/s]Fetching 12 files: 100%|| 12/12 [00:01<00:00, 10.35it/s]
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  80%|  | 4/5 [00:00<00:00, 26.73it/s]Loading pipeline components...:  80%|  | 4/5 [00:00<00:00, 26.11it/s]
________________ ERROR collecting tests/test_stablediffusion.py ________________
tests/test_stablediffusion.py:222: in <module>
    class TestStableDiffusionModel(PeftCommonTester):
tests/test_stablediffusion.py:228: in TestStableDiffusionModel
    sd_model = StableDiffusionPipeline.from_pretrained("hf-internal-testing/tiny-sd-pipe")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'
------------------------------- Captured stderr --------------------------------
Fetching 12 files:   0%|                                | 0/12 [00:00<?, ?it/s]Fetching 12 files:  17%|                    | 2/12 [00:00<00:02,  3.51it/s]Fetching 12 files:  33%|                | 4/12 [00:01<00:02,  3.92it/s]Fetching 12 files: 100%|| 12/12 [00:01<00:00, 14.02it/s]Fetching 12 files: 100%|| 12/12 [00:01<00:00, 10.23it/s]
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  80%|  | 4/5 [00:00<00:00, 22.31it/s]Loading pipeline components...:  80%|  | 4/5 [00:00<00:00, 21.79it/s]
________________ ERROR collecting tests/test_stablediffusion.py ________________
tests/test_stablediffusion.py:222: in <module>
    class TestStableDiffusionModel(PeftCommonTester):
tests/test_stablediffusion.py:228: in TestStableDiffusionModel
    sd_model = StableDiffusionPipeline.from_pretrained("hf-internal-testing/tiny-sd-pipe")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'
------------------------------- Captured stderr --------------------------------
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  60%|    | 3/5 [00:00<00:00, 24.52it/s]Loading pipeline components...:  60%|    | 3/5 [00:00<00:00, 23.78it/s]
____________________ ERROR at setup of test_incremental_pca ____________________
[gw1] linux -- Python 3.11.0 /app/.venv/bin/python

    @pytest.fixture(scope="module")
    def iris():
>       return load_dataset("scikit-learn/iris", split="train")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_incremental_pca.py:30:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/datasets/load.py:1392: in load_dataset
    builder_instance = load_dataset_builder(
.venv/lib/python3.11/site-packages/datasets/load.py:1132: in load_dataset_builder
    dataset_module = dataset_module_factory(
.venv/lib/python3.11/site-packages/datasets/load.py:1031: in dataset_module_factory
    raise e1 from None
.venv/lib/python3.11/site-packages/datasets/load.py:1004: in dataset_module_factory
    ).get_module()
      ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/datasets/load.py:591: in get_module
    standalone_yaml_path = cached_path(
.venv/lib/python3.11/site-packages/datasets/utils/file_utils.py:169: in cached_path
    ).resolve_path(url_or_filename)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:198: in resolve_path
    repo_and_revision_exist, err = self._repo_and_revision_exist(repo_type, repo_id, revision)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:125: in _repo_and_revision_exist
    self._api.repo_info(
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py:2864: in repo_info
    return method(
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py:2722: in dataset_info
    hf_raise_for_status(r)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

response = <Response [429]>, endpoint_name = None

    def hf_raise_for_status(response: Response, endpoint_name: Optional[str] = None) -> None:
        """
        Internal version of `response.raise_for_status()` that will refine a
        potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.

        This helper is meant to be the unique method to raise_for_status when making a call
        to the Hugging Face Hub.


        Example:
        ```py
            import requests
            from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError

            response = get_session().post(...)
            try:
                hf_raise_for_status(response)
            except HfHubHTTPError as e:
                print(str(e)) # formatted message
                e.request_id, e.server_message # details returned by server

                # Complete the error message with additional information once it's raised
                e.append_to_message("\n`create_commit` expects the repository to exist.")
                raise
        ```

        Args:
            response (`Response`):
                Response from the server.
            endpoint_name (`str`, *optional*):
                Name of the endpoint that has been called. If provided, the error message
                will be more complete.

        <Tip warning={true}>

        Raises when the request has failed:

            - [`~utils.RepositoryNotFoundError`]
                If the repository to download from cannot be found. This may be because it
                doesn't exist, because `repo_type` is not set correctly, or because the repo
                is `private` and you do not have access.
            - [`~utils.GatedRepoError`]
                If the repository exists but is gated and the user is not on the authorized
                list.
            - [`~utils.RevisionNotFoundError`]
                If the repository exists but the revision couldn't be find.
            - [`~utils.EntryNotFoundError`]
                If the repository exists but the entry (e.g. the requested file) couldn't be
                find.
            - [`~utils.BadRequestError`]
                If request failed with a HTTP 400 BadRequest error.
            - [`~utils.HfHubHTTPError`]
                If request failed for a reason not listed above.

        </Tip>
        """
        try:
            response.raise_for_status()
        except HTTPError as e:
            error_code = response.headers.get("X-Error-Code")
            error_message = response.headers.get("X-Error-Message")

            if error_code == "RevisionNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Revision Not Found for url: {response.url}."
                raise _format(RevisionNotFoundError, message, response) from e

            elif error_code == "EntryNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Entry Not Found for url: {response.url}."
                raise _format(EntryNotFoundError, message, response) from e

            elif error_code == "GatedRepo":
                message = (
                    f"{response.status_code} Client Error." + "\n\n" + f"Cannot access gated repo for url {response.url}."
                )
                raise _format(GatedRepoError, message, response) from e

            elif error_message == "Access to this resource is disabled.":
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Cannot access repository for url {response.url}."
                    + "\n"
                    + "Access to this resource is disabled."
                )
                raise _format(DisabledRepoError, message, response) from e

            elif error_code == "RepoNotFound" or (
                response.status_code == 401
                and error_message != "Invalid credentials in Authorization header"
                and response.request is not None
                and response.request.url is not None
                and REPO_API_REGEX.search(response.request.url) is not None
            ):
                # 401 is misleading as it is returned for:
                #    - private and gated repos if user is not authenticated
                #    - missing repos
                # => for now, we process them as `RepoNotFound` anyway.
                # See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Repository Not Found for url: {response.url}."
                    + "\nPlease make sure you specified the correct `repo_id` and"
                    " `repo_type`.\nIf you are trying to access a private or gated repo,"
                    " make sure you are authenticated. For more details, see"
                    " https://huggingface.co/docs/huggingface_hub/authentication"
                )
                raise _format(RepositoryNotFoundError, message, response) from e

            elif response.status_code == 400:
                message = (
                    f"\n\nBad request for {endpoint_name} endpoint:" if endpoint_name is not None else "\n\nBad request:"
                )
                raise _format(BadRequestError, message, response) from e

            elif response.status_code == 403:
                message = (
                    f"\n\n{response.status_code} Forbidden: {error_message}."
                    + f"\nCannot access content at: {response.url}."
                    + "\nMake sure your token has the correct permissions."
                )
                raise _format(HfHubHTTPError, message, response) from e

            elif response.status_code == 416:
                range_header = response.request.headers.get("Range")
                message = f"{e}. Requested range: {range_header}. Content-Range: {response.headers.get('Content-Range')}."
                raise _format(HfHubHTTPError, message, response) from e

            # Convert `HTTPError` into a `HfHubHTTPError` to display request information
            # as well (request id and/or server error message)
>           raise _format(HfHubHTTPError, str(e), response) from e
E           huggingface_hub.errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/datasets/scikit-learn/iris/revision/0bda0ce801be0fa2f464ff845a9d5ceae99aad7d (Request ID: Root=1-68e58b22-4046f91d2f1a064922cb4586;6b021bc5-ea4c-4f9a-a362-d52cb469d70d)
E
E           We had to rate limit your IP (169.237.118.139). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API.

.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:480: HfHubHTTPError
________________ ERROR at setup of test_incremental_pca_lowrank ________________
[gw1] linux -- Python 3.11.0 /app/.venv/bin/python

    @pytest.fixture(scope="module")
    def iris():
>       return load_dataset("scikit-learn/iris", split="train")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_incremental_pca.py:30:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/datasets/load.py:1392: in load_dataset
    builder_instance = load_dataset_builder(
.venv/lib/python3.11/site-packages/datasets/load.py:1132: in load_dataset_builder
    dataset_module = dataset_module_factory(
.venv/lib/python3.11/site-packages/datasets/load.py:1031: in dataset_module_factory
    raise e1 from None
.venv/lib/python3.11/site-packages/datasets/load.py:1004: in dataset_module_factory
    ).get_module()
      ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/datasets/load.py:591: in get_module
    standalone_yaml_path = cached_path(
.venv/lib/python3.11/site-packages/datasets/utils/file_utils.py:169: in cached_path
    ).resolve_path(url_or_filename)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:198: in resolve_path
    repo_and_revision_exist, err = self._repo_and_revision_exist(repo_type, repo_id, revision)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:125: in _repo_and_revision_exist
    self._api.repo_info(
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py:2864: in repo_info
    return method(
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py:2722: in dataset_info
    hf_raise_for_status(r)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

response = <Response [429]>, endpoint_name = None

    def hf_raise_for_status(response: Response, endpoint_name: Optional[str] = None) -> None:
        """
        Internal version of `response.raise_for_status()` that will refine a
        potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.

        This helper is meant to be the unique method to raise_for_status when making a call
        to the Hugging Face Hub.


        Example:
        ```py
            import requests
            from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError

            response = get_session().post(...)
            try:
                hf_raise_for_status(response)
            except HfHubHTTPError as e:
                print(str(e)) # formatted message
                e.request_id, e.server_message # details returned by server

                # Complete the error message with additional information once it's raised
                e.append_to_message("\n`create_commit` expects the repository to exist.")
                raise
        ```

        Args:
            response (`Response`):
                Response from the server.
            endpoint_name (`str`, *optional*):
                Name of the endpoint that has been called. If provided, the error message
                will be more complete.

        <Tip warning={true}>

        Raises when the request has failed:

            - [`~utils.RepositoryNotFoundError`]
                If the repository to download from cannot be found. This may be because it
                doesn't exist, because `repo_type` is not set correctly, or because the repo
                is `private` and you do not have access.
            - [`~utils.GatedRepoError`]
                If the repository exists but is gated and the user is not on the authorized
                list.
            - [`~utils.RevisionNotFoundError`]
                If the repository exists but the revision couldn't be find.
            - [`~utils.EntryNotFoundError`]
                If the repository exists but the entry (e.g. the requested file) couldn't be
                find.
            - [`~utils.BadRequestError`]
                If request failed with a HTTP 400 BadRequest error.
            - [`~utils.HfHubHTTPError`]
                If request failed for a reason not listed above.

        </Tip>
        """
        try:
            response.raise_for_status()
        except HTTPError as e:
            error_code = response.headers.get("X-Error-Code")
            error_message = response.headers.get("X-Error-Message")

            if error_code == "RevisionNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Revision Not Found for url: {response.url}."
                raise _format(RevisionNotFoundError, message, response) from e

            elif error_code == "EntryNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Entry Not Found for url: {response.url}."
                raise _format(EntryNotFoundError, message, response) from e

            elif error_code == "GatedRepo":
                message = (
                    f"{response.status_code} Client Error." + "\n\n" + f"Cannot access gated repo for url {response.url}."
                )
                raise _format(GatedRepoError, message, response) from e

            elif error_message == "Access to this resource is disabled.":
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Cannot access repository for url {response.url}."
                    + "\n"
                    + "Access to this resource is disabled."
                )
                raise _format(DisabledRepoError, message, response) from e

            elif error_code == "RepoNotFound" or (
                response.status_code == 401
                and error_message != "Invalid credentials in Authorization header"
                and response.request is not None
                and response.request.url is not None
                and REPO_API_REGEX.search(response.request.url) is not None
            ):
                # 401 is misleading as it is returned for:
                #    - private and gated repos if user is not authenticated
                #    - missing repos
                # => for now, we process them as `RepoNotFound` anyway.
                # See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Repository Not Found for url: {response.url}."
                    + "\nPlease make sure you specified the correct `repo_id` and"
                    " `repo_type`.\nIf you are trying to access a private or gated repo,"
                    " make sure you are authenticated. For more details, see"
                    " https://huggingface.co/docs/huggingface_hub/authentication"
                )
                raise _format(RepositoryNotFoundError, message, response) from e

            elif response.status_code == 400:
                message = (
                    f"\n\nBad request for {endpoint_name} endpoint:" if endpoint_name is not None else "\n\nBad request:"
                )
                raise _format(BadRequestError, message, response) from e

            elif response.status_code == 403:
                message = (
                    f"\n\n{response.status_code} Forbidden: {error_message}."
                    + f"\nCannot access content at: {response.url}."
                    + "\nMake sure your token has the correct permissions."
                )
                raise _format(HfHubHTTPError, message, response) from e

            elif response.status_code == 416:
                range_header = response.request.headers.get("Range")
                message = f"{e}. Requested range: {range_header}. Content-Range: {response.headers.get('Content-Range')}."
                raise _format(HfHubHTTPError, message, response) from e

            # Convert `HTTPError` into a `HfHubHTTPError` to display request information
            # as well (request id and/or server error message)
>           raise _format(HfHubHTTPError, str(e), response) from e
E           huggingface_hub.errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/datasets/scikit-learn/iris/revision/0bda0ce801be0fa2f464ff845a9d5ceae99aad7d (Request ID: Root=1-68e58b22-4046f91d2f1a064922cb4586;6b021bc5-ea4c-4f9a-a362-d52cb469d70d)
E
E           We had to rate limit your IP (169.237.118.139). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API.

.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:480: HfHubHTTPError
=================================== FAILURES ===================================
_____________________ TestFSDPWrap.test_bnb_4bit_wrap_fsdp _____________________
[gw2] linux -- Python 3.11.0 /app/.venv/bin/python

self = <tests.test_gpu_examples.TestFSDPWrap object at 0x7f4687bd8110>

    @pytest.mark.single_gpu_tests
    @require_bitsandbytes
    def test_bnb_4bit_wrap_fsdp(self):
        quant_config = BitsAndBytesConfig(
            load_in_4bit=True,
            # float32 must be used, or else FSDP will complain about mixed int and float dtypes
            bnb_4bit_compute_dtype=torch.float32,
            bnb_4bit_quant_storage=torch.float32,
            bnb_4bit_use_double_quant=True,
        )
        model = AutoModelForCausalLM.from_pretrained(
            "facebook/opt-125m",
            quantization_config=quant_config,
            torch_dtype=torch.float32,
        )
        # model = prepare_model_for_kbit_training(model)
        config = LoraConfig(
            target_modules=["q_proj", "v_proj"],
            task_type="CAUSAL_LM",
            use_dora=True,
        )
>       model = get_peft_model(model, config)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_gpu_examples.py:4599:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src/peft/mapping_func.py:125: in get_peft_model
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](
src/peft/peft_model.py:1845: in __init__
    super().__init__(model, peft_config, adapter_name, **kwargs)
src/peft/peft_model.py:128: in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/tuners_utils.py:292: in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage, state_dict=state_dict)
src/peft/tuners/tuners_utils.py:784: in inject_adapter
    self._create_and_replace(
src/peft/tuners/lora/model.py:248: in _create_and_replace
    new_module = self._create_new_module(lora_config, adapter_name, target, device_map=device_map, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/lora/model.py:335: in _create_new_module
    new_module = dispatcher(target, adapter_name, lora_config=lora_config, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/lora/bnb.py:609: in dispatch_bnb_4bit
    new_module = Linear4bit(target, adapter_name, **fourbit_kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/tuners/lora/bnb.py:345: in __init__
    self.update_layer(
src/peft/tuners/lora/layer.py:233: in update_layer
    self.lora_variant[adapter_name].init(self, **kwargs)
src/peft/tuners/lora/variants.py:154: in init
    dora_layer.update_layer(
src/peft/tuners/lora/dora.py:50: in update_layer
    weight = dequantize_module_weight(base_layer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/peft/utils/integrations.py:82: in dequantize_module_weight
    weight = dequantize_bnb_weight(weight, state=quant_state)  # no-op if not bnb
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

weight = Parameter containing:
Parameter(Params4bit([[ 2.8171e-18],
            [-1.8291e+19],
            [-3.9777e-21],
            ...,
            [ 2.4155e-28],
            [ 7.5266e-24],
            [ 1.8461e+28]]))
state = None

    def dequantize_bnb_weight(weight: torch.nn.Parameter, state=None):
        """Helper function to dequantize 4bit or 8bit bnb weights.

        Since dequantization is not supported on CPU, the weight will be temporarily moved to CUDA if necessary.
        """
        import bitsandbytes as bnb

>       if state.SCB is None:
           ^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'SCB'

src/peft/utils/integrations.py:96: AttributeError
__________________________ tests/test_gpu_examples.py __________________________
[gw2] linux -- Python 3.11.0 /app/.venv/bin/python
worker 'gw2' crashed while running 'tests/test_gpu_examples.py::TestLowCpuMemUsageDifferentDevices::test_low_cpu_mem_usage_with_quantization[bnb-4bit]'
_______________________ test_lora_plus_optimizer_sucess ________________________
[gw3] linux -- Python 3.11.0 /app/.venv/bin/python

    @require_bitsandbytes
    def test_lora_plus_optimizer_sucess():
        """
        Test if the optimizer is correctly created and step function runs without any exception
        """
        optimizer_cls = bnb.optim.Adam8bit
        optim_config = {
            "eps": 1e-6,
            "betas": (0.9, 0.999),
            "loraplus_weight_decay": 0.0,
        }
        model: SimpleNet = SimpleNet().to(torch_device)
        optim = create_loraplus_optimizer(
            model=model,
            optimizer_cls=optimizer_cls,
            lr=5e-5,
            loraplus_lr_ratio=1.2,
            loraplus_lr_embedding=1e-6,
            **optim_config,
        )
        loss = torch.nn.CrossEntropyLoss()
        bnb.optim.GlobalOptimManager.get_instance().register_parameters(model.parameters())
        x = torch.randint(100, (2, 4, 10)).to(torch_device)
        output = model(x).permute(0, 3, 1, 2)
        label = torch.randint(16, (2, 4, 10)).to(torch_device)
        loss_value = loss(output, label)
        loss_value.backward()
>       optim.step()

tests/test_loraplus.py:99:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:516: in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:120: in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/bitsandbytes/optim/optimizer.py:291: in step
    self.update_step(group, p, gindex, pindex)
.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:120: in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/bitsandbytes/optim/optimizer.py:520: in update_step
    F.optimizer_update_32bit(
.venv/lib/python3.11/site-packages/bitsandbytes/functional.py:1178: in optimizer_update_32bit
    is_on_gpu([g, p, state1, state2, unorm_vec])
.venv/lib/python3.11/site-packages/bitsandbytes/functional.py:361: in is_on_gpu
    f"All input tensors need to be on the same GPU, but found some tensors to not be on a GPU:\n {[(t.shape, t.device) for t in tensors]}",
                                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

.0 = <list_iterator object at 0x7f9ac39141c0>

>       f"All input tensors need to be on the same GPU, but found some tensors to not be on a GPU:\n {[(t.shape, t.device) for t in tensors]}",
                                                                                                        ^^^^^^^
    )
E   AttributeError: 'NoneType' object has no attribute 'shape'

.venv/lib/python3.11/site-packages/bitsandbytes/functional.py:361: AttributeError
_ TestInjectAdapterFromStateDict.test_inject_from_state_dict_stable_diffusion __
[gw3] linux -- Python 3.11.0 /app/.venv/bin/python

self = <tests.test_low_level_api.TestInjectAdapterFromStateDict object at 0x7f9ac83b9810>

    def test_inject_from_state_dict_stable_diffusion(self):
        # same test as above, but with stable diffusion model and only testing LoRA
        model_id = "hf-internal-testing/tiny-sd-pipe"
        config_text_encoder = LoraConfig(target_modules=["k_proj", "q_proj", "v_proj", "out_proj", "fc1", "fc2"])
        config_unet = LoraConfig(
            target_modules=[
                "proj_in",
                "proj_out",
                "to_k",
                "to_q",
                "to_v",
                "to_out.0",
                "ff.net.0.proj",
                "ff.net.2",
            ]
        )
        with hub_online_once(model_id):
>           pipe = StableDiffusionPipeline.from_pretrained(model_id)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_low_level_api.py:286:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'transformers.models.clip.modeling_clip.CLIPTextModel'>
pretrained_model_name_or_path = '/root/.cache/huggingface/hub/models--hf-internal-testing--tiny-sd-pipe/snapshots/39f7e1819d772fbb8dd7f2aa3da8e2fc5a9bd917/text_encoder'
config = CLIPTextConfig {
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dty...en_layers": 5,
  "pad_token_id": 1,
  "projection_dim": 32,
  "transformers_version": "4.57.0",
  "vocab_size": 1000
}

cache_dir = None, ignore_mismatched_sizes = False, force_download = False
local_files_only = False, token = None, revision = 'main'
use_safetensors = None, weights_only = True, model_args = ()
kwargs = {'offload_state_dict': None}, state_dict = None

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        **kwargs,
    ) -> SpecificPreTrainedModelType:
        r"""
        Instantiate a pretrained pytorch model from a pre-trained model configuration.

        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you should first set it back in training mode with `model.train()`.

        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come
        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
        task.

        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those
        weights are discarded.

        Parameters:
            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):
                Can be either:

                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
                    - A path to a *directory* containing model weights saved using
                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In
                      this case, `from_tf` should be set to `True` and a configuration object should be provided as
                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a
                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.
                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,
                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to
                      `True`.
                    - `None` if you are both providing the configuration and state dictionary (resp. with keyword
                      arguments `config` and `state_dict`).
            model_args (sequence of positional arguments, *optional*):
                All remaining positional arguments will be passed to the underlying model's `__init__` method.
            config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):
                Can be either:

                    - an instance of a class derived from [`PretrainedConfig`],
                    - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].

                Configuration for the model to use instead of an automatically loaded configuration. Configuration can
                be automatically loaded when:

                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained
                      model).
                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the
                      save directory.
                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a
                      configuration JSON file named *config.json* is found in the directory.
            state_dict (`dict[str, torch.Tensor]`, *optional*):
                A state dictionary to use instead of a state dictionary loaded from saved weights file.

                This option can be used if you want to create a model from a pretrained configuration but load your own
                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and
                [`~PreTrainedModel.from_pretrained`] is not a simpler option.
            cache_dir (`Union[str, os.PathLike]`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the
                standard cache should not be used.
            from_tf (`bool`, *optional*, defaults to `False`):
                Load the model weights from a TensorFlow checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            from_flax (`bool`, *optional*, defaults to `False`):
                Load the model weights from a Flax checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):
                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size
                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a
                checkpoint with 3 labels).
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
                cached versions if they exist.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
            output_loading_info(`bool`, *optional*, defaults to `False`):
                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.
            local_files_only(`bool`, *optional*, defaults to `False`):
                Whether or not to only look at local files (i.e., do not try to download the model).
            token (`str` or `bool`, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use
                the token generated when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.

                <Tip>

                To test a pull request you made on the Hub, you can pass `revision="refs/pr/<pr_number>"`.

                </Tip>
            attn_implementation (`str`, *optional*):
                The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)), or `"flash_attention_3"` (using [Dao-AILab/flash-attention/hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.

                Accept HF kernel references in the form:
                  <namespace>/<repo_name>[@<revision>][:<kernel_name>]

                - <namespace> and <repo_name> are any non-"/" and non-":" sequences.
                - "@<revision>" is optional (branch, tag, or commit-ish), e.g. "@main", "@v1.2.0", "@abc123".
                - ":<kernel_name>" is optional and selects a function inside the kernel repo.
                - Both options can appear together and in this order only: @revision first, then :kernel_name.
                - We intentionally allow a leading "<wrapper>|" prefix (e.g., "flash|...") because the code
                  strips it before loading; '|' is not excluded in the character classes here.

                Examples that match:
                  "org/model"
                  "org/model@main"
                  "org/model:custom_kernel"
                  "org/model@v1.2.3:custom_kernel"

            > Parameters for big model inference

            dtype (`str` or `torch.dtype`, *optional*):
                Override the default `torch_dtype` and load the model under a specific `dtype`. The different options
                are:

                1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified
                  `dtype`, ignoring the model's `config.dtype` if one exists. If not specified
                  - the model will get loaded in `torch.float` (fp32).

                2. `"auto"` - A `dtype` or `torch_dtype` entry in the `config.json` file of the model will be
                  attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in
                  the checkpoint that's of a floating point type and use that as `dtype`. This will load the model
                  using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how
                  the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.

                3. A string that is a valid `torch.dtype`. E.g. "float32" loads the model in `torch.float32`, "float16" loads in `torch.float16` etc.

                <Tip>

                For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or
                reach out to the authors and ask them to add this information to the model's card and to insert the
                `dtype` or `torch_dtype` entry in `config.json` on the hub.

                </Tip>

            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):
                A map that specifies where each submodule should go. It doesn't need to be refined to each
                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
                same device. If we only pass the device (*e.g.*, `"cpu"`, `"cuda:1"`, `"mps"`, or a GPU ordinal rank
                like `1`) on which the model will be allocated, the device map will map the entire model to this
                device. Passing `device_map = 0` means put the whole model on GPU 0.

                To have Accelerate compute the most optimized `device_map` automatically, set `device_map="auto"`. For
                more information about each option see [designing a device
                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).
            max_memory (`Dict`, *optional*):
                A dictionary device identifier to maximum memory if using `device_map`. Will default to the maximum memory available for each
                GPU and the available CPU RAM if unset.
            tp_plan (`str`, *optional*):
                A torch tensor parallel plan, see [here](https://pytorch.org/tutorials/intermediate/TP_tutorial.html). Currently, it only accepts
                `tp_plan="auto"` to use predefined plan based on the model. Note that if you use it, you should launch your script accordingly with
                `torchrun [args] script.py`. This will be much faster than using a `device_map`, but has limitations.
            tp_size (`str`, *optional*):
                A torch tensor parallel degree. If not provided would default to world size.
            device_mesh (`torch.distributed.DeviceMesh`, *optional*):
                A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.
                If provided, it has to contain dimension named `"tp"` in case it's > 1 dimensional, this dimension will be used for tensor parallelism
            offload_folder (`str` or `os.PathLike`, *optional*):
                If the `device_map` contains any value `"disk"`, the folder where we will offload weights.
            offload_buffers (`bool`, *optional*):
                Whether or not to offload the buffers with the model parameters.
            quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):
                A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g
                bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and
                `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes
                quantizations and not preferred. consider inserting all such arguments into quantization_config
                instead.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            variant (`str`, *optional*):
                If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is
                ignored when using `from_tf` or `from_flax`.
            use_safetensors (`bool`, *optional*, defaults to `None`):
                Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`
                is not installed, it will be set to `False`.
            weights_only (`bool`, *optional*, defaults to `True`):
                Indicates whether unpickler should be restricted to loading only tensors, primitive types,
                dictionaries and any types added via torch.serialization.add_safe_globals().
                When set to False, we can load wrapper tensor subclass weights.
            key_mapping (`dict[str, str], *optional*):
                A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers
                architecture, but was not converted accordingly.
            kwargs (remaining dictionary of keyword arguments, *optional*):
                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or
                automatically loaded:

                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the
                      underlying model's `__init__` method (we assume all relevant updates to the configuration have
                      already been done)
                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class
                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that
                      corresponds to a configuration attribute will be used to override said attribute with the
                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute
                      will be passed to the underlying model's `__init__` function.

        <Tip>

        Activate the special ["offline-mode"](https://huggingface.co/transformers/installation.html#offline-mode) to
        use this method in a firewalled environment.

        </Tip>

        Examples:

        ```python
        >>> from transformers import BertConfig, BertModel

        >>> # Download model and configuration from huggingface.co and cache.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased")
        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
        >>> model = BertModel.from_pretrained("./test/saved_model/")
        >>> # Update configuration during loading.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", output_attentions=True)
        >>> assert model.config.output_attentions == True
        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).
        >>> config = BertConfig.from_json_file("./tf_model/my_tf_model_config.json")
        >>> model = BertModel.from_pretrained("./tf_model/my_tf_checkpoint.ckpt.index", from_tf=True, config=config)
        >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", from_flax=True)
        ```
        """
        state_dict = kwargs.pop("state_dict", None)
        from_tf = kwargs.pop("from_tf", False)
        from_flax = kwargs.pop("from_flax", False)
        proxies = kwargs.pop("proxies", None)
        output_loading_info = kwargs.pop("output_loading_info", False)
        use_auth_token = kwargs.pop("use_auth_token", None)
        from_pipeline = kwargs.pop("_from_pipeline", None)
        from_auto_class = kwargs.pop("_from_auto", False)
        dtype = kwargs.pop("dtype", None)
        torch_dtype = kwargs.pop("torch_dtype", None)  # kept for BC
        device_map = kwargs.pop("device_map", None)
        max_memory = kwargs.pop("max_memory", None)
        offload_folder = kwargs.pop("offload_folder", None)
        offload_buffers = kwargs.pop("offload_buffers", False)
        load_in_8bit = kwargs.pop("load_in_8bit", False)
        load_in_4bit = kwargs.pop("load_in_4bit", False)
        quantization_config = kwargs.pop("quantization_config", None)
        subfolder = kwargs.pop("subfolder", "")
        commit_hash = kwargs.pop("_commit_hash", None)
        variant = kwargs.pop("variant", None)
        adapter_kwargs = kwargs.pop("adapter_kwargs", {})
        adapter_name = kwargs.pop("adapter_name", "default")
        generation_config = kwargs.pop("generation_config", None)
        gguf_file = kwargs.pop("gguf_file", None)
        tp_plan = kwargs.pop("tp_plan", None)
        tp_size = kwargs.pop("tp_size", None)
        distributed_config: DistributedConfig = kwargs.pop("distributed_config", None)
        device_mesh = kwargs.pop("device_mesh", None)
        trust_remote_code = kwargs.pop("trust_remote_code", None)
        use_kernels = kwargs.pop("use_kernels", False)

        key_mapping = kwargs.pop("key_mapping", None)
        # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model
        if key_mapping is None and any(
            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS
        ):
            key_mapping = cls._checkpoint_conversion_mapping

        if distributed_config is not None:
            tp_plan = "auto"

        # Not used anymore -- remove them from the kwargs
        _ = kwargs.pop("resume_download", None)
        _ = kwargs.pop("mirror", None)
        _ = kwargs.pop("_fast_init", True)
        _ = kwargs.pop("low_cpu_mem_usage", None)

        # For BC on torch_dtype argument
        if torch_dtype is not None:
            logger.warning_once("`torch_dtype` is deprecated! Use `dtype` instead!")
            # If both kwargs are provided, use `dtype`
            dtype = dtype if dtype is not None else torch_dtype

        if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):
            raise ValueError(
                "`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies."
            )
        if tp_size is not None and tp_plan is None:
            raise ValueError("tp_plan has to be set when tp_size is passed.")
        if tp_plan is not None and tp_plan != "auto":
            # TODO: we can relax this check when we support taking tp_plan from a json file, for example.
            raise ValueError(f"tp_plan supports 'auto' only for now but got {tp_plan}.")
        if tp_plan is not None and device_map is not None:
            raise ValueError(
                "`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization."
            )

        if device_map == "auto" and int(os.environ.get("WORLD_SIZE", "0")):
            logger.info(
                "You've set device_map=`auto` while triggering a distributed run with torchrun. This might lead to unexpected behavior. "
                "If your plan is to load the model on each device, you should set device_map={"
                ": PartialState().process_index} where PartialState comes from accelerate library"
            )

        # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple
        # `device_map` pointing to the correct device
        if tp_plan is not None:
            if device_mesh is None:
                tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(tp_plan, tp_size=tp_size)
            else:
                if device_mesh.ndim > 1:
                    if "tp" not in device_mesh.mesh_dim_names:
                        raise ValueError(
                            "When using `tp_plan` and n-d `device_mesh`, it must contain a 'tp' dimension. "
                            "Please provide a valid `device_mesh`."
                        )
                    device_mesh = device_mesh["tp"]
                tp_size = device_mesh.size()
                device_map = torch.device(f"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}")

            if tp_size is None:
                tp_size = torch.distributed.get_world_size()

        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError(
                    "`token` and `use_auth_token` are both specified. Please set only the argument `token`."
                )
            token = use_auth_token

        if token is not None and adapter_kwargs is not None and "token" not in adapter_kwargs:
            adapter_kwargs["token"] = token

        if gguf_file is not None and not is_accelerate_available():
            raise ValueError("accelerate is required when loading a GGUF file `pip install accelerate`.")

        if commit_hash is None:
            if not isinstance(config, PretrainedConfig):
                # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible
                resolved_config_file = cached_file(
                    pretrained_model_name_or_path,
                    CONFIG_NAME,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    token=token,
                    revision=revision,
                    subfolder=subfolder,
                    _raise_exceptions_for_gated_repo=False,
                    _raise_exceptions_for_missing_entries=False,
                    _raise_exceptions_for_connection_errors=False,
                )
                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)
            else:
                commit_hash = getattr(config, "_commit_hash", None)

        if is_peft_available():
            _adapter_model_path = adapter_kwargs.pop("_adapter_model_path", None)

            if _adapter_model_path is None:
                _adapter_model_path = find_adapter_config_file(
                    pretrained_model_name_or_path,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    _commit_hash=commit_hash,
                    **adapter_kwargs,
                )
            if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):
                with open(_adapter_model_path, "r", encoding="utf-8") as f:
                    _adapter_model_path = pretrained_model_name_or_path
                    pretrained_model_name_or_path = json.load(f)["base_model_name_or_path"]
        else:
            _adapter_model_path = None

        # Potentially detect context manager or global device, and use it (only if no device_map was provided)
        if device_map is None and not is_deepspeed_zero3_enabled():
            device_in_context = get_torch_context_manager_or_global_device()
            if device_in_context == torch.device("meta"):
                raise RuntimeError(
                    "You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\n"
                    "This is an anti-pattern as `from_pretrained` wants to load existing weights.\nIf you want to initialize an "
                    "empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`"
                )
            device_map = device_in_context

        # change device_map into a map if we passed an int, a str or a torch.device
        if isinstance(device_map, torch.device):
            device_map = {"": device_map}
        elif isinstance(device_map, str) and device_map not in ["auto", "balanced", "balanced_low_0", "sequential"]:
            try:
                device_map = {"": torch.device(device_map)}
            except RuntimeError:
                raise ValueError(
                    "When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or "
                    f"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}."
                )
        elif isinstance(device_map, int):
            if device_map < 0:
                raise ValueError(
                    "You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' "
                )
            else:
                device_map = {"": device_map}

        if device_map is not None:
            if is_deepspeed_zero3_enabled():
                raise ValueError("DeepSpeed Zero-3 is not compatible with passing a `device_map`.")
            if not is_accelerate_available():
                raise ValueError(
                    "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` "
                    "requires `accelerate`. You can install it with `pip install accelerate`"
                )

        # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.
        if load_in_4bit or load_in_8bit:
            if quantization_config is not None:
                raise ValueError(
                    "You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing "
                    "`quantization_config` argument at the same time."
                )

            # preparing BitsAndBytesConfig from kwargs
            config_dict = {k: v for k, v in kwargs.items() if k in inspect.signature(BitsAndBytesConfig).parameters}
            config_dict = {**config_dict, "load_in_4bit": load_in_4bit, "load_in_8bit": load_in_8bit}
            quantization_config, kwargs = BitsAndBytesConfig.from_dict(
                config_dict=config_dict, return_unused_kwargs=True, **kwargs
            )
            logger.warning(
                "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. "
                "Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead."
            )

        from_pt = not (from_tf | from_flax)

        user_agent = {"file_type": "model", "framework": "pytorch", "from_auto_class": from_auto_class}
        if from_pipeline is not None:
            user_agent["using_pipeline"] = from_pipeline

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True

        # Load config if we don't provide a configuration
        if not isinstance(config, PretrainedConfig):
            config_path = config if config is not None else pretrained_model_name_or_path
            config, model_kwargs = cls.config_class.from_pretrained(
                config_path,
                cache_dir=cache_dir,
                return_unused_kwargs=True,
                force_download=force_download,
                proxies=proxies,
                local_files_only=local_files_only,
                token=token,
                revision=revision,
                subfolder=subfolder,
                gguf_file=gguf_file,
                _from_auto=from_auto_class,
                _from_pipeline=from_pipeline,
                **kwargs,
            )
            if "gguf_file" in model_kwargs:
                model_kwargs.pop("gguf_file")
        else:
            config = copy.deepcopy(config)
            model_kwargs = kwargs

        # Because some composite configs call super().__init__ before instantiating the sub-configs, we need this call
        # to correctly redispatch recursively if the kwarg is provided
        if "attn_implementation" in kwargs:
            config._attn_implementation = kwargs.pop("attn_implementation")

        transformers_explicit_filename = getattr(config, "transformers_weights", None)

        if transformers_explicit_filename is not None:
            if not transformers_explicit_filename.endswith(
                ".safetensors"
            ) and not transformers_explicit_filename.endswith(".safetensors.index.json"):
                raise ValueError(
                    "The transformers file in the config seems to be incorrect: it is neither a safetensors file "
                    "(*.safetensors) nor a safetensors index file (*.safetensors.index.json): "
                    f"{transformers_explicit_filename}"
                )

        hf_quantizer, config, dtype, device_map = get_hf_quantizer(
            config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent
        )

        if gguf_file is not None and hf_quantizer is not None:
            raise ValueError(
                "You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub."
            )

        if (
            gguf_file
            and device_map is not None
            and ((isinstance(device_map, dict) and "disk" in device_map.values()) or "disk" in device_map)
        ):
            raise RuntimeError(
                "One or more modules is configured to be mapped to disk. Disk offload is not supported for models "
                "loaded from GGUF files."
            )

        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
            pretrained_model_name_or_path=pretrained_model_name_or_path,
            subfolder=subfolder,
            variant=variant,
            gguf_file=gguf_file,
            from_tf=from_tf,
            from_flax=from_flax,
            use_safetensors=use_safetensors,
            cache_dir=cache_dir,
            force_download=force_download,
            proxies=proxies,
            local_files_only=local_files_only,
            token=token,
            user_agent=user_agent,
            revision=revision,
            commit_hash=commit_hash,
            is_remote_code=cls._auto_class is not None,
            transformers_explicit_filename=transformers_explicit_filename,
        )

        is_sharded = sharded_metadata is not None
        is_quantized = hf_quantizer is not None
        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None

        if is_from_file and not is_sharded and checkpoint_files[0].endswith(".safetensors"):
            with safe_open(checkpoint_files[0], framework="pt") as f:
                metadata = f.metadata()

            if metadata is None:
                # Assume it's a pytorch checkpoint (introduced for timm checkpoints)
                pass
            elif metadata.get("format") == "pt":
                pass
            elif metadata.get("format") == "tf":
                from_tf = True
                logger.info("A TensorFlow safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "flax":
                from_flax = True
                logger.info("A Flax safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "mlx":
                # This is a mlx file, we assume weights are compatible with pt
                pass
            else:
                raise ValueError(
                    f"Incompatible safetensors file. File metadata is not ['pt', 'tf', 'flax', 'mlx'] but {metadata.get('format')}"
                )

        from_pt = not (from_tf | from_flax)

        if from_pt:
            if gguf_file:
                from .modeling_gguf_pytorch_utils import load_gguf_checkpoint

                # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was
                # passed directly as a kwarg from now on
                with torch.device("meta"):
                    dummy_model = cls(config)
                state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[
                    "tensors"
                ]

            # Find the correct dtype based on current state
            config, dtype, dtype_orig = _get_dtype(
                cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only
            )

        config.name_or_path = pretrained_model_name_or_path
        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)
        config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.
        with ContextManagers(model_init_context):
            # Let's make sure we don't run the init function of buffer modules
>           model = cls(config, *model_args, **model_kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'

.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: TypeError
----------------------------- Captured stderr call -----------------------------
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  60%|    | 3/5 [00:00<00:00, 24.54it/s]Loading pipeline components...:  60%|    | 3/5 [00:00<00:00, 23.83it/s]
_______________ TestHotSwapping.test_hotswap_works[True-config0] _______________
[gw1] linux -- Python 3.11.0 /app/.venv/bin/python

self = <tests.test_initialization.TestHotSwapping object at 0x7fbc3b1d30d0>
config = LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.17.2.dev0@UNKNOWN', b...time_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None)
do_compile = True
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/popen-gw1/test_hotswap_works_True_config0')

    @pytest.mark.parametrize(
        "config",
        [
            LoraConfig(init_lora_weights=0, target_modules=["lin0"]),
            LoraConfig(init_lora_weights=0, target_modules=["lin0", "lin1"]),
        ],
    )
    @pytest.mark.parametrize("do_compile", [False, True])
    def test_hotswap_works(self, config, do_compile, tmp_path):
        # Load 2 different adapters and check that we can hotswap between them, with the model optionally being
        # compiled.
        atol, rtol = 1e-4, 1e-4
        inputs = torch.rand(3, 10).to(self.torch_device)

        # create adapter 0
        model = self.get_model()
        torch.manual_seed(0)
        model = get_peft_model(model, config)
        model = self.compile(model, do_compile=do_compile)
        model.eval()
        with torch.inference_mode():
>           output0 = model(inputs)
                      ^^^^^^^^^^^^^

tests/test_initialization.py:3494:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:375: in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:749: in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:923: in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:907: in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1578: in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1456: in codegen_and_compile
    compiled_module = graph.compile_to_module()
                      ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2293: in compile_to_module
    return self._compile_to_module()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2299: in _compile_to_module
    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
                                                             ^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2238: in codegen
    self.scheduler.codegen()
.venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py:4598: in codegen
    else self._codegen(self.nodes)
         ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py:4750: in _codegen
    self.get_backend(device).codegen_node(node)
.venv/lib/python3.11/site-packages/torch/_inductor/codegen/cpp.py:5076: in codegen_node
    cpp_kernel_proxy = self.kernel_proxy_cls(kernel_group)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/codegen/cpp.py:3816: in __init__
    self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:432: in pick_vec_isa
    _valid_vec_isa_list: list[VecISA] = valid_vec_isa_list()
                                        ^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:419: in valid_vec_isa_list
    isa_list.extend(
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:419: in <genexpr>
    isa_list.extend(
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:147: in __bool__
    return self.__bool__impl(config.cpp.vec_isa_ok)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:157: in __bool__impl
    return self.check_build(VecISA._avx_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:102: in check_build
    extra=_get_isa_dry_compile_fingerprint(self._arch_flags),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:28: in _get_isa_dry_compile_fingerprint
    compiler_info = get_compiler_version_info(get_cpp_compiler())
                                              ^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:155: in get_cpp_compiler
    compiler = cpp_compiler_search(search)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

search = (None, 'g++')

    @functools.lru_cache(1)
    def cpp_compiler_search(search: str) -> str:
        from torch._inductor.codecache import get_lock_dir, LOCK_TIMEOUT

        for cxx in search:
            try:
                if cxx is None:
                    # gxx package is only available for Linux
                    # according to https://anaconda.org/conda-forge/gxx/
                    if sys.platform != "linux":
                        continue
                    # Do not install GXX by default
                    if not os.getenv("TORCH_INDUCTOR_INSTALL_GXX"):
                        continue
                    from torch.utils._filelock import FileLock

                    lock_dir = get_lock_dir()
                    lock = FileLock(
                        os.path.join(lock_dir, "g++.lock"), timeout=LOCK_TIMEOUT
                    )
                    with lock:
                        cxx = install_gcc_via_conda()
                subprocess.check_output([cxx, "--version"])
                return cxx
            except (subprocess.SubprocessError, FileNotFoundError, ImportError):
                continue
>       raise exc.InvalidCxxCompiler
E       torch._inductor.exc.InductorError: InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')
E
E       Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

.venv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:101: InductorError
_______________ TestHotSwapping.test_hotswap_works[True-config1] _______________
[gw1] linux -- Python 3.11.0 /app/.venv/bin/python

self = <tests.test_initialization.TestHotSwapping object at 0x7fbc3b1d0d50>
config = LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.17.2.dev0@UNKNOWN', b...time_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None)
do_compile = True
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/popen-gw1/test_hotswap_works_True_config1')

    @pytest.mark.parametrize(
        "config",
        [
            LoraConfig(init_lora_weights=0, target_modules=["lin0"]),
            LoraConfig(init_lora_weights=0, target_modules=["lin0", "lin1"]),
        ],
    )
    @pytest.mark.parametrize("do_compile", [False, True])
    def test_hotswap_works(self, config, do_compile, tmp_path):
        # Load 2 different adapters and check that we can hotswap between them, with the model optionally being
        # compiled.
        atol, rtol = 1e-4, 1e-4
        inputs = torch.rand(3, 10).to(self.torch_device)

        # create adapter 0
        model = self.get_model()
        torch.manual_seed(0)
        model = get_peft_model(model, config)
        model = self.compile(model, do_compile=do_compile)
        model.eval()
        with torch.inference_mode():
>           output0 = model(inputs)
                      ^^^^^^^^^^^^^

tests/test_initialization.py:3494:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:375: in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:749: in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:923: in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:907: in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1578: in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1456: in codegen_and_compile
    compiled_module = graph.compile_to_module()
                      ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2293: in compile_to_module
    return self._compile_to_module()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2299: in _compile_to_module
    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
                                                             ^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2238: in codegen
    self.scheduler.codegen()
.venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py:4598: in codegen
    else self._codegen(self.nodes)
         ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py:4750: in _codegen
    self.get_backend(device).codegen_node(node)
.venv/lib/python3.11/site-packages/torch/_inductor/codegen/cpp.py:5076: in codegen_node
    cpp_kernel_proxy = self.kernel_proxy_cls(kernel_group)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/codegen/cpp.py:3816: in __init__
    self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:432: in pick_vec_isa
    _valid_vec_isa_list: list[VecISA] = valid_vec_isa_list()
                                        ^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:419: in valid_vec_isa_list
    isa_list.extend(
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:419: in <genexpr>
    isa_list.extend(
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:147: in __bool__
    return self.__bool__impl(config.cpp.vec_isa_ok)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:157: in __bool__impl
    return self.check_build(VecISA._avx_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:102: in check_build
    extra=_get_isa_dry_compile_fingerprint(self._arch_flags),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpu_vec_isa.py:28: in _get_isa_dry_compile_fingerprint
    compiler_info = get_compiler_version_info(get_cpp_compiler())
                                              ^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:155: in get_cpp_compiler
    compiler = cpp_compiler_search(search)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

search = (None, 'g++')

    @functools.lru_cache(1)
    def cpp_compiler_search(search: str) -> str:
        from torch._inductor.codecache import get_lock_dir, LOCK_TIMEOUT

        for cxx in search:
            try:
                if cxx is None:
                    # gxx package is only available for Linux
                    # according to https://anaconda.org/conda-forge/gxx/
                    if sys.platform != "linux":
                        continue
                    # Do not install GXX by default
                    if not os.getenv("TORCH_INDUCTOR_INSTALL_GXX"):
                        continue
                    from torch.utils._filelock import FileLock

                    lock_dir = get_lock_dir()
                    lock = FileLock(
                        os.path.join(lock_dir, "g++.lock"), timeout=LOCK_TIMEOUT
                    )
                    with lock:
                        cxx = install_gcc_via_conda()
                subprocess.check_output([cxx, "--version"])
                return cxx
            except (subprocess.SubprocessError, FileNotFoundError, ImportError):
                continue
>       raise exc.InvalidCxxCompiler
E       torch._inductor.exc.InductorError: InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')
E
E       Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

.venv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:101: InductorError
____ PeftCustomKwargsTester.test_maybe_include_all_linear_layers_diffusion _____
[gw0] linux -- Python 3.11.0 /app/.venv/bin/python

self = <tests.test_tuners_utils.PeftCustomKwargsTester testMethod=test_maybe_include_all_linear_layers_diffusion>

    def test_maybe_include_all_linear_layers_diffusion(self):
        model_id = "hf-internal-testing/tiny-sd-pipe"
        with hub_online_once(model_id):
>           model = StableDiffusionPipeline.from_pretrained(model_id)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_tuners_utils.py:347:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'transformers.models.clip.modeling_clip.CLIPTextModel'>
pretrained_model_name_or_path = '/root/.cache/huggingface/hub/models--hf-internal-testing--tiny-sd-pipe/snapshots/39f7e1819d772fbb8dd7f2aa3da8e2fc5a9bd917/text_encoder'
config = CLIPTextConfig {
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dty...en_layers": 5,
  "pad_token_id": 1,
  "projection_dim": 32,
  "transformers_version": "4.57.0",
  "vocab_size": 1000
}

cache_dir = None, ignore_mismatched_sizes = False, force_download = False
local_files_only = False, token = None, revision = 'main'
use_safetensors = None, weights_only = True, model_args = ()
kwargs = {'offload_state_dict': None}, state_dict = None

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        **kwargs,
    ) -> SpecificPreTrainedModelType:
        r"""
        Instantiate a pretrained pytorch model from a pre-trained model configuration.

        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you should first set it back in training mode with `model.train()`.

        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come
        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
        task.

        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those
        weights are discarded.

        Parameters:
            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):
                Can be either:

                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
                    - A path to a *directory* containing model weights saved using
                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In
                      this case, `from_tf` should be set to `True` and a configuration object should be provided as
                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a
                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.
                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,
                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to
                      `True`.
                    - `None` if you are both providing the configuration and state dictionary (resp. with keyword
                      arguments `config` and `state_dict`).
            model_args (sequence of positional arguments, *optional*):
                All remaining positional arguments will be passed to the underlying model's `__init__` method.
            config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):
                Can be either:

                    - an instance of a class derived from [`PretrainedConfig`],
                    - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].

                Configuration for the model to use instead of an automatically loaded configuration. Configuration can
                be automatically loaded when:

                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained
                      model).
                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the
                      save directory.
                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a
                      configuration JSON file named *config.json* is found in the directory.
            state_dict (`dict[str, torch.Tensor]`, *optional*):
                A state dictionary to use instead of a state dictionary loaded from saved weights file.

                This option can be used if you want to create a model from a pretrained configuration but load your own
                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and
                [`~PreTrainedModel.from_pretrained`] is not a simpler option.
            cache_dir (`Union[str, os.PathLike]`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the
                standard cache should not be used.
            from_tf (`bool`, *optional*, defaults to `False`):
                Load the model weights from a TensorFlow checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            from_flax (`bool`, *optional*, defaults to `False`):
                Load the model weights from a Flax checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):
                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size
                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a
                checkpoint with 3 labels).
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
                cached versions if they exist.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
            output_loading_info(`bool`, *optional*, defaults to `False`):
                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.
            local_files_only(`bool`, *optional*, defaults to `False`):
                Whether or not to only look at local files (i.e., do not try to download the model).
            token (`str` or `bool`, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use
                the token generated when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.

                <Tip>

                To test a pull request you made on the Hub, you can pass `revision="refs/pr/<pr_number>"`.

                </Tip>
            attn_implementation (`str`, *optional*):
                The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)), or `"flash_attention_3"` (using [Dao-AILab/flash-attention/hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.

                Accept HF kernel references in the form:
                  <namespace>/<repo_name>[@<revision>][:<kernel_name>]

                - <namespace> and <repo_name> are any non-"/" and non-":" sequences.
                - "@<revision>" is optional (branch, tag, or commit-ish), e.g. "@main", "@v1.2.0", "@abc123".
                - ":<kernel_name>" is optional and selects a function inside the kernel repo.
                - Both options can appear together and in this order only: @revision first, then :kernel_name.
                - We intentionally allow a leading "<wrapper>|" prefix (e.g., "flash|...") because the code
                  strips it before loading; '|' is not excluded in the character classes here.

                Examples that match:
                  "org/model"
                  "org/model@main"
                  "org/model:custom_kernel"
                  "org/model@v1.2.3:custom_kernel"

            > Parameters for big model inference

            dtype (`str` or `torch.dtype`, *optional*):
                Override the default `torch_dtype` and load the model under a specific `dtype`. The different options
                are:

                1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified
                  `dtype`, ignoring the model's `config.dtype` if one exists. If not specified
                  - the model will get loaded in `torch.float` (fp32).

                2. `"auto"` - A `dtype` or `torch_dtype` entry in the `config.json` file of the model will be
                  attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in
                  the checkpoint that's of a floating point type and use that as `dtype`. This will load the model
                  using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how
                  the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.

                3. A string that is a valid `torch.dtype`. E.g. "float32" loads the model in `torch.float32`, "float16" loads in `torch.float16` etc.

                <Tip>

                For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or
                reach out to the authors and ask them to add this information to the model's card and to insert the
                `dtype` or `torch_dtype` entry in `config.json` on the hub.

                </Tip>

            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):
                A map that specifies where each submodule should go. It doesn't need to be refined to each
                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
                same device. If we only pass the device (*e.g.*, `"cpu"`, `"cuda:1"`, `"mps"`, or a GPU ordinal rank
                like `1`) on which the model will be allocated, the device map will map the entire model to this
                device. Passing `device_map = 0` means put the whole model on GPU 0.

                To have Accelerate compute the most optimized `device_map` automatically, set `device_map="auto"`. For
                more information about each option see [designing a device
                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).
            max_memory (`Dict`, *optional*):
                A dictionary device identifier to maximum memory if using `device_map`. Will default to the maximum memory available for each
                GPU and the available CPU RAM if unset.
            tp_plan (`str`, *optional*):
                A torch tensor parallel plan, see [here](https://pytorch.org/tutorials/intermediate/TP_tutorial.html). Currently, it only accepts
                `tp_plan="auto"` to use predefined plan based on the model. Note that if you use it, you should launch your script accordingly with
                `torchrun [args] script.py`. This will be much faster than using a `device_map`, but has limitations.
            tp_size (`str`, *optional*):
                A torch tensor parallel degree. If not provided would default to world size.
            device_mesh (`torch.distributed.DeviceMesh`, *optional*):
                A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.
                If provided, it has to contain dimension named `"tp"` in case it's > 1 dimensional, this dimension will be used for tensor parallelism
            offload_folder (`str` or `os.PathLike`, *optional*):
                If the `device_map` contains any value `"disk"`, the folder where we will offload weights.
            offload_buffers (`bool`, *optional*):
                Whether or not to offload the buffers with the model parameters.
            quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):
                A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g
                bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and
                `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes
                quantizations and not preferred. consider inserting all such arguments into quantization_config
                instead.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            variant (`str`, *optional*):
                If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is
                ignored when using `from_tf` or `from_flax`.
            use_safetensors (`bool`, *optional*, defaults to `None`):
                Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`
                is not installed, it will be set to `False`.
            weights_only (`bool`, *optional*, defaults to `True`):
                Indicates whether unpickler should be restricted to loading only tensors, primitive types,
                dictionaries and any types added via torch.serialization.add_safe_globals().
                When set to False, we can load wrapper tensor subclass weights.
            key_mapping (`dict[str, str], *optional*):
                A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers
                architecture, but was not converted accordingly.
            kwargs (remaining dictionary of keyword arguments, *optional*):
                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or
                automatically loaded:

                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the
                      underlying model's `__init__` method (we assume all relevant updates to the configuration have
                      already been done)
                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class
                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that
                      corresponds to a configuration attribute will be used to override said attribute with the
                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute
                      will be passed to the underlying model's `__init__` function.

        <Tip>

        Activate the special ["offline-mode"](https://huggingface.co/transformers/installation.html#offline-mode) to
        use this method in a firewalled environment.

        </Tip>

        Examples:

        ```python
        >>> from transformers import BertConfig, BertModel

        >>> # Download model and configuration from huggingface.co and cache.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased")
        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
        >>> model = BertModel.from_pretrained("./test/saved_model/")
        >>> # Update configuration during loading.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", output_attentions=True)
        >>> assert model.config.output_attentions == True
        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).
        >>> config = BertConfig.from_json_file("./tf_model/my_tf_model_config.json")
        >>> model = BertModel.from_pretrained("./tf_model/my_tf_checkpoint.ckpt.index", from_tf=True, config=config)
        >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", from_flax=True)
        ```
        """
        state_dict = kwargs.pop("state_dict", None)
        from_tf = kwargs.pop("from_tf", False)
        from_flax = kwargs.pop("from_flax", False)
        proxies = kwargs.pop("proxies", None)
        output_loading_info = kwargs.pop("output_loading_info", False)
        use_auth_token = kwargs.pop("use_auth_token", None)
        from_pipeline = kwargs.pop("_from_pipeline", None)
        from_auto_class = kwargs.pop("_from_auto", False)
        dtype = kwargs.pop("dtype", None)
        torch_dtype = kwargs.pop("torch_dtype", None)  # kept for BC
        device_map = kwargs.pop("device_map", None)
        max_memory = kwargs.pop("max_memory", None)
        offload_folder = kwargs.pop("offload_folder", None)
        offload_buffers = kwargs.pop("offload_buffers", False)
        load_in_8bit = kwargs.pop("load_in_8bit", False)
        load_in_4bit = kwargs.pop("load_in_4bit", False)
        quantization_config = kwargs.pop("quantization_config", None)
        subfolder = kwargs.pop("subfolder", "")
        commit_hash = kwargs.pop("_commit_hash", None)
        variant = kwargs.pop("variant", None)
        adapter_kwargs = kwargs.pop("adapter_kwargs", {})
        adapter_name = kwargs.pop("adapter_name", "default")
        generation_config = kwargs.pop("generation_config", None)
        gguf_file = kwargs.pop("gguf_file", None)
        tp_plan = kwargs.pop("tp_plan", None)
        tp_size = kwargs.pop("tp_size", None)
        distributed_config: DistributedConfig = kwargs.pop("distributed_config", None)
        device_mesh = kwargs.pop("device_mesh", None)
        trust_remote_code = kwargs.pop("trust_remote_code", None)
        use_kernels = kwargs.pop("use_kernels", False)

        key_mapping = kwargs.pop("key_mapping", None)
        # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model
        if key_mapping is None and any(
            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS
        ):
            key_mapping = cls._checkpoint_conversion_mapping

        if distributed_config is not None:
            tp_plan = "auto"

        # Not used anymore -- remove them from the kwargs
        _ = kwargs.pop("resume_download", None)
        _ = kwargs.pop("mirror", None)
        _ = kwargs.pop("_fast_init", True)
        _ = kwargs.pop("low_cpu_mem_usage", None)

        # For BC on torch_dtype argument
        if torch_dtype is not None:
            logger.warning_once("`torch_dtype` is deprecated! Use `dtype` instead!")
            # If both kwargs are provided, use `dtype`
            dtype = dtype if dtype is not None else torch_dtype

        if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):
            raise ValueError(
                "`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies."
            )
        if tp_size is not None and tp_plan is None:
            raise ValueError("tp_plan has to be set when tp_size is passed.")
        if tp_plan is not None and tp_plan != "auto":
            # TODO: we can relax this check when we support taking tp_plan from a json file, for example.
            raise ValueError(f"tp_plan supports 'auto' only for now but got {tp_plan}.")
        if tp_plan is not None and device_map is not None:
            raise ValueError(
                "`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization."
            )

        if device_map == "auto" and int(os.environ.get("WORLD_SIZE", "0")):
            logger.info(
                "You've set device_map=`auto` while triggering a distributed run with torchrun. This might lead to unexpected behavior. "
                "If your plan is to load the model on each device, you should set device_map={"
                ": PartialState().process_index} where PartialState comes from accelerate library"
            )

        # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple
        # `device_map` pointing to the correct device
        if tp_plan is not None:
            if device_mesh is None:
                tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(tp_plan, tp_size=tp_size)
            else:
                if device_mesh.ndim > 1:
                    if "tp" not in device_mesh.mesh_dim_names:
                        raise ValueError(
                            "When using `tp_plan` and n-d `device_mesh`, it must contain a 'tp' dimension. "
                            "Please provide a valid `device_mesh`."
                        )
                    device_mesh = device_mesh["tp"]
                tp_size = device_mesh.size()
                device_map = torch.device(f"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}")

            if tp_size is None:
                tp_size = torch.distributed.get_world_size()

        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError(
                    "`token` and `use_auth_token` are both specified. Please set only the argument `token`."
                )
            token = use_auth_token

        if token is not None and adapter_kwargs is not None and "token" not in adapter_kwargs:
            adapter_kwargs["token"] = token

        if gguf_file is not None and not is_accelerate_available():
            raise ValueError("accelerate is required when loading a GGUF file `pip install accelerate`.")

        if commit_hash is None:
            if not isinstance(config, PretrainedConfig):
                # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible
                resolved_config_file = cached_file(
                    pretrained_model_name_or_path,
                    CONFIG_NAME,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    token=token,
                    revision=revision,
                    subfolder=subfolder,
                    _raise_exceptions_for_gated_repo=False,
                    _raise_exceptions_for_missing_entries=False,
                    _raise_exceptions_for_connection_errors=False,
                )
                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)
            else:
                commit_hash = getattr(config, "_commit_hash", None)

        if is_peft_available():
            _adapter_model_path = adapter_kwargs.pop("_adapter_model_path", None)

            if _adapter_model_path is None:
                _adapter_model_path = find_adapter_config_file(
                    pretrained_model_name_or_path,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    _commit_hash=commit_hash,
                    **adapter_kwargs,
                )
            if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):
                with open(_adapter_model_path, "r", encoding="utf-8") as f:
                    _adapter_model_path = pretrained_model_name_or_path
                    pretrained_model_name_or_path = json.load(f)["base_model_name_or_path"]
        else:
            _adapter_model_path = None

        # Potentially detect context manager or global device, and use it (only if no device_map was provided)
        if device_map is None and not is_deepspeed_zero3_enabled():
            device_in_context = get_torch_context_manager_or_global_device()
            if device_in_context == torch.device("meta"):
                raise RuntimeError(
                    "You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\n"
                    "This is an anti-pattern as `from_pretrained` wants to load existing weights.\nIf you want to initialize an "
                    "empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`"
                )
            device_map = device_in_context

        # change device_map into a map if we passed an int, a str or a torch.device
        if isinstance(device_map, torch.device):
            device_map = {"": device_map}
        elif isinstance(device_map, str) and device_map not in ["auto", "balanced", "balanced_low_0", "sequential"]:
            try:
                device_map = {"": torch.device(device_map)}
            except RuntimeError:
                raise ValueError(
                    "When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or "
                    f"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}."
                )
        elif isinstance(device_map, int):
            if device_map < 0:
                raise ValueError(
                    "You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' "
                )
            else:
                device_map = {"": device_map}

        if device_map is not None:
            if is_deepspeed_zero3_enabled():
                raise ValueError("DeepSpeed Zero-3 is not compatible with passing a `device_map`.")
            if not is_accelerate_available():
                raise ValueError(
                    "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` "
                    "requires `accelerate`. You can install it with `pip install accelerate`"
                )

        # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.
        if load_in_4bit or load_in_8bit:
            if quantization_config is not None:
                raise ValueError(
                    "You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing "
                    "`quantization_config` argument at the same time."
                )

            # preparing BitsAndBytesConfig from kwargs
            config_dict = {k: v for k, v in kwargs.items() if k in inspect.signature(BitsAndBytesConfig).parameters}
            config_dict = {**config_dict, "load_in_4bit": load_in_4bit, "load_in_8bit": load_in_8bit}
            quantization_config, kwargs = BitsAndBytesConfig.from_dict(
                config_dict=config_dict, return_unused_kwargs=True, **kwargs
            )
            logger.warning(
                "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. "
                "Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead."
            )

        from_pt = not (from_tf | from_flax)

        user_agent = {"file_type": "model", "framework": "pytorch", "from_auto_class": from_auto_class}
        if from_pipeline is not None:
            user_agent["using_pipeline"] = from_pipeline

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True

        # Load config if we don't provide a configuration
        if not isinstance(config, PretrainedConfig):
            config_path = config if config is not None else pretrained_model_name_or_path
            config, model_kwargs = cls.config_class.from_pretrained(
                config_path,
                cache_dir=cache_dir,
                return_unused_kwargs=True,
                force_download=force_download,
                proxies=proxies,
                local_files_only=local_files_only,
                token=token,
                revision=revision,
                subfolder=subfolder,
                gguf_file=gguf_file,
                _from_auto=from_auto_class,
                _from_pipeline=from_pipeline,
                **kwargs,
            )
            if "gguf_file" in model_kwargs:
                model_kwargs.pop("gguf_file")
        else:
            config = copy.deepcopy(config)
            model_kwargs = kwargs

        # Because some composite configs call super().__init__ before instantiating the sub-configs, we need this call
        # to correctly redispatch recursively if the kwarg is provided
        if "attn_implementation" in kwargs:
            config._attn_implementation = kwargs.pop("attn_implementation")

        transformers_explicit_filename = getattr(config, "transformers_weights", None)

        if transformers_explicit_filename is not None:
            if not transformers_explicit_filename.endswith(
                ".safetensors"
            ) and not transformers_explicit_filename.endswith(".safetensors.index.json"):
                raise ValueError(
                    "The transformers file in the config seems to be incorrect: it is neither a safetensors file "
                    "(*.safetensors) nor a safetensors index file (*.safetensors.index.json): "
                    f"{transformers_explicit_filename}"
                )

        hf_quantizer, config, dtype, device_map = get_hf_quantizer(
            config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent
        )

        if gguf_file is not None and hf_quantizer is not None:
            raise ValueError(
                "You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub."
            )

        if (
            gguf_file
            and device_map is not None
            and ((isinstance(device_map, dict) and "disk" in device_map.values()) or "disk" in device_map)
        ):
            raise RuntimeError(
                "One or more modules is configured to be mapped to disk. Disk offload is not supported for models "
                "loaded from GGUF files."
            )

        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
            pretrained_model_name_or_path=pretrained_model_name_or_path,
            subfolder=subfolder,
            variant=variant,
            gguf_file=gguf_file,
            from_tf=from_tf,
            from_flax=from_flax,
            use_safetensors=use_safetensors,
            cache_dir=cache_dir,
            force_download=force_download,
            proxies=proxies,
            local_files_only=local_files_only,
            token=token,
            user_agent=user_agent,
            revision=revision,
            commit_hash=commit_hash,
            is_remote_code=cls._auto_class is not None,
            transformers_explicit_filename=transformers_explicit_filename,
        )

        is_sharded = sharded_metadata is not None
        is_quantized = hf_quantizer is not None
        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None

        if is_from_file and not is_sharded and checkpoint_files[0].endswith(".safetensors"):
            with safe_open(checkpoint_files[0], framework="pt") as f:
                metadata = f.metadata()

            if metadata is None:
                # Assume it's a pytorch checkpoint (introduced for timm checkpoints)
                pass
            elif metadata.get("format") == "pt":
                pass
            elif metadata.get("format") == "tf":
                from_tf = True
                logger.info("A TensorFlow safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "flax":
                from_flax = True
                logger.info("A Flax safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "mlx":
                # This is a mlx file, we assume weights are compatible with pt
                pass
            else:
                raise ValueError(
                    f"Incompatible safetensors file. File metadata is not ['pt', 'tf', 'flax', 'mlx'] but {metadata.get('format')}"
                )

        from_pt = not (from_tf | from_flax)

        if from_pt:
            if gguf_file:
                from .modeling_gguf_pytorch_utils import load_gguf_checkpoint

                # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was
                # passed directly as a kwarg from now on
                with torch.device("meta"):
                    dummy_model = cls(config)
                state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[
                    "tensors"
                ]

            # Find the correct dtype based on current state
            config, dtype, dtype_orig = _get_dtype(
                cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only
            )

        config.name_or_path = pretrained_model_name_or_path
        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)
        config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.
        with ContextManagers(model_init_context):
            # Let's make sure we don't run the init function of buffer modules
>           model = cls(config, *model_args, **model_kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'

.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: TypeError
----------------------------- Captured stderr call -----------------------------
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  40%|       | 2/5 [00:00<00:00, 46.85it/s]
____ TestBaseModelRevision.test_load_different_peft_and_base_model_revision ____
[gw1] linux -- Python 3.11.0 /app/.venv/bin/python

response = <Response [429]>, endpoint_name = None

    def hf_raise_for_status(response: Response, endpoint_name: Optional[str] = None) -> None:
        """
        Internal version of `response.raise_for_status()` that will refine a
        potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.

        This helper is meant to be the unique method to raise_for_status when making a call
        to the Hugging Face Hub.


        Example:
        ```py
            import requests
            from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError

            response = get_session().post(...)
            try:
                hf_raise_for_status(response)
            except HfHubHTTPError as e:
                print(str(e)) # formatted message
                e.request_id, e.server_message # details returned by server

                # Complete the error message with additional information once it's raised
                e.append_to_message("\n`create_commit` expects the repository to exist.")
                raise
        ```

        Args:
            response (`Response`):
                Response from the server.
            endpoint_name (`str`, *optional*):
                Name of the endpoint that has been called. If provided, the error message
                will be more complete.

        <Tip warning={true}>

        Raises when the request has failed:

            - [`~utils.RepositoryNotFoundError`]
                If the repository to download from cannot be found. This may be because it
                doesn't exist, because `repo_type` is not set correctly, or because the repo
                is `private` and you do not have access.
            - [`~utils.GatedRepoError`]
                If the repository exists but is gated and the user is not on the authorized
                list.
            - [`~utils.RevisionNotFoundError`]
                If the repository exists but the revision couldn't be find.
            - [`~utils.EntryNotFoundError`]
                If the repository exists but the entry (e.g. the requested file) couldn't be
                find.
            - [`~utils.BadRequestError`]
                If request failed with a HTTP 400 BadRequest error.
            - [`~utils.HfHubHTTPError`]
                If request failed for a reason not listed above.

        </Tip>
        """
        try:
>           response.raise_for_status()

.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:407:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <Response [429]>

    def raise_for_status(self):
        """Raises :class:`HTTPError`, if one occurred."""

        http_error_msg = ""
        if isinstance(self.reason, bytes):
            # We attempt to decode utf-8 first because some servers
            # choose to localize their reason strings. If the string
            # isn't utf-8, we fall back to iso-8859-1 for all other
            # encodings. (See PR #3538)
            try:
                reason = self.reason.decode("utf-8")
            except UnicodeDecodeError:
                reason = self.reason.decode("iso-8859-1")
        else:
            reason = self.reason

        if 400 <= self.status_code < 500:
            http_error_msg = (
                f"{self.status_code} Client Error: {reason} for url: {self.url}"
            )

        elif 500 <= self.status_code < 600:
            http_error_msg = (
                f"{self.status_code} Server Error: {reason} for url: {self.url}"
            )

        if http_error_msg:
>           raise HTTPError(http_error_msg, response=self)
E           requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/peft-internal-testing/tiny-random-BertModel-lora/xet-read-token/725b8056ecf78182ad5b8fae7e2e4aa0a614da95

.venv/lib/python3.11/site-packages/requests/models.py:1026: HTTPError

The above exception was the direct cause of the following exception:

self = <tests.test_hub_features.TestBaseModelRevision object at 0x7fbc3b6fce90>
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/popen-gw1/test_load_different_peft_and_b0')

    @pytest.mark.xfail(reason="Test is flaky on CI", raises=ValueError)
    def test_load_different_peft_and_base_model_revision(self, tmp_path):
        r"""
        Test loading an AutoPeftModel from the hub where the base model revision and peft revision differ
        """
        base_model_id = "hf-internal-testing/tiny-random-BertModel"
        base_model_revision = None
        peft_model_id = "peft-internal-testing/tiny-random-BertModel-lora"
        peft_model_revision = "v1.2.3"

>       peft_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, revision=peft_model_revision).eval()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_hub_features.py:114:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src/peft/auto.py:142: in from_pretrained
    return cls._target_peft_class.from_pretrained(
src/peft/peft_model.py:554: in from_pretrained
    load_result = model.load_adapter(
src/peft/peft_model.py:1342: in load_adapter
    adapters_weights = load_peft_weights(
src/peft/utils/save_and_load.py:675: in load_peft_weights
    filename = hf_hub_download(
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1010: in hf_hub_download
    return _hf_hub_download_to_cache_dir(
.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1171: in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1723: in _download_to_tmp_and_move
    xet_get(
.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:594: in xet_get
    connection_info = refresh_xet_connection_info(file_data=xet_file_data, headers=headers)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_xet.py:116: in refresh_xet_connection_info
    return _fetch_xet_connection_info_with_url(file_data.refresh_route, headers)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_xet.py:187: in _fetch_xet_connection_info_with_url
    hf_raise_for_status(resp)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

response = <Response [429]>, endpoint_name = None

    def hf_raise_for_status(response: Response, endpoint_name: Optional[str] = None) -> None:
        """
        Internal version of `response.raise_for_status()` that will refine a
        potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.

        This helper is meant to be the unique method to raise_for_status when making a call
        to the Hugging Face Hub.


        Example:
        ```py
            import requests
            from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError

            response = get_session().post(...)
            try:
                hf_raise_for_status(response)
            except HfHubHTTPError as e:
                print(str(e)) # formatted message
                e.request_id, e.server_message # details returned by server

                # Complete the error message with additional information once it's raised
                e.append_to_message("\n`create_commit` expects the repository to exist.")
                raise
        ```

        Args:
            response (`Response`):
                Response from the server.
            endpoint_name (`str`, *optional*):
                Name of the endpoint that has been called. If provided, the error message
                will be more complete.

        <Tip warning={true}>

        Raises when the request has failed:

            - [`~utils.RepositoryNotFoundError`]
                If the repository to download from cannot be found. This may be because it
                doesn't exist, because `repo_type` is not set correctly, or because the repo
                is `private` and you do not have access.
            - [`~utils.GatedRepoError`]
                If the repository exists but is gated and the user is not on the authorized
                list.
            - [`~utils.RevisionNotFoundError`]
                If the repository exists but the revision couldn't be find.
            - [`~utils.EntryNotFoundError`]
                If the repository exists but the entry (e.g. the requested file) couldn't be
                find.
            - [`~utils.BadRequestError`]
                If request failed with a HTTP 400 BadRequest error.
            - [`~utils.HfHubHTTPError`]
                If request failed for a reason not listed above.

        </Tip>
        """
        try:
            response.raise_for_status()
        except HTTPError as e:
            error_code = response.headers.get("X-Error-Code")
            error_message = response.headers.get("X-Error-Message")

            if error_code == "RevisionNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Revision Not Found for url: {response.url}."
                raise _format(RevisionNotFoundError, message, response) from e

            elif error_code == "EntryNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Entry Not Found for url: {response.url}."
                raise _format(EntryNotFoundError, message, response) from e

            elif error_code == "GatedRepo":
                message = (
                    f"{response.status_code} Client Error." + "\n\n" + f"Cannot access gated repo for url {response.url}."
                )
                raise _format(GatedRepoError, message, response) from e

            elif error_message == "Access to this resource is disabled.":
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Cannot access repository for url {response.url}."
                    + "\n"
                    + "Access to this resource is disabled."
                )
                raise _format(DisabledRepoError, message, response) from e

            elif error_code == "RepoNotFound" or (
                response.status_code == 401
                and error_message != "Invalid credentials in Authorization header"
                and response.request is not None
                and response.request.url is not None
                and REPO_API_REGEX.search(response.request.url) is not None
            ):
                # 401 is misleading as it is returned for:
                #    - private and gated repos if user is not authenticated
                #    - missing repos
                # => for now, we process them as `RepoNotFound` anyway.
                # See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Repository Not Found for url: {response.url}."
                    + "\nPlease make sure you specified the correct `repo_id` and"
                    " `repo_type`.\nIf you are trying to access a private or gated repo,"
                    " make sure you are authenticated. For more details, see"
                    " https://huggingface.co/docs/huggingface_hub/authentication"
                )
                raise _format(RepositoryNotFoundError, message, response) from e

            elif response.status_code == 400:
                message = (
                    f"\n\nBad request for {endpoint_name} endpoint:" if endpoint_name is not None else "\n\nBad request:"
                )
                raise _format(BadRequestError, message, response) from e

            elif response.status_code == 403:
                message = (
                    f"\n\n{response.status_code} Forbidden: {error_message}."
                    + f"\nCannot access content at: {response.url}."
                    + "\nMake sure your token has the correct permissions."
                )
                raise _format(HfHubHTTPError, message, response) from e

            elif response.status_code == 416:
                range_header = response.request.headers.get("Range")
                message = f"{e}. Requested range: {range_header}. Content-Range: {response.headers.get('Content-Range')}."
                raise _format(HfHubHTTPError, message, response) from e

            # Convert `HTTPError` into a `HfHubHTTPError` to display request information
            # as well (request id and/or server error message)
>           raise _format(HfHubHTTPError, str(e), response) from e
E           huggingface_hub.errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/peft-internal-testing/tiny-random-BertModel-lora/xet-read-token/725b8056ecf78182ad5b8fae7e2e4aa0a614da95 (Request ID: Root=1-68e58b20-47b363ea5bbb121b7b6fae48;c5e716f6-8b3d-4a1c-aa96-1714343a50c7)
E
E           We had to rate limit your IP (169.237.118.139). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API.

.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:480: HfHubHTTPError
----------------------------- Captured stderr call -----------------------------
If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`
Some weights of BertLMHeadModel were not initialized from the model checkpoint at hf-internal-testing/tiny-random-BertModel and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
_________________ TestScalingAdapters.test_diffusers_pipeline __________________
[gw0] linux -- Python 3.11.0 /app/.venv/bin/python

self = <tests.test_helpers.TestScalingAdapters object at 0x7f72b8b8d010>

    def test_diffusers_pipeline(self):
        model_id = "hf-internal-testing/tiny-sd-pipe"
>       pipeline = StableDiffusionPipeline.from_pretrained(model_id)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_helpers.py:185:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1025: in from_pretrained
    loaded_sub_model = load_sub_model(
.venv/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849: in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'transformers.models.clip.modeling_clip.CLIPTextModel'>
pretrained_model_name_or_path = '/root/.cache/huggingface/hub/models--hf-internal-testing--tiny-sd-pipe/snapshots/39f7e1819d772fbb8dd7f2aa3da8e2fc5a9bd917/text_encoder'
config = CLIPTextConfig {
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dty...en_layers": 5,
  "pad_token_id": 1,
  "projection_dim": 32,
  "transformers_version": "4.57.0",
  "vocab_size": 1000
}

cache_dir = None, ignore_mismatched_sizes = False, force_download = False
local_files_only = False, token = None, revision = 'main'
use_safetensors = None, weights_only = True, model_args = ()
kwargs = {'offload_state_dict': None}, state_dict = None

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        **kwargs,
    ) -> SpecificPreTrainedModelType:
        r"""
        Instantiate a pretrained pytorch model from a pre-trained model configuration.

        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you should first set it back in training mode with `model.train()`.

        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come
        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
        task.

        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those
        weights are discarded.

        Parameters:
            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):
                Can be either:

                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
                    - A path to a *directory* containing model weights saved using
                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In
                      this case, `from_tf` should be set to `True` and a configuration object should be provided as
                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a
                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.
                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,
                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to
                      `True`.
                    - `None` if you are both providing the configuration and state dictionary (resp. with keyword
                      arguments `config` and `state_dict`).
            model_args (sequence of positional arguments, *optional*):
                All remaining positional arguments will be passed to the underlying model's `__init__` method.
            config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):
                Can be either:

                    - an instance of a class derived from [`PretrainedConfig`],
                    - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].

                Configuration for the model to use instead of an automatically loaded configuration. Configuration can
                be automatically loaded when:

                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained
                      model).
                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the
                      save directory.
                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a
                      configuration JSON file named *config.json* is found in the directory.
            state_dict (`dict[str, torch.Tensor]`, *optional*):
                A state dictionary to use instead of a state dictionary loaded from saved weights file.

                This option can be used if you want to create a model from a pretrained configuration but load your own
                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and
                [`~PreTrainedModel.from_pretrained`] is not a simpler option.
            cache_dir (`Union[str, os.PathLike]`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the
                standard cache should not be used.
            from_tf (`bool`, *optional*, defaults to `False`):
                Load the model weights from a TensorFlow checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            from_flax (`bool`, *optional*, defaults to `False`):
                Load the model weights from a Flax checkpoint save file (see docstring of
                `pretrained_model_name_or_path` argument).
            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):
                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size
                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a
                checkpoint with 3 labels).
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
                cached versions if they exist.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
            output_loading_info(`bool`, *optional*, defaults to `False`):
                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.
            local_files_only(`bool`, *optional*, defaults to `False`):
                Whether or not to only look at local files (i.e., do not try to download the model).
            token (`str` or `bool`, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use
                the token generated when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.

                <Tip>

                To test a pull request you made on the Hub, you can pass `revision="refs/pr/<pr_number>"`.

                </Tip>
            attn_implementation (`str`, *optional*):
                The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)), or `"flash_attention_3"` (using [Dao-AILab/flash-attention/hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.

                Accept HF kernel references in the form:
                  <namespace>/<repo_name>[@<revision>][:<kernel_name>]

                - <namespace> and <repo_name> are any non-"/" and non-":" sequences.
                - "@<revision>" is optional (branch, tag, or commit-ish), e.g. "@main", "@v1.2.0", "@abc123".
                - ":<kernel_name>" is optional and selects a function inside the kernel repo.
                - Both options can appear together and in this order only: @revision first, then :kernel_name.
                - We intentionally allow a leading "<wrapper>|" prefix (e.g., "flash|...") because the code
                  strips it before loading; '|' is not excluded in the character classes here.

                Examples that match:
                  "org/model"
                  "org/model@main"
                  "org/model:custom_kernel"
                  "org/model@v1.2.3:custom_kernel"

            > Parameters for big model inference

            dtype (`str` or `torch.dtype`, *optional*):
                Override the default `torch_dtype` and load the model under a specific `dtype`. The different options
                are:

                1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified
                  `dtype`, ignoring the model's `config.dtype` if one exists. If not specified
                  - the model will get loaded in `torch.float` (fp32).

                2. `"auto"` - A `dtype` or `torch_dtype` entry in the `config.json` file of the model will be
                  attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in
                  the checkpoint that's of a floating point type and use that as `dtype`. This will load the model
                  using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how
                  the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.

                3. A string that is a valid `torch.dtype`. E.g. "float32" loads the model in `torch.float32`, "float16" loads in `torch.float16` etc.

                <Tip>

                For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or
                reach out to the authors and ask them to add this information to the model's card and to insert the
                `dtype` or `torch_dtype` entry in `config.json` on the hub.

                </Tip>

            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):
                A map that specifies where each submodule should go. It doesn't need to be refined to each
                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
                same device. If we only pass the device (*e.g.*, `"cpu"`, `"cuda:1"`, `"mps"`, or a GPU ordinal rank
                like `1`) on which the model will be allocated, the device map will map the entire model to this
                device. Passing `device_map = 0` means put the whole model on GPU 0.

                To have Accelerate compute the most optimized `device_map` automatically, set `device_map="auto"`. For
                more information about each option see [designing a device
                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).
            max_memory (`Dict`, *optional*):
                A dictionary device identifier to maximum memory if using `device_map`. Will default to the maximum memory available for each
                GPU and the available CPU RAM if unset.
            tp_plan (`str`, *optional*):
                A torch tensor parallel plan, see [here](https://pytorch.org/tutorials/intermediate/TP_tutorial.html). Currently, it only accepts
                `tp_plan="auto"` to use predefined plan based on the model. Note that if you use it, you should launch your script accordingly with
                `torchrun [args] script.py`. This will be much faster than using a `device_map`, but has limitations.
            tp_size (`str`, *optional*):
                A torch tensor parallel degree. If not provided would default to world size.
            device_mesh (`torch.distributed.DeviceMesh`, *optional*):
                A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.
                If provided, it has to contain dimension named `"tp"` in case it's > 1 dimensional, this dimension will be used for tensor parallelism
            offload_folder (`str` or `os.PathLike`, *optional*):
                If the `device_map` contains any value `"disk"`, the folder where we will offload weights.
            offload_buffers (`bool`, *optional*):
                Whether or not to offload the buffers with the model parameters.
            quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):
                A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g
                bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and
                `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes
                quantizations and not preferred. consider inserting all such arguments into quantization_config
                instead.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            variant (`str`, *optional*):
                If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is
                ignored when using `from_tf` or `from_flax`.
            use_safetensors (`bool`, *optional*, defaults to `None`):
                Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`
                is not installed, it will be set to `False`.
            weights_only (`bool`, *optional*, defaults to `True`):
                Indicates whether unpickler should be restricted to loading only tensors, primitive types,
                dictionaries and any types added via torch.serialization.add_safe_globals().
                When set to False, we can load wrapper tensor subclass weights.
            key_mapping (`dict[str, str], *optional*):
                A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers
                architecture, but was not converted accordingly.
            kwargs (remaining dictionary of keyword arguments, *optional*):
                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or
                automatically loaded:

                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the
                      underlying model's `__init__` method (we assume all relevant updates to the configuration have
                      already been done)
                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class
                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that
                      corresponds to a configuration attribute will be used to override said attribute with the
                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute
                      will be passed to the underlying model's `__init__` function.

        <Tip>

        Activate the special ["offline-mode"](https://huggingface.co/transformers/installation.html#offline-mode) to
        use this method in a firewalled environment.

        </Tip>

        Examples:

        ```python
        >>> from transformers import BertConfig, BertModel

        >>> # Download model and configuration from huggingface.co and cache.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased")
        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
        >>> model = BertModel.from_pretrained("./test/saved_model/")
        >>> # Update configuration during loading.
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", output_attentions=True)
        >>> assert model.config.output_attentions == True
        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).
        >>> config = BertConfig.from_json_file("./tf_model/my_tf_model_config.json")
        >>> model = BertModel.from_pretrained("./tf_model/my_tf_checkpoint.ckpt.index", from_tf=True, config=config)
        >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)
        >>> model = BertModel.from_pretrained("google-bert/bert-base-uncased", from_flax=True)
        ```
        """
        state_dict = kwargs.pop("state_dict", None)
        from_tf = kwargs.pop("from_tf", False)
        from_flax = kwargs.pop("from_flax", False)
        proxies = kwargs.pop("proxies", None)
        output_loading_info = kwargs.pop("output_loading_info", False)
        use_auth_token = kwargs.pop("use_auth_token", None)
        from_pipeline = kwargs.pop("_from_pipeline", None)
        from_auto_class = kwargs.pop("_from_auto", False)
        dtype = kwargs.pop("dtype", None)
        torch_dtype = kwargs.pop("torch_dtype", None)  # kept for BC
        device_map = kwargs.pop("device_map", None)
        max_memory = kwargs.pop("max_memory", None)
        offload_folder = kwargs.pop("offload_folder", None)
        offload_buffers = kwargs.pop("offload_buffers", False)
        load_in_8bit = kwargs.pop("load_in_8bit", False)
        load_in_4bit = kwargs.pop("load_in_4bit", False)
        quantization_config = kwargs.pop("quantization_config", None)
        subfolder = kwargs.pop("subfolder", "")
        commit_hash = kwargs.pop("_commit_hash", None)
        variant = kwargs.pop("variant", None)
        adapter_kwargs = kwargs.pop("adapter_kwargs", {})
        adapter_name = kwargs.pop("adapter_name", "default")
        generation_config = kwargs.pop("generation_config", None)
        gguf_file = kwargs.pop("gguf_file", None)
        tp_plan = kwargs.pop("tp_plan", None)
        tp_size = kwargs.pop("tp_size", None)
        distributed_config: DistributedConfig = kwargs.pop("distributed_config", None)
        device_mesh = kwargs.pop("device_mesh", None)
        trust_remote_code = kwargs.pop("trust_remote_code", None)
        use_kernels = kwargs.pop("use_kernels", False)

        key_mapping = kwargs.pop("key_mapping", None)
        # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model
        if key_mapping is None and any(
            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS
        ):
            key_mapping = cls._checkpoint_conversion_mapping

        if distributed_config is not None:
            tp_plan = "auto"

        # Not used anymore -- remove them from the kwargs
        _ = kwargs.pop("resume_download", None)
        _ = kwargs.pop("mirror", None)
        _ = kwargs.pop("_fast_init", True)
        _ = kwargs.pop("low_cpu_mem_usage", None)

        # For BC on torch_dtype argument
        if torch_dtype is not None:
            logger.warning_once("`torch_dtype` is deprecated! Use `dtype` instead!")
            # If both kwargs are provided, use `dtype`
            dtype = dtype if dtype is not None else torch_dtype

        if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):
            raise ValueError(
                "`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies."
            )
        if tp_size is not None and tp_plan is None:
            raise ValueError("tp_plan has to be set when tp_size is passed.")
        if tp_plan is not None and tp_plan != "auto":
            # TODO: we can relax this check when we support taking tp_plan from a json file, for example.
            raise ValueError(f"tp_plan supports 'auto' only for now but got {tp_plan}.")
        if tp_plan is not None and device_map is not None:
            raise ValueError(
                "`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization."
            )

        if device_map == "auto" and int(os.environ.get("WORLD_SIZE", "0")):
            logger.info(
                "You've set device_map=`auto` while triggering a distributed run with torchrun. This might lead to unexpected behavior. "
                "If your plan is to load the model on each device, you should set device_map={"
                ": PartialState().process_index} where PartialState comes from accelerate library"
            )

        # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple
        # `device_map` pointing to the correct device
        if tp_plan is not None:
            if device_mesh is None:
                tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(tp_plan, tp_size=tp_size)
            else:
                if device_mesh.ndim > 1:
                    if "tp" not in device_mesh.mesh_dim_names:
                        raise ValueError(
                            "When using `tp_plan` and n-d `device_mesh`, it must contain a 'tp' dimension. "
                            "Please provide a valid `device_mesh`."
                        )
                    device_mesh = device_mesh["tp"]
                tp_size = device_mesh.size()
                device_map = torch.device(f"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}")

            if tp_size is None:
                tp_size = torch.distributed.get_world_size()

        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError(
                    "`token` and `use_auth_token` are both specified. Please set only the argument `token`."
                )
            token = use_auth_token

        if token is not None and adapter_kwargs is not None and "token" not in adapter_kwargs:
            adapter_kwargs["token"] = token

        if gguf_file is not None and not is_accelerate_available():
            raise ValueError("accelerate is required when loading a GGUF file `pip install accelerate`.")

        if commit_hash is None:
            if not isinstance(config, PretrainedConfig):
                # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible
                resolved_config_file = cached_file(
                    pretrained_model_name_or_path,
                    CONFIG_NAME,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    token=token,
                    revision=revision,
                    subfolder=subfolder,
                    _raise_exceptions_for_gated_repo=False,
                    _raise_exceptions_for_missing_entries=False,
                    _raise_exceptions_for_connection_errors=False,
                )
                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)
            else:
                commit_hash = getattr(config, "_commit_hash", None)

        if is_peft_available():
            _adapter_model_path = adapter_kwargs.pop("_adapter_model_path", None)

            if _adapter_model_path is None:
                _adapter_model_path = find_adapter_config_file(
                    pretrained_model_name_or_path,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    local_files_only=local_files_only,
                    _commit_hash=commit_hash,
                    **adapter_kwargs,
                )
            if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):
                with open(_adapter_model_path, "r", encoding="utf-8") as f:
                    _adapter_model_path = pretrained_model_name_or_path
                    pretrained_model_name_or_path = json.load(f)["base_model_name_or_path"]
        else:
            _adapter_model_path = None

        # Potentially detect context manager or global device, and use it (only if no device_map was provided)
        if device_map is None and not is_deepspeed_zero3_enabled():
            device_in_context = get_torch_context_manager_or_global_device()
            if device_in_context == torch.device("meta"):
                raise RuntimeError(
                    "You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\n"
                    "This is an anti-pattern as `from_pretrained` wants to load existing weights.\nIf you want to initialize an "
                    "empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`"
                )
            device_map = device_in_context

        # change device_map into a map if we passed an int, a str or a torch.device
        if isinstance(device_map, torch.device):
            device_map = {"": device_map}
        elif isinstance(device_map, str) and device_map not in ["auto", "balanced", "balanced_low_0", "sequential"]:
            try:
                device_map = {"": torch.device(device_map)}
            except RuntimeError:
                raise ValueError(
                    "When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or "
                    f"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}."
                )
        elif isinstance(device_map, int):
            if device_map < 0:
                raise ValueError(
                    "You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' "
                )
            else:
                device_map = {"": device_map}

        if device_map is not None:
            if is_deepspeed_zero3_enabled():
                raise ValueError("DeepSpeed Zero-3 is not compatible with passing a `device_map`.")
            if not is_accelerate_available():
                raise ValueError(
                    "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` "
                    "requires `accelerate`. You can install it with `pip install accelerate`"
                )

        # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.
        if load_in_4bit or load_in_8bit:
            if quantization_config is not None:
                raise ValueError(
                    "You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing "
                    "`quantization_config` argument at the same time."
                )

            # preparing BitsAndBytesConfig from kwargs
            config_dict = {k: v for k, v in kwargs.items() if k in inspect.signature(BitsAndBytesConfig).parameters}
            config_dict = {**config_dict, "load_in_4bit": load_in_4bit, "load_in_8bit": load_in_8bit}
            quantization_config, kwargs = BitsAndBytesConfig.from_dict(
                config_dict=config_dict, return_unused_kwargs=True, **kwargs
            )
            logger.warning(
                "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. "
                "Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead."
            )

        from_pt = not (from_tf | from_flax)

        user_agent = {"file_type": "model", "framework": "pytorch", "from_auto_class": from_auto_class}
        if from_pipeline is not None:
            user_agent["using_pipeline"] = from_pipeline

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True

        # Load config if we don't provide a configuration
        if not isinstance(config, PretrainedConfig):
            config_path = config if config is not None else pretrained_model_name_or_path
            config, model_kwargs = cls.config_class.from_pretrained(
                config_path,
                cache_dir=cache_dir,
                return_unused_kwargs=True,
                force_download=force_download,
                proxies=proxies,
                local_files_only=local_files_only,
                token=token,
                revision=revision,
                subfolder=subfolder,
                gguf_file=gguf_file,
                _from_auto=from_auto_class,
                _from_pipeline=from_pipeline,
                **kwargs,
            )
            if "gguf_file" in model_kwargs:
                model_kwargs.pop("gguf_file")
        else:
            config = copy.deepcopy(config)
            model_kwargs = kwargs

        # Because some composite configs call super().__init__ before instantiating the sub-configs, we need this call
        # to correctly redispatch recursively if the kwarg is provided
        if "attn_implementation" in kwargs:
            config._attn_implementation = kwargs.pop("attn_implementation")

        transformers_explicit_filename = getattr(config, "transformers_weights", None)

        if transformers_explicit_filename is not None:
            if not transformers_explicit_filename.endswith(
                ".safetensors"
            ) and not transformers_explicit_filename.endswith(".safetensors.index.json"):
                raise ValueError(
                    "The transformers file in the config seems to be incorrect: it is neither a safetensors file "
                    "(*.safetensors) nor a safetensors index file (*.safetensors.index.json): "
                    f"{transformers_explicit_filename}"
                )

        hf_quantizer, config, dtype, device_map = get_hf_quantizer(
            config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent
        )

        if gguf_file is not None and hf_quantizer is not None:
            raise ValueError(
                "You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub."
            )

        if (
            gguf_file
            and device_map is not None
            and ((isinstance(device_map, dict) and "disk" in device_map.values()) or "disk" in device_map)
        ):
            raise RuntimeError(
                "One or more modules is configured to be mapped to disk. Disk offload is not supported for models "
                "loaded from GGUF files."
            )

        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
            pretrained_model_name_or_path=pretrained_model_name_or_path,
            subfolder=subfolder,
            variant=variant,
            gguf_file=gguf_file,
            from_tf=from_tf,
            from_flax=from_flax,
            use_safetensors=use_safetensors,
            cache_dir=cache_dir,
            force_download=force_download,
            proxies=proxies,
            local_files_only=local_files_only,
            token=token,
            user_agent=user_agent,
            revision=revision,
            commit_hash=commit_hash,
            is_remote_code=cls._auto_class is not None,
            transformers_explicit_filename=transformers_explicit_filename,
        )

        is_sharded = sharded_metadata is not None
        is_quantized = hf_quantizer is not None
        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None

        if is_from_file and not is_sharded and checkpoint_files[0].endswith(".safetensors"):
            with safe_open(checkpoint_files[0], framework="pt") as f:
                metadata = f.metadata()

            if metadata is None:
                # Assume it's a pytorch checkpoint (introduced for timm checkpoints)
                pass
            elif metadata.get("format") == "pt":
                pass
            elif metadata.get("format") == "tf":
                from_tf = True
                logger.info("A TensorFlow safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "flax":
                from_flax = True
                logger.info("A Flax safetensors file is being loaded in a PyTorch model.")
            elif metadata.get("format") == "mlx":
                # This is a mlx file, we assume weights are compatible with pt
                pass
            else:
                raise ValueError(
                    f"Incompatible safetensors file. File metadata is not ['pt', 'tf', 'flax', 'mlx'] but {metadata.get('format')}"
                )

        from_pt = not (from_tf | from_flax)

        if from_pt:
            if gguf_file:
                from .modeling_gguf_pytorch_utils import load_gguf_checkpoint

                # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was
                # passed directly as a kwarg from now on
                with torch.device("meta"):
                    dummy_model = cls(config)
                state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[
                    "tensors"
                ]

            # Find the correct dtype based on current state
            config, dtype, dtype_orig = _get_dtype(
                cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only
            )

        config.name_or_path = pretrained_model_name_or_path
        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)
        config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.
        with ContextManagers(model_init_context):
            # Let's make sure we don't run the init function of buffer modules
>           model = cls(config, *model_args, **model_kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           TypeError: CLIPTextModel.__init__() got an unexpected keyword argument 'offload_state_dict'

.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4974: TypeError
----------------------------- Captured stderr call -----------------------------
Couldn't connect to the Hub: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/hf-internal-testing/tiny-sd-pipe (Request ID: Root=1-68e58b20-35068ef21e390cc47ae67675;c8e47a06-671e-4038-ac10-d9eee56309c7)

We had to rate limit your IP (169.237.118.139). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API..
Will try to load from local cache.
Loading pipeline components...:   0%|                    | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  40%|       | 2/5 [00:00<00:00, 38.44it/s]
_________ TestBaseModelRevision.test_save_and_load_base_model_revision _________
[gw0] linux -- Python 3.11.0 /app/.venv/bin/python

response = <Response [429]>, endpoint_name = None

    def hf_raise_for_status(response: Response, endpoint_name: Optional[str] = None) -> None:
        """
        Internal version of `response.raise_for_status()` that will refine a
        potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.

        This helper is meant to be the unique method to raise_for_status when making a call
        to the Hugging Face Hub.


        Example:
        ```py
            import requests
            from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError

            response = get_session().post(...)
            try:
                hf_raise_for_status(response)
            except HfHubHTTPError as e:
                print(str(e)) # formatted message
                e.request_id, e.server_message # details returned by server

                # Complete the error message with additional information once it's raised
                e.append_to_message("\n`create_commit` expects the repository to exist.")
                raise
        ```

        Args:
            response (`Response`):
                Response from the server.
            endpoint_name (`str`, *optional*):
                Name of the endpoint that has been called. If provided, the error message
                will be more complete.

        <Tip warning={true}>

        Raises when the request has failed:

            - [`~utils.RepositoryNotFoundError`]
                If the repository to download from cannot be found. This may be because it
                doesn't exist, because `repo_type` is not set correctly, or because the repo
                is `private` and you do not have access.
            - [`~utils.GatedRepoError`]
                If the repository exists but is gated and the user is not on the authorized
                list.
            - [`~utils.RevisionNotFoundError`]
                If the repository exists but the revision couldn't be find.
            - [`~utils.EntryNotFoundError`]
                If the repository exists but the entry (e.g. the requested file) couldn't be
                find.
            - [`~utils.BadRequestError`]
                If request failed with a HTTP 400 BadRequest error.
            - [`~utils.HfHubHTTPError`]
                If request failed for a reason not listed above.

        </Tip>
        """
        try:
>           response.raise_for_status()

.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:407:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <Response [429]>

    def raise_for_status(self):
        """Raises :class:`HTTPError`, if one occurred."""

        http_error_msg = ""
        if isinstance(self.reason, bytes):
            # We attempt to decode utf-8 first because some servers
            # choose to localize their reason strings. If the string
            # isn't utf-8, we fall back to iso-8859-1 for all other
            # encodings. (See PR #3538)
            try:
                reason = self.reason.decode("utf-8")
            except UnicodeDecodeError:
                reason = self.reason.decode("iso-8859-1")
        else:
            reason = self.reason

        if 400 <= self.status_code < 500:
            http_error_msg = (
                f"{self.status_code} Client Error: {reason} for url: {self.url}"
            )

        elif 500 <= self.status_code < 600:
            http_error_msg = (
                f"{self.status_code} Server Error: {reason} for url: {self.url}"
            )

        if http_error_msg:
>           raise HTTPError(http_error_msg, response=self)
E           requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/peft-internal-testing/tiny-random-BertModel/xet-read-token/08f3923b4fa5c5b5945392c81ee228b83acd623b

.venv/lib/python3.11/site-packages/requests/models.py:1026: HTTPError

The above exception was the direct cause of the following exception:

path_or_repo_id = 'peft-internal-testing/tiny-random-BertModel'
filenames = ['model.safetensors'], cache_dir = '/root/.cache/huggingface/hub'
force_download = False, resume_download = None, proxies = None, token = None
revision = 'main', local_files_only = False, subfolder = '', repo_type = None
user_agent = 'transformers/4.57.0; python/3.11.0rc1; session_id/35c32e5ae0794844b6384eb99f5b631c; torch/2.8.0; file_type/model; framework/pytorch; from_auto_class/False'
_raise_exceptions_for_gated_repo = False
_raise_exceptions_for_missing_entries = False
_raise_exceptions_for_connection_errors = True
_commit_hash = '08f3923b4fa5c5b5945392c81ee228b83acd623b'
deprecated_kwargs = {}, use_auth_token = None
full_filenames = ['model.safetensors'], existing_files = []
filename = 'model.safetensors', resolved_file = None, file_counter = 0

    def cached_files(
        path_or_repo_id: Union[str, os.PathLike],
        filenames: list[str],
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        force_download: bool = False,
        resume_download: Optional[bool] = None,
        proxies: Optional[dict[str, str]] = None,
        token: Optional[Union[bool, str]] = None,
        revision: Optional[str] = None,
        local_files_only: bool = False,
        subfolder: str = "",
        repo_type: Optional[str] = None,
        user_agent: Optional[Union[str, dict[str, str]]] = None,
        _raise_exceptions_for_gated_repo: bool = True,
        _raise_exceptions_for_missing_entries: bool = True,
        _raise_exceptions_for_connection_errors: bool = True,
        _commit_hash: Optional[str] = None,
        **deprecated_kwargs,
    ) -> Optional[str]:
        """
        Tries to locate several files in a local folder and repo, downloads and cache them if necessary.

        Args:
            path_or_repo_id (`str` or `os.PathLike`):
                This can be either:
                - a string, the *model id* of a model repo on huggingface.co.
                - a path to a *directory* potentially containing the file.
            filenames (`list[str]`):
                The name of all the files to locate in `path_or_repo`.
            cache_dir (`str` or `os.PathLike`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the standard
                cache should not be used.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force to (re-)download the configuration files and override the cached versions if they
                exist.
            resume_download:
                Deprecated and ignored. All downloads are now resumed by default when possible.
                Will be removed in v5 of Transformers.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.
            token (`str` or *bool*, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated
                when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.
            local_files_only (`bool`, *optional*, defaults to `False`):
                If `True`, will only try to load the tokenizer configuration from local files.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            repo_type (`str`, *optional*):
                Specify the repo type (useful when downloading from a space for instance).

        Private args:
            _raise_exceptions_for_gated_repo (`bool`):
                if False, do not raise an exception for gated repo error but return None.
            _raise_exceptions_for_missing_entries (`bool`):
                if False, do not raise an exception for missing entries but return None.
            _raise_exceptions_for_connection_errors (`bool`):
                if False, do not raise an exception for connection errors but return None.
            _commit_hash (`str`, *optional*):
                passed when we are chaining several calls to various files (e.g. when loading a tokenizer or
                a pipeline). If files are cached for this commit hash, avoid calls to head and get from the cache.

        <Tip>

        Passing `token=True` is required when you want to use a private model.

        </Tip>

        Returns:
            `Optional[str]`: Returns the resolved file (to the cache folder if downloaded from a repo).

        Examples:

        ```python
        # Download a model weight from the Hub and cache it.
        model_weights_file = cached_file("google-bert/bert-base-uncased", "pytorch_model.bin")
        ```
        """
        use_auth_token = deprecated_kwargs.pop("use_auth_token", None)
        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
            token = use_auth_token

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True
        if subfolder is None:
            subfolder = ""

        # Add folder to filenames
        full_filenames = [os.path.join(subfolder, file) for file in filenames]

        path_or_repo_id = str(path_or_repo_id)
        existing_files = []
        for filename in full_filenames:
            if os.path.isdir(path_or_repo_id):
                resolved_file = os.path.join(path_or_repo_id, filename)
                if not os.path.isfile(resolved_file):
                    if _raise_exceptions_for_missing_entries and filename != os.path.join(subfolder, "config.json"):
                        revision_ = "main" if revision is None else revision
                        raise OSError(
                            f"{path_or_repo_id} does not appear to have a file named {filename}. Checkout "
                            f"'https://huggingface.co/{path_or_repo_id}/tree/{revision_}' for available files."
                        )
                    else:
                        continue
                existing_files.append(resolved_file)

        if os.path.isdir(path_or_repo_id):
            return existing_files if existing_files else None

        if cache_dir is None:
            cache_dir = TRANSFORMERS_CACHE
        if isinstance(cache_dir, Path):
            cache_dir = str(cache_dir)

        existing_files = []
        file_counter = 0
        if _commit_hash is not None and not force_download:
            for filename in full_filenames:
                # If the file is cached under that commit hash, we return it directly.
                resolved_file = try_to_load_from_cache(
                    path_or_repo_id, filename, cache_dir=cache_dir, revision=_commit_hash, repo_type=repo_type
                )
                if resolved_file is not None:
                    if resolved_file is not _CACHED_NO_EXIST:
                        file_counter += 1
                        existing_files.append(resolved_file)
                    elif not _raise_exceptions_for_missing_entries:
                        file_counter += 1
                    else:
                        raise OSError(f"Could not locate {filename} inside {path_or_repo_id}.")

        # Either all the files were found, or some were _CACHED_NO_EXIST but we do not raise for missing entries
        if file_counter == len(full_filenames):
            return existing_files if len(existing_files) > 0 else None

        user_agent = http_user_agent(user_agent)
        # download the files if needed
        try:
            if len(full_filenames) == 1:
                # This is slightly better for only 1 file
>               hf_hub_download(
                    path_or_repo_id,
                    filenames[0],
                    subfolder=None if len(subfolder) == 0 else subfolder,
                    repo_type=repo_type,
                    revision=revision,
                    cache_dir=cache_dir,
                    user_agent=user_agent,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    token=token,
                    local_files_only=local_files_only,
                )

.venv/lib/python3.11/site-packages/transformers/utils/hub.py:479:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1010: in hf_hub_download
    return _hf_hub_download_to_cache_dir(
.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1171: in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1723: in _download_to_tmp_and_move
    xet_get(
.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:594: in xet_get
    connection_info = refresh_xet_connection_info(file_data=xet_file_data, headers=headers)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_xet.py:116: in refresh_xet_connection_info
    return _fetch_xet_connection_info_with_url(file_data.refresh_route, headers)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_xet.py:187: in _fetch_xet_connection_info_with_url
    hf_raise_for_status(resp)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

response = <Response [429]>, endpoint_name = None

    def hf_raise_for_status(response: Response, endpoint_name: Optional[str] = None) -> None:
        """
        Internal version of `response.raise_for_status()` that will refine a
        potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.

        This helper is meant to be the unique method to raise_for_status when making a call
        to the Hugging Face Hub.


        Example:
        ```py
            import requests
            from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError

            response = get_session().post(...)
            try:
                hf_raise_for_status(response)
            except HfHubHTTPError as e:
                print(str(e)) # formatted message
                e.request_id, e.server_message # details returned by server

                # Complete the error message with additional information once it's raised
                e.append_to_message("\n`create_commit` expects the repository to exist.")
                raise
        ```

        Args:
            response (`Response`):
                Response from the server.
            endpoint_name (`str`, *optional*):
                Name of the endpoint that has been called. If provided, the error message
                will be more complete.

        <Tip warning={true}>

        Raises when the request has failed:

            - [`~utils.RepositoryNotFoundError`]
                If the repository to download from cannot be found. This may be because it
                doesn't exist, because `repo_type` is not set correctly, or because the repo
                is `private` and you do not have access.
            - [`~utils.GatedRepoError`]
                If the repository exists but is gated and the user is not on the authorized
                list.
            - [`~utils.RevisionNotFoundError`]
                If the repository exists but the revision couldn't be find.
            - [`~utils.EntryNotFoundError`]
                If the repository exists but the entry (e.g. the requested file) couldn't be
                find.
            - [`~utils.BadRequestError`]
                If request failed with a HTTP 400 BadRequest error.
            - [`~utils.HfHubHTTPError`]
                If request failed for a reason not listed above.

        </Tip>
        """
        try:
            response.raise_for_status()
        except HTTPError as e:
            error_code = response.headers.get("X-Error-Code")
            error_message = response.headers.get("X-Error-Message")

            if error_code == "RevisionNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Revision Not Found for url: {response.url}."
                raise _format(RevisionNotFoundError, message, response) from e

            elif error_code == "EntryNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Entry Not Found for url: {response.url}."
                raise _format(EntryNotFoundError, message, response) from e

            elif error_code == "GatedRepo":
                message = (
                    f"{response.status_code} Client Error." + "\n\n" + f"Cannot access gated repo for url {response.url}."
                )
                raise _format(GatedRepoError, message, response) from e

            elif error_message == "Access to this resource is disabled.":
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Cannot access repository for url {response.url}."
                    + "\n"
                    + "Access to this resource is disabled."
                )
                raise _format(DisabledRepoError, message, response) from e

            elif error_code == "RepoNotFound" or (
                response.status_code == 401
                and error_message != "Invalid credentials in Authorization header"
                and response.request is not None
                and response.request.url is not None
                and REPO_API_REGEX.search(response.request.url) is not None
            ):
                # 401 is misleading as it is returned for:
                #    - private and gated repos if user is not authenticated
                #    - missing repos
                # => for now, we process them as `RepoNotFound` anyway.
                # See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Repository Not Found for url: {response.url}."
                    + "\nPlease make sure you specified the correct `repo_id` and"
                    " `repo_type`.\nIf you are trying to access a private or gated repo,"
                    " make sure you are authenticated. For more details, see"
                    " https://huggingface.co/docs/huggingface_hub/authentication"
                )
                raise _format(RepositoryNotFoundError, message, response) from e

            elif response.status_code == 400:
                message = (
                    f"\n\nBad request for {endpoint_name} endpoint:" if endpoint_name is not None else "\n\nBad request:"
                )
                raise _format(BadRequestError, message, response) from e

            elif response.status_code == 403:
                message = (
                    f"\n\n{response.status_code} Forbidden: {error_message}."
                    + f"\nCannot access content at: {response.url}."
                    + "\nMake sure your token has the correct permissions."
                )
                raise _format(HfHubHTTPError, message, response) from e

            elif response.status_code == 416:
                range_header = response.request.headers.get("Range")
                message = f"{e}. Requested range: {range_header}. Content-Range: {response.headers.get('Content-Range')}."
                raise _format(HfHubHTTPError, message, response) from e

            # Convert `HTTPError` into a `HfHubHTTPError` to display request information
            # as well (request id and/or server error message)
>           raise _format(HfHubHTTPError, str(e), response) from e
E           huggingface_hub.errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/peft-internal-testing/tiny-random-BertModel/xet-read-token/08f3923b4fa5c5b5945392c81ee228b83acd623b (Request ID: Root=1-68e58b28-790fad3f08b612de7aadd6f2;bbb888a8-f707-4677-baa9-5303728cdfce)
E
E           We had to rate limit your IP (169.237.118.139). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API.

.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:480: HfHubHTTPError

The above exception was the direct cause of the following exception:

self = <tests.test_hub_features.TestBaseModelRevision object at 0x7f72b8bd6610>
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/popen-gw0/test_save_and_load_base_model_0')

    def test_save_and_load_base_model_revision(self, tmp_path):
        r"""
        Test saving a PeftModel with a base model revision and loading with AutoPeftModel to recover the same base
        model
        """
        lora_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.0)
        test_inputs = torch.arange(10).reshape(-1, 1)

        base_model_id = "peft-internal-testing/tiny-random-BertModel"
        revision = "v2.0.0"

        base_model_revision = AutoModelForCausalLM.from_pretrained(base_model_id, revision=revision).eval()
        peft_model_revision = get_peft_model(base_model_revision, lora_config, revision=revision)
        output_revision = peft_model_revision(test_inputs).logits

        # sanity check: the model without revision should be different
>       base_model_no_revision = AutoModelForCausalLM.from_pretrained(base_model_id, revision="main").eval()
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_hub_features.py:86:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:604: in from_pretrained
    return model_class.from_pretrained(
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4903: in from_pretrained
    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:1041: in _get_resolved_checkpoint_files
    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/utils/hub.py:322: in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

path_or_repo_id = 'peft-internal-testing/tiny-random-BertModel'
filenames = ['model.safetensors'], cache_dir = '/root/.cache/huggingface/hub'
force_download = False, resume_download = None, proxies = None, token = None
revision = 'main', local_files_only = False, subfolder = '', repo_type = None
user_agent = 'transformers/4.57.0; python/3.11.0rc1; session_id/35c32e5ae0794844b6384eb99f5b631c; torch/2.8.0; file_type/model; framework/pytorch; from_auto_class/False'
_raise_exceptions_for_gated_repo = False
_raise_exceptions_for_missing_entries = False
_raise_exceptions_for_connection_errors = True
_commit_hash = '08f3923b4fa5c5b5945392c81ee228b83acd623b'
deprecated_kwargs = {}, use_auth_token = None
full_filenames = ['model.safetensors'], existing_files = []
filename = 'model.safetensors', resolved_file = None, file_counter = 0

    def cached_files(
        path_or_repo_id: Union[str, os.PathLike],
        filenames: list[str],
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        force_download: bool = False,
        resume_download: Optional[bool] = None,
        proxies: Optional[dict[str, str]] = None,
        token: Optional[Union[bool, str]] = None,
        revision: Optional[str] = None,
        local_files_only: bool = False,
        subfolder: str = "",
        repo_type: Optional[str] = None,
        user_agent: Optional[Union[str, dict[str, str]]] = None,
        _raise_exceptions_for_gated_repo: bool = True,
        _raise_exceptions_for_missing_entries: bool = True,
        _raise_exceptions_for_connection_errors: bool = True,
        _commit_hash: Optional[str] = None,
        **deprecated_kwargs,
    ) -> Optional[str]:
        """
        Tries to locate several files in a local folder and repo, downloads and cache them if necessary.

        Args:
            path_or_repo_id (`str` or `os.PathLike`):
                This can be either:
                - a string, the *model id* of a model repo on huggingface.co.
                - a path to a *directory* potentially containing the file.
            filenames (`list[str]`):
                The name of all the files to locate in `path_or_repo`.
            cache_dir (`str` or `os.PathLike`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the standard
                cache should not be used.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force to (re-)download the configuration files and override the cached versions if they
                exist.
            resume_download:
                Deprecated and ignored. All downloads are now resumed by default when possible.
                Will be removed in v5 of Transformers.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.
            token (`str` or *bool*, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated
                when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.
            local_files_only (`bool`, *optional*, defaults to `False`):
                If `True`, will only try to load the tokenizer configuration from local files.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            repo_type (`str`, *optional*):
                Specify the repo type (useful when downloading from a space for instance).

        Private args:
            _raise_exceptions_for_gated_repo (`bool`):
                if False, do not raise an exception for gated repo error but return None.
            _raise_exceptions_for_missing_entries (`bool`):
                if False, do not raise an exception for missing entries but return None.
            _raise_exceptions_for_connection_errors (`bool`):
                if False, do not raise an exception for connection errors but return None.
            _commit_hash (`str`, *optional*):
                passed when we are chaining several calls to various files (e.g. when loading a tokenizer or
                a pipeline). If files are cached for this commit hash, avoid calls to head and get from the cache.

        <Tip>

        Passing `token=True` is required when you want to use a private model.

        </Tip>

        Returns:
            `Optional[str]`: Returns the resolved file (to the cache folder if downloaded from a repo).

        Examples:

        ```python
        # Download a model weight from the Hub and cache it.
        model_weights_file = cached_file("google-bert/bert-base-uncased", "pytorch_model.bin")
        ```
        """
        use_auth_token = deprecated_kwargs.pop("use_auth_token", None)
        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
            token = use_auth_token

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True
        if subfolder is None:
            subfolder = ""

        # Add folder to filenames
        full_filenames = [os.path.join(subfolder, file) for file in filenames]

        path_or_repo_id = str(path_or_repo_id)
        existing_files = []
        for filename in full_filenames:
            if os.path.isdir(path_or_repo_id):
                resolved_file = os.path.join(path_or_repo_id, filename)
                if not os.path.isfile(resolved_file):
                    if _raise_exceptions_for_missing_entries and filename != os.path.join(subfolder, "config.json"):
                        revision_ = "main" if revision is None else revision
                        raise OSError(
                            f"{path_or_repo_id} does not appear to have a file named {filename}. Checkout "
                            f"'https://huggingface.co/{path_or_repo_id}/tree/{revision_}' for available files."
                        )
                    else:
                        continue
                existing_files.append(resolved_file)

        if os.path.isdir(path_or_repo_id):
            return existing_files if existing_files else None

        if cache_dir is None:
            cache_dir = TRANSFORMERS_CACHE
        if isinstance(cache_dir, Path):
            cache_dir = str(cache_dir)

        existing_files = []
        file_counter = 0
        if _commit_hash is not None and not force_download:
            for filename in full_filenames:
                # If the file is cached under that commit hash, we return it directly.
                resolved_file = try_to_load_from_cache(
                    path_or_repo_id, filename, cache_dir=cache_dir, revision=_commit_hash, repo_type=repo_type
                )
                if resolved_file is not None:
                    if resolved_file is not _CACHED_NO_EXIST:
                        file_counter += 1
                        existing_files.append(resolved_file)
                    elif not _raise_exceptions_for_missing_entries:
                        file_counter += 1
                    else:
                        raise OSError(f"Could not locate {filename} inside {path_or_repo_id}.")

        # Either all the files were found, or some were _CACHED_NO_EXIST but we do not raise for missing entries
        if file_counter == len(full_filenames):
            return existing_files if len(existing_files) > 0 else None

        user_agent = http_user_agent(user_agent)
        # download the files if needed
        try:
            if len(full_filenames) == 1:
                # This is slightly better for only 1 file
                hf_hub_download(
                    path_or_repo_id,
                    filenames[0],
                    subfolder=None if len(subfolder) == 0 else subfolder,
                    repo_type=repo_type,
                    revision=revision,
                    cache_dir=cache_dir,
                    user_agent=user_agent,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    token=token,
                    local_files_only=local_files_only,
                )
            else:
                snapshot_download(
                    path_or_repo_id,
                    allow_patterns=full_filenames,
                    repo_type=repo_type,
                    revision=revision,
                    cache_dir=cache_dir,
                    user_agent=user_agent,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    token=token,
                    local_files_only=local_files_only,
                )

        except Exception as e:
            # We cannot recover from them
            if isinstance(e, RepositoryNotFoundError) and not isinstance(e, GatedRepoError):
                raise OSError(
                    f"{path_or_repo_id} is not a local folder and is not a valid model identifier "
                    "listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token "
                    "having permission to this repo either by logging in with `hf auth login` or by passing "
                    "`token=<your_token>`"
                ) from e
            elif isinstance(e, RevisionNotFoundError):
                raise OSError(
                    f"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists "
                    "for this model name. Check the model page at "
                    f"'https://huggingface.co/{path_or_repo_id}' for available revisions."
                ) from e
            elif isinstance(e, PermissionError):
                raise OSError(
                    f"PermissionError at {e.filename} when downloading {path_or_repo_id}. "
                    "Check cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); "
                    "2) a previous download was canceled and the lock file needs manual removal."
                ) from e

            # Now we try to recover if we can find all files correctly in the cache
            resolved_files = [
                _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
                for filename in full_filenames
            ]
            if all(file is not None for file in resolved_files):
                return resolved_files

            # Raise based on the flags. Note that we will raise for missing entries at the very end, even when
            # not entering this Except block, as it may also happen when `snapshot_download` does not raise
            if isinstance(e, GatedRepoError):
                if not _raise_exceptions_for_gated_repo:
                    return None
                raise OSError(
                    "You are trying to access a gated repo.\nMake sure to have access to it at "
                    f"https://huggingface.co/{path_or_repo_id}.\n{str(e)}"
                ) from e
            elif isinstance(e, LocalEntryNotFoundError):
                if not _raise_exceptions_for_connection_errors:
                    return None
                # Here we only raise if both flags for missing entry and connection errors are True (because it can be raised
                # even when `local_files_only` is True, in which case raising for connections errors only would not make sense)
                elif _raise_exceptions_for_missing_entries:
                    raise OSError(
                        f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load the files, and couldn't find them in the"
                        f" cached files.\nCheck your internet connection or see how to run the library in offline mode at"
                        " 'https://huggingface.co/docs/transformers/installation#offline-mode'."
                    ) from e
            # snapshot_download will not raise EntryNotFoundError, but hf_hub_download can. If this is the case, it will be treated
            # later on anyway and re-raised if needed
            elif isinstance(e, HTTPError) and not isinstance(e, EntryNotFoundError):
                if not _raise_exceptions_for_connection_errors:
                    return None
>               raise OSError(f"There was a specific connection error when trying to load {path_or_repo_id}:\n{e}") from e
E               OSError: There was a specific connection error when trying to load peft-internal-testing/tiny-random-BertModel:
E               429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/peft-internal-testing/tiny-random-BertModel/xet-read-token/08f3923b4fa5c5b5945392c81ee228b83acd623b (Request ID: Root=1-68e58b28-790fad3f08b612de7aadd6f2;bbb888a8-f707-4677-baa9-5303728cdfce)
E
E               We had to rate limit your IP (169.237.118.139). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API.

.venv/lib/python3.11/site-packages/transformers/utils/hub.py:563: OSError
----------------------------- Captured stderr call -----------------------------
If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`
We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
_________ TestLoraInitialization.test_lora_incompatible_mamba_modules __________
[gw0] linux -- Python 3.11.0 /app/.venv/bin/python

response = <Response [429]>, endpoint_name = None

    def hf_raise_for_status(response: Response, endpoint_name: Optional[str] = None) -> None:
        """
        Internal version of `response.raise_for_status()` that will refine a
        potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.

        This helper is meant to be the unique method to raise_for_status when making a call
        to the Hugging Face Hub.


        Example:
        ```py
            import requests
            from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError

            response = get_session().post(...)
            try:
                hf_raise_for_status(response)
            except HfHubHTTPError as e:
                print(str(e)) # formatted message
                e.request_id, e.server_message # details returned by server

                # Complete the error message with additional information once it's raised
                e.append_to_message("\n`create_commit` expects the repository to exist.")
                raise
        ```

        Args:
            response (`Response`):
                Response from the server.
            endpoint_name (`str`, *optional*):
                Name of the endpoint that has been called. If provided, the error message
                will be more complete.

        <Tip warning={true}>

        Raises when the request has failed:

            - [`~utils.RepositoryNotFoundError`]
                If the repository to download from cannot be found. This may be because it
                doesn't exist, because `repo_type` is not set correctly, or because the repo
                is `private` and you do not have access.
            - [`~utils.GatedRepoError`]
                If the repository exists but is gated and the user is not on the authorized
                list.
            - [`~utils.RevisionNotFoundError`]
                If the repository exists but the revision couldn't be find.
            - [`~utils.EntryNotFoundError`]
                If the repository exists but the entry (e.g. the requested file) couldn't be
                find.
            - [`~utils.BadRequestError`]
                If request failed with a HTTP 400 BadRequest error.
            - [`~utils.HfHubHTTPError`]
                If request failed for a reason not listed above.

        </Tip>
        """
        try:
>           response.raise_for_status()

.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:407:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <Response [429]>

    def raise_for_status(self):
        """Raises :class:`HTTPError`, if one occurred."""

        http_error_msg = ""
        if isinstance(self.reason, bytes):
            # We attempt to decode utf-8 first because some servers
            # choose to localize their reason strings. If the string
            # isn't utf-8, we fall back to iso-8859-1 for all other
            # encodings. (See PR #3538)
            try:
                reason = self.reason.decode("utf-8")
            except UnicodeDecodeError:
                reason = self.reason.decode("iso-8859-1")
        else:
            reason = self.reason

        if 400 <= self.status_code < 500:
            http_error_msg = (
                f"{self.status_code} Client Error: {reason} for url: {self.url}"
            )

        elif 500 <= self.status_code < 600:
            http_error_msg = (
                f"{self.status_code} Server Error: {reason} for url: {self.url}"
            )

        if http_error_msg:
>           raise HTTPError(http_error_msg, response=self)
E           requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/tiiuae/falcon-mamba-tiny-dev/xet-read-token/f22d33f72a4cc27c1827f3020a6df28c5097fd06

.venv/lib/python3.11/site-packages/requests/models.py:1026: HTTPError

The above exception was the direct cause of the following exception:

path_or_repo_id = 'tiiuae/falcon-mamba-tiny-dev'
filenames = ['model.safetensors'], cache_dir = '/root/.cache/huggingface/hub'
force_download = False, resume_download = None, proxies = None, token = None
revision = 'main', local_files_only = False, subfolder = '', repo_type = None
user_agent = 'transformers/4.57.0; python/3.11.0rc1; session_id/35c32e5ae0794844b6384eb99f5b631c; torch/2.8.0; file_type/model; framework/pytorch; from_auto_class/False'
_raise_exceptions_for_gated_repo = False
_raise_exceptions_for_missing_entries = False
_raise_exceptions_for_connection_errors = True
_commit_hash = 'f22d33f72a4cc27c1827f3020a6df28c5097fd06'
deprecated_kwargs = {}, use_auth_token = None
full_filenames = ['model.safetensors'], existing_files = []
filename = 'model.safetensors', resolved_file = None, file_counter = 0

    def cached_files(
        path_or_repo_id: Union[str, os.PathLike],
        filenames: list[str],
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        force_download: bool = False,
        resume_download: Optional[bool] = None,
        proxies: Optional[dict[str, str]] = None,
        token: Optional[Union[bool, str]] = None,
        revision: Optional[str] = None,
        local_files_only: bool = False,
        subfolder: str = "",
        repo_type: Optional[str] = None,
        user_agent: Optional[Union[str, dict[str, str]]] = None,
        _raise_exceptions_for_gated_repo: bool = True,
        _raise_exceptions_for_missing_entries: bool = True,
        _raise_exceptions_for_connection_errors: bool = True,
        _commit_hash: Optional[str] = None,
        **deprecated_kwargs,
    ) -> Optional[str]:
        """
        Tries to locate several files in a local folder and repo, downloads and cache them if necessary.

        Args:
            path_or_repo_id (`str` or `os.PathLike`):
                This can be either:
                - a string, the *model id* of a model repo on huggingface.co.
                - a path to a *directory* potentially containing the file.
            filenames (`list[str]`):
                The name of all the files to locate in `path_or_repo`.
            cache_dir (`str` or `os.PathLike`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the standard
                cache should not be used.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force to (re-)download the configuration files and override the cached versions if they
                exist.
            resume_download:
                Deprecated and ignored. All downloads are now resumed by default when possible.
                Will be removed in v5 of Transformers.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.
            token (`str` or *bool*, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated
                when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.
            local_files_only (`bool`, *optional*, defaults to `False`):
                If `True`, will only try to load the tokenizer configuration from local files.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            repo_type (`str`, *optional*):
                Specify the repo type (useful when downloading from a space for instance).

        Private args:
            _raise_exceptions_for_gated_repo (`bool`):
                if False, do not raise an exception for gated repo error but return None.
            _raise_exceptions_for_missing_entries (`bool`):
                if False, do not raise an exception for missing entries but return None.
            _raise_exceptions_for_connection_errors (`bool`):
                if False, do not raise an exception for connection errors but return None.
            _commit_hash (`str`, *optional*):
                passed when we are chaining several calls to various files (e.g. when loading a tokenizer or
                a pipeline). If files are cached for this commit hash, avoid calls to head and get from the cache.

        <Tip>

        Passing `token=True` is required when you want to use a private model.

        </Tip>

        Returns:
            `Optional[str]`: Returns the resolved file (to the cache folder if downloaded from a repo).

        Examples:

        ```python
        # Download a model weight from the Hub and cache it.
        model_weights_file = cached_file("google-bert/bert-base-uncased", "pytorch_model.bin")
        ```
        """
        use_auth_token = deprecated_kwargs.pop("use_auth_token", None)
        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
            token = use_auth_token

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True
        if subfolder is None:
            subfolder = ""

        # Add folder to filenames
        full_filenames = [os.path.join(subfolder, file) for file in filenames]

        path_or_repo_id = str(path_or_repo_id)
        existing_files = []
        for filename in full_filenames:
            if os.path.isdir(path_or_repo_id):
                resolved_file = os.path.join(path_or_repo_id, filename)
                if not os.path.isfile(resolved_file):
                    if _raise_exceptions_for_missing_entries and filename != os.path.join(subfolder, "config.json"):
                        revision_ = "main" if revision is None else revision
                        raise OSError(
                            f"{path_or_repo_id} does not appear to have a file named {filename}. Checkout "
                            f"'https://huggingface.co/{path_or_repo_id}/tree/{revision_}' for available files."
                        )
                    else:
                        continue
                existing_files.append(resolved_file)

        if os.path.isdir(path_or_repo_id):
            return existing_files if existing_files else None

        if cache_dir is None:
            cache_dir = TRANSFORMERS_CACHE
        if isinstance(cache_dir, Path):
            cache_dir = str(cache_dir)

        existing_files = []
        file_counter = 0
        if _commit_hash is not None and not force_download:
            for filename in full_filenames:
                # If the file is cached under that commit hash, we return it directly.
                resolved_file = try_to_load_from_cache(
                    path_or_repo_id, filename, cache_dir=cache_dir, revision=_commit_hash, repo_type=repo_type
                )
                if resolved_file is not None:
                    if resolved_file is not _CACHED_NO_EXIST:
                        file_counter += 1
                        existing_files.append(resolved_file)
                    elif not _raise_exceptions_for_missing_entries:
                        file_counter += 1
                    else:
                        raise OSError(f"Could not locate {filename} inside {path_or_repo_id}.")

        # Either all the files were found, or some were _CACHED_NO_EXIST but we do not raise for missing entries
        if file_counter == len(full_filenames):
            return existing_files if len(existing_files) > 0 else None

        user_agent = http_user_agent(user_agent)
        # download the files if needed
        try:
            if len(full_filenames) == 1:
                # This is slightly better for only 1 file
>               hf_hub_download(
                    path_or_repo_id,
                    filenames[0],
                    subfolder=None if len(subfolder) == 0 else subfolder,
                    repo_type=repo_type,
                    revision=revision,
                    cache_dir=cache_dir,
                    user_agent=user_agent,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    token=token,
                    local_files_only=local_files_only,
                )

.venv/lib/python3.11/site-packages/transformers/utils/hub.py:479:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1010: in hf_hub_download
    return _hf_hub_download_to_cache_dir(
.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1171: in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1723: in _download_to_tmp_and_move
    xet_get(
.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:594: in xet_get
    connection_info = refresh_xet_connection_info(file_data=xet_file_data, headers=headers)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_xet.py:116: in refresh_xet_connection_info
    return _fetch_xet_connection_info_with_url(file_data.refresh_route, headers)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/huggingface_hub/utils/_xet.py:187: in _fetch_xet_connection_info_with_url
    hf_raise_for_status(resp)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

response = <Response [429]>, endpoint_name = None

    def hf_raise_for_status(response: Response, endpoint_name: Optional[str] = None) -> None:
        """
        Internal version of `response.raise_for_status()` that will refine a
        potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.

        This helper is meant to be the unique method to raise_for_status when making a call
        to the Hugging Face Hub.


        Example:
        ```py
            import requests
            from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError

            response = get_session().post(...)
            try:
                hf_raise_for_status(response)
            except HfHubHTTPError as e:
                print(str(e)) # formatted message
                e.request_id, e.server_message # details returned by server

                # Complete the error message with additional information once it's raised
                e.append_to_message("\n`create_commit` expects the repository to exist.")
                raise
        ```

        Args:
            response (`Response`):
                Response from the server.
            endpoint_name (`str`, *optional*):
                Name of the endpoint that has been called. If provided, the error message
                will be more complete.

        <Tip warning={true}>

        Raises when the request has failed:

            - [`~utils.RepositoryNotFoundError`]
                If the repository to download from cannot be found. This may be because it
                doesn't exist, because `repo_type` is not set correctly, or because the repo
                is `private` and you do not have access.
            - [`~utils.GatedRepoError`]
                If the repository exists but is gated and the user is not on the authorized
                list.
            - [`~utils.RevisionNotFoundError`]
                If the repository exists but the revision couldn't be find.
            - [`~utils.EntryNotFoundError`]
                If the repository exists but the entry (e.g. the requested file) couldn't be
                find.
            - [`~utils.BadRequestError`]
                If request failed with a HTTP 400 BadRequest error.
            - [`~utils.HfHubHTTPError`]
                If request failed for a reason not listed above.

        </Tip>
        """
        try:
            response.raise_for_status()
        except HTTPError as e:
            error_code = response.headers.get("X-Error-Code")
            error_message = response.headers.get("X-Error-Message")

            if error_code == "RevisionNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Revision Not Found for url: {response.url}."
                raise _format(RevisionNotFoundError, message, response) from e

            elif error_code == "EntryNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Entry Not Found for url: {response.url}."
                raise _format(EntryNotFoundError, message, response) from e

            elif error_code == "GatedRepo":
                message = (
                    f"{response.status_code} Client Error." + "\n\n" + f"Cannot access gated repo for url {response.url}."
                )
                raise _format(GatedRepoError, message, response) from e

            elif error_message == "Access to this resource is disabled.":
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Cannot access repository for url {response.url}."
                    + "\n"
                    + "Access to this resource is disabled."
                )
                raise _format(DisabledRepoError, message, response) from e

            elif error_code == "RepoNotFound" or (
                response.status_code == 401
                and error_message != "Invalid credentials in Authorization header"
                and response.request is not None
                and response.request.url is not None
                and REPO_API_REGEX.search(response.request.url) is not None
            ):
                # 401 is misleading as it is returned for:
                #    - private and gated repos if user is not authenticated
                #    - missing repos
                # => for now, we process them as `RepoNotFound` anyway.
                # See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Repository Not Found for url: {response.url}."
                    + "\nPlease make sure you specified the correct `repo_id` and"
                    " `repo_type`.\nIf you are trying to access a private or gated repo,"
                    " make sure you are authenticated. For more details, see"
                    " https://huggingface.co/docs/huggingface_hub/authentication"
                )
                raise _format(RepositoryNotFoundError, message, response) from e

            elif response.status_code == 400:
                message = (
                    f"\n\nBad request for {endpoint_name} endpoint:" if endpoint_name is not None else "\n\nBad request:"
                )
                raise _format(BadRequestError, message, response) from e

            elif response.status_code == 403:
                message = (
                    f"\n\n{response.status_code} Forbidden: {error_message}."
                    + f"\nCannot access content at: {response.url}."
                    + "\nMake sure your token has the correct permissions."
                )
                raise _format(HfHubHTTPError, message, response) from e

            elif response.status_code == 416:
                range_header = response.request.headers.get("Range")
                message = f"{e}. Requested range: {range_header}. Content-Range: {response.headers.get('Content-Range')}."
                raise _format(HfHubHTTPError, message, response) from e

            # Convert `HTTPError` into a `HfHubHTTPError` to display request information
            # as well (request id and/or server error message)
>           raise _format(HfHubHTTPError, str(e), response) from e
E           huggingface_hub.errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/tiiuae/falcon-mamba-tiny-dev/xet-read-token/f22d33f72a4cc27c1827f3020a6df28c5097fd06 (Request ID: Root=1-68e58b2d-0cc095e3799a7c202b339589;45796e52-5f2d-410c-9c0e-707938b6694d)
E
E           We had to rate limit your IP (169.237.118.139). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API.

.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:480: HfHubHTTPError

The above exception was the direct cause of the following exception:

self = <tests.test_initialization.TestLoraInitialization object at 0x7f72b8ca6a10>

    def test_lora_incompatible_mamba_modules(self):
        # Ensure LoRA raises an error when applying to forbidden modules
        # ('out_proj', 'conv1d') in Mamba-based architectures like Falcon-Mamba tiny.
>       model = AutoModelForCausalLM.from_pretrained("tiiuae/falcon-mamba-tiny-dev")
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_initialization.py:1398:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:604: in from_pretrained
    return model_class.from_pretrained(
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277: in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4903: in from_pretrained
    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:1041: in _get_resolved_checkpoint_files
    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/transformers/utils/hub.py:322: in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

path_or_repo_id = 'tiiuae/falcon-mamba-tiny-dev'
filenames = ['model.safetensors'], cache_dir = '/root/.cache/huggingface/hub'
force_download = False, resume_download = None, proxies = None, token = None
revision = 'main', local_files_only = False, subfolder = '', repo_type = None
user_agent = 'transformers/4.57.0; python/3.11.0rc1; session_id/35c32e5ae0794844b6384eb99f5b631c; torch/2.8.0; file_type/model; framework/pytorch; from_auto_class/False'
_raise_exceptions_for_gated_repo = False
_raise_exceptions_for_missing_entries = False
_raise_exceptions_for_connection_errors = True
_commit_hash = 'f22d33f72a4cc27c1827f3020a6df28c5097fd06'
deprecated_kwargs = {}, use_auth_token = None
full_filenames = ['model.safetensors'], existing_files = []
filename = 'model.safetensors', resolved_file = None, file_counter = 0

    def cached_files(
        path_or_repo_id: Union[str, os.PathLike],
        filenames: list[str],
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        force_download: bool = False,
        resume_download: Optional[bool] = None,
        proxies: Optional[dict[str, str]] = None,
        token: Optional[Union[bool, str]] = None,
        revision: Optional[str] = None,
        local_files_only: bool = False,
        subfolder: str = "",
        repo_type: Optional[str] = None,
        user_agent: Optional[Union[str, dict[str, str]]] = None,
        _raise_exceptions_for_gated_repo: bool = True,
        _raise_exceptions_for_missing_entries: bool = True,
        _raise_exceptions_for_connection_errors: bool = True,
        _commit_hash: Optional[str] = None,
        **deprecated_kwargs,
    ) -> Optional[str]:
        """
        Tries to locate several files in a local folder and repo, downloads and cache them if necessary.

        Args:
            path_or_repo_id (`str` or `os.PathLike`):
                This can be either:
                - a string, the *model id* of a model repo on huggingface.co.
                - a path to a *directory* potentially containing the file.
            filenames (`list[str]`):
                The name of all the files to locate in `path_or_repo`.
            cache_dir (`str` or `os.PathLike`, *optional*):
                Path to a directory in which a downloaded pretrained model configuration should be cached if the standard
                cache should not be used.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force to (re-)download the configuration files and override the cached versions if they
                exist.
            resume_download:
                Deprecated and ignored. All downloads are now resumed by default when possible.
                Will be removed in v5 of Transformers.
            proxies (`dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.
            token (`str` or *bool*, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated
                when running `hf auth login` (stored in `~/.huggingface`).
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.
            local_files_only (`bool`, *optional*, defaults to `False`):
                If `True`, will only try to load the tokenizer configuration from local files.
            subfolder (`str`, *optional*, defaults to `""`):
                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
                specify the folder name here.
            repo_type (`str`, *optional*):
                Specify the repo type (useful when downloading from a space for instance).

        Private args:
            _raise_exceptions_for_gated_repo (`bool`):
                if False, do not raise an exception for gated repo error but return None.
            _raise_exceptions_for_missing_entries (`bool`):
                if False, do not raise an exception for missing entries but return None.
            _raise_exceptions_for_connection_errors (`bool`):
                if False, do not raise an exception for connection errors but return None.
            _commit_hash (`str`, *optional*):
                passed when we are chaining several calls to various files (e.g. when loading a tokenizer or
                a pipeline). If files are cached for this commit hash, avoid calls to head and get from the cache.

        <Tip>

        Passing `token=True` is required when you want to use a private model.

        </Tip>

        Returns:
            `Optional[str]`: Returns the resolved file (to the cache folder if downloaded from a repo).

        Examples:

        ```python
        # Download a model weight from the Hub and cache it.
        model_weights_file = cached_file("google-bert/bert-base-uncased", "pytorch_model.bin")
        ```
        """
        use_auth_token = deprecated_kwargs.pop("use_auth_token", None)
        if use_auth_token is not None:
            warnings.warn(
                "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.",
                FutureWarning,
            )
            if token is not None:
                raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
            token = use_auth_token

        if is_offline_mode() and not local_files_only:
            logger.info("Offline mode: forcing local_files_only=True")
            local_files_only = True
        if subfolder is None:
            subfolder = ""

        # Add folder to filenames
        full_filenames = [os.path.join(subfolder, file) for file in filenames]

        path_or_repo_id = str(path_or_repo_id)
        existing_files = []
        for filename in full_filenames:
            if os.path.isdir(path_or_repo_id):
                resolved_file = os.path.join(path_or_repo_id, filename)
                if not os.path.isfile(resolved_file):
                    if _raise_exceptions_for_missing_entries and filename != os.path.join(subfolder, "config.json"):
                        revision_ = "main" if revision is None else revision
                        raise OSError(
                            f"{path_or_repo_id} does not appear to have a file named {filename}. Checkout "
                            f"'https://huggingface.co/{path_or_repo_id}/tree/{revision_}' for available files."
                        )
                    else:
                        continue
                existing_files.append(resolved_file)

        if os.path.isdir(path_or_repo_id):
            return existing_files if existing_files else None

        if cache_dir is None:
            cache_dir = TRANSFORMERS_CACHE
        if isinstance(cache_dir, Path):
            cache_dir = str(cache_dir)

        existing_files = []
        file_counter = 0
        if _commit_hash is not None and not force_download:
            for filename in full_filenames:
                # If the file is cached under that commit hash, we return it directly.
                resolved_file = try_to_load_from_cache(
                    path_or_repo_id, filename, cache_dir=cache_dir, revision=_commit_hash, repo_type=repo_type
                )
                if resolved_file is not None:
                    if resolved_file is not _CACHED_NO_EXIST:
                        file_counter += 1
                        existing_files.append(resolved_file)
                    elif not _raise_exceptions_for_missing_entries:
                        file_counter += 1
                    else:
                        raise OSError(f"Could not locate {filename} inside {path_or_repo_id}.")

        # Either all the files were found, or some were _CACHED_NO_EXIST but we do not raise for missing entries
        if file_counter == len(full_filenames):
            return existing_files if len(existing_files) > 0 else None

        user_agent = http_user_agent(user_agent)
        # download the files if needed
        try:
            if len(full_filenames) == 1:
                # This is slightly better for only 1 file
                hf_hub_download(
                    path_or_repo_id,
                    filenames[0],
                    subfolder=None if len(subfolder) == 0 else subfolder,
                    repo_type=repo_type,
                    revision=revision,
                    cache_dir=cache_dir,
                    user_agent=user_agent,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    token=token,
                    local_files_only=local_files_only,
                )
            else:
                snapshot_download(
                    path_or_repo_id,
                    allow_patterns=full_filenames,
                    repo_type=repo_type,
                    revision=revision,
                    cache_dir=cache_dir,
                    user_agent=user_agent,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    token=token,
                    local_files_only=local_files_only,
                )

        except Exception as e:
            # We cannot recover from them
            if isinstance(e, RepositoryNotFoundError) and not isinstance(e, GatedRepoError):
                raise OSError(
                    f"{path_or_repo_id} is not a local folder and is not a valid model identifier "
                    "listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token "
                    "having permission to this repo either by logging in with `hf auth login` or by passing "
                    "`token=<your_token>`"
                ) from e
            elif isinstance(e, RevisionNotFoundError):
                raise OSError(
                    f"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists "
                    "for this model name. Check the model page at "
                    f"'https://huggingface.co/{path_or_repo_id}' for available revisions."
                ) from e
            elif isinstance(e, PermissionError):
                raise OSError(
                    f"PermissionError at {e.filename} when downloading {path_or_repo_id}. "
                    "Check cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); "
                    "2) a previous download was canceled and the lock file needs manual removal."
                ) from e

            # Now we try to recover if we can find all files correctly in the cache
            resolved_files = [
                _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
                for filename in full_filenames
            ]
            if all(file is not None for file in resolved_files):
                return resolved_files

            # Raise based on the flags. Note that we will raise for missing entries at the very end, even when
            # not entering this Except block, as it may also happen when `snapshot_download` does not raise
            if isinstance(e, GatedRepoError):
                if not _raise_exceptions_for_gated_repo:
                    return None
                raise OSError(
                    "You are trying to access a gated repo.\nMake sure to have access to it at "
                    f"https://huggingface.co/{path_or_repo_id}.\n{str(e)}"
                ) from e
            elif isinstance(e, LocalEntryNotFoundError):
                if not _raise_exceptions_for_connection_errors:
                    return None
                # Here we only raise if both flags for missing entry and connection errors are True (because it can be raised
                # even when `local_files_only` is True, in which case raising for connections errors only would not make sense)
                elif _raise_exceptions_for_missing_entries:
                    raise OSError(
                        f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load the files, and couldn't find them in the"
                        f" cached files.\nCheck your internet connection or see how to run the library in offline mode at"
                        " 'https://huggingface.co/docs/transformers/installation#offline-mode'."
                    ) from e
            # snapshot_download will not raise EntryNotFoundError, but hf_hub_download can. If this is the case, it will be treated
            # later on anyway and re-raised if needed
            elif isinstance(e, HTTPError) and not isinstance(e, EntryNotFoundError):
                if not _raise_exceptions_for_connection_errors:
                    return None
>               raise OSError(f"There was a specific connection error when trying to load {path_or_repo_id}:\n{e}") from e
E               OSError: There was a specific connection error when trying to load tiiuae/falcon-mamba-tiny-dev:
E               429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/tiiuae/falcon-mamba-tiny-dev/xet-read-token/f22d33f72a4cc27c1827f3020a6df28c5097fd06 (Request ID: Root=1-68e58b2d-0cc095e3799a7c202b339589;45796e52-5f2d-410c-9c0e-707938b6694d)
E
E               We had to rate limit your IP (169.237.118.139). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API.

.venv/lib/python3.11/site-packages/transformers/utils/hub.py:563: OSError
=============================== warnings summary ===============================
src/peft/tuners/bone/config.py:126: 4 warnings
tests/test_custom_models.py: 241 warnings
tests/test_config.py: 22 warnings
tests/test_decoder_models.py: 166 warnings
tests/test_encoder_decoder_models.py: 38 warnings
tests/test_feature_extraction_models.py: 54 warnings
tests/test_seq_classifier.py: 27 warnings
  /app/src/peft/tuners/bone/config.py:126: UserWarning: Bone will be removed in v0.19.0 of PEFT, use `MissConfig` instead. If you already have a Bone checkpoint, you can use `/scripts/convert-bone-to-miss.py` to convert it into
    warnings.warn(

tests/test_custom_models.py: 3632 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MLP' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 811 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2d' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 207 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv1d' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 116 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv1dBigger' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 129 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MLP_LayerNorm' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 79 warnings
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'torch.nn.modules.normalization.LayerNorm'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_custom_models.py: 58 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2d1x1' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_save_pretrained[Vanilla MLP 1 BOFT-MLP-BOFTConfig-config_kwargs137]
tests/test_custom_models.py::TestPeftCustomModel::test_safe_merge[Vanilla MLP 1 BOFT-MLP-BOFTConfig-config_kwargs137]
tests/test_boft.py::TestBoft::test_boft_state_dict
tests/test_seq_classifier.py::TestSequenceClassificationModels::test_attributes_parametrized[BOFTConfig-config_kwargs1-hf-internal-testing/tiny-random-BertForSequenceClassification]
  /app/.venv/lib/python3.11/site-packages/pkg_resources/_vendor/pyparsing.py:87: DeprecationWarning: module 'sre_constants' is deprecated
    import sre_constants

tests/test_custom_models.py::TestPeftCustomModel::test_save_pretrained[Vanilla MLP 1 BOFT-MLP-BOFTConfig-config_kwargs137]
tests/test_custom_models.py::TestPeftCustomModel::test_safe_merge[Vanilla MLP 1 BOFT-MLP-BOFTConfig-config_kwargs137]
tests/test_boft.py::TestBoft::test_boft_state_dict
tests/test_seq_classifier.py::TestSequenceClassificationModels::test_attributes_parametrized[BOFTConfig-config_kwargs1-hf-internal-testing/tiny-random-BertForSequenceClassification]
  /app/src/peft/tuners/boft/layer.py:95: UserWarning: Failed to load the CUDA extension: Ninja is required to load C++ extensions (pip install ninja to get it), check if ninja is available.
    warnings.warn(f"Failed to load the CUDA extension: {e}, check if ninja is available.")

tests/test_custom_models.py: 353 warnings
tests/test_boft.py: 2 warnings
tests/test_decoder_models.py: 160 warnings
tests/test_encoder_decoder_models.py: 36 warnings
tests/test_feature_extraction_models.py: 46 warnings
tests/test_seq_classifier.py: 27 warnings
  /app/src/peft/tuners/boft/layer.py:96: UserWarning: Setting boft_n_butterfly_factor to 1 to speed up the finetuning process.
    warnings.warn("Setting boft_n_butterfly_factor to 1 to speed up the finetuning process.")

tests/test_custom_models.py: 351 warnings
tests/test_boft.py: 2 warnings
tests/test_decoder_models.py: 160 warnings
tests/test_encoder_decoder_models.py: 36 warnings
tests/test_feature_extraction_models.py: 46 warnings
tests/test_seq_classifier.py: 27 warnings
  /app/src/peft/tuners/boft/layer.py:95: UserWarning: Failed to load the CUDA extension: /root/.cache/torch_extensions/py311_cu128/fbd_cuda/fbd_cuda.so: cannot open shared object file: No such file or directory, check if ninja is available.
    warnings.warn(f"Failed to load the CUDA extension: {e}, check if ninja is available.")

tests/test_custom_models.py: 56 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MLP2' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 56 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2d2' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 385 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'EmbConv1D' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 27 warnings
tests/test_decoder_models.py: 17 warnings
  /app/src/peft/tuners/vera/model.py:275: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py: 30 warnings
tests/test_decoder_models.py: 19 warnings
  /app/src/peft/tuners/vblora/model.py:141: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py: 160 warnings
tests/test_decoder_models.py: 100 warnings
tests/test_initialization.py: 4 warnings
tests/test_tuners_utils.py: 2 warnings
tests/test_helpers.py: 2 warnings
  /app/src/peft/tuners/lora/layer.py:2264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py: 52 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2dGroups' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 104 warnings
tests/test_initialization.py: 6 warnings
  /app/src/peft/tuners/lora/layer.py:1138: UserWarning: LoRA adapter added to ConvNd layer with groups > 1. Merging is not supported.
    warnings.warn("LoRA adapter added to ConvNd layer with groups > 1. Merging is not supported.")

tests/test_custom_models.py: 52 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv2dGroups2' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 300 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Conv3d' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 63 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MHA' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 26 warnings
  /app/src/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'MlpUsingParameters' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.
    warnings.warn(

tests/test_custom_models.py: 26 warnings
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'tests.test_custom_models._LinearUsingParameter'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_custom_models.py: 87 warnings
tests/test_decoder_models.py: 20 warnings
  /app/src/peft/tuners/ia3/model.py:130: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_custom_models.py: 29 warnings
  /app/src/peft/tuners/ia3/model.py:122: UserWarning: fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. Setting fan_in_fan_out to False.
    warnings.warn(

tests/test_custom_models.py: 14 warnings
  /app/src/peft/tuners/tuners_utils.py:1683: UserWarning: All adapters are already merged, nothing to do.
    warnings.warn("All adapters are already merged, nothing to do.")

tests/test_custom_models.py: 10 warnings
tests/test_tuners_utils.py: 2 warnings
  /app/src/peft/tuners/lora/layer.py:728: UserWarning: Already unmerged. Nothing to do.
    warnings.warn("Already unmerged. Nothing to do.")

tests/test_custom_models.py: 110 warnings
tests/test_decoder_models.py: 7 warnings
tests/test_encoder_decoder_models.py: 2 warnings
tests/test_feature_extraction_models.py: 4 warnings
  /app/src/peft/tuners/ia3/layer.py:142: UserWarning: Unmerge result can be inaccurate for (IA)^3.
    warnings.warn("Unmerge result can be inaccurate for (IA)^3.")

tests/test_custom_models.py: 60 warnings
  /app/src/peft/tuners/ia3/layer.py:268: UserWarning: Unmerge result can be inaccurate for (IA)^3.
    warnings.warn("Unmerge result can be inaccurate for (IA)^3.")

tests/test_custom_models.py: 160 warnings
tests/test_decoder_models.py: 112 warnings
tests/test_encoder_decoder_models.py: 22 warnings
tests/test_feature_extraction_models.py: 44 warnings
  /app/src/peft/tuners/tuners_utils.py:1350: UserWarning: Adapter delete_me was active which is now deleted. Setting active adapter to default.
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter[MHA 1 LoRA-MHA-LoraConfig-config_kwargs33]
tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter[MHA 2 LoRA-MHA-LoraConfig-config_kwargs34]
tests/test_custom_models.py::TestPeftCustomModel::test_delete_inactive_adapter[MHA 1 LoRA-MHA-LoraConfig-config_kwargs33]
tests/test_custom_models.py::TestPeftCustomModel::test_delete_inactive_adapter[MHA 2 LoRA-MHA-LoraConfig-config_kwargs34]
  /app/src/peft/tuners/lora/layer.py:1679: UserWarning: Already unmerged. Nothing to do.
    warnings.warn("Already unmerged. Nothing to do.")

tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_with_multiple_adapters_works
tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_modules_to_save
tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_trainable_token_indices
tests/test_mixed.py::TestMixedAdapterTypes::test_delete_adapter
  /app/src/peft/tuners/tuners_utils.py:1350: UserWarning: Adapter adapter0 was active which is now deleted. Setting active adapter to adapter1.
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_modules_to_save
tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_trainable_token_indices
  /app/src/peft/tuners/tuners_utils.py:1350: UserWarning: Adapter adapter1 was active which is now deleted. Setting active adapter to adapter2.
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_delete_adapter_multiple_adapters_with_trainable_token_indices
  /app/src/peft/tuners/tuners_utils.py:1350: UserWarning: Adapter adapter0 was active which is now deleted. Setting active adapter to adapter2.
    warnings.warn(

tests/test_custom_models.py: 1 warning
tests/test_mapping.py: 2 warnings
tests/test_tuners_utils.py: 9 warnings
  /app/src/peft/tuners/tuners_utils.py:279: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
    warnings.warn(

tests/test_custom_models.py::TestPeftCustomModel::test_load_resized_embedding_ignore_mismatched_sizes
  /app/src/peft/utils/save_and_load.py:587: UserWarning: Some weights of PeftModel were not initialized from the model checkpoint and are being ignored because you passed `ignore_mismatched_sizes=True`: - base_model.model.emb.lora_embedding_A.default: found shape torch.Size([8, 100]) in the checkpoint and torch.Size([8, 105]) in the model instantiated.
    warnings.warn(msg)

tests/test_custom_models.py::TestPeftCustomModel::test_load_resized_embedding_ignore_mismatched_sizes
  /app/src/peft/peft_model.py:584: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.emb.lora_embedding_A.default'].
    warnings.warn(warn_message)

tests/test_custom_models.py::TestDynamicDispatch::test_custom_lora_layer_used
tests/test_custom_models.py::TestDynamicDispatch::test_training_works
tests/test_custom_models.py::TestDynamicDispatch::test_saving_and_loading
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'tests.test_custom_models.TestDynamicDispatch.custom_module_cls.<locals>.MyModule'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_config.py::TestPeftConfig::test_from_peft_type
  /app/src/peft/tuners/xlora/config.py:83: UserWarning: No value was provided for `hidden_size`. This will be set to 4096 by default, please ensure that this is correct.
    warnings.warn(

tests/test_config.py::TestPeftConfig::test_from_peft_type
  /app/src/peft/tuners/xlora/config.py:88: UserWarning: No value was provided for for `adapters`. This will be set to empty, please ensure that this is correct.
    warnings.warn(

tests/test_config.py::TestPeftConfig::test_from_peft_type
tests/test_cpt.py::test_model_initialization_text
tests/test_cpt.py::test_model_initialization_random
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-hf-internal-testing/tiny-random-gpt2]
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-hf-internal-testing/tiny-random-OPTForCausalLM]
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-hf-internal-testing/tiny-random-MistralForCausalLM]
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-peft-internal-testing/tiny-dummy-qwen2]
tests/test_decoder_models.py::TestDecoderModels::test_prompt_learning_with_gradient_checkpointing[CPTConfig-config_kwargs3-trl-internal-testing/tiny-random-LlamaForCausalLM]
  /app/src/peft/tuners/cpt/config.py:85: FutureWarning: CPTConfig only supports task_type = CAUSAL_LM, setting it automatically. This will raise an error starting from PEFT v0.18.0.
    warnings.warn(

tests/test_decoder_models.py: 57 warnings
tests/test_encoder_decoder_models.py: 17 warnings
tests/test_seq_classifier.py: 9 warnings
  /app/src/peft/tuners/oft/layer.py:446: UserWarning: Invalid `oft_block_size` (32)! Adjusted `oft_block_size` to (16).
    warnings.warn(

tests/test_decoder_models.py: 14 warnings
  /app/src/peft/tuners/adalora/model.py:211: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_decoder_models.py: 19 warnings
  /app/src/peft/tuners/oft/layer.py:446: UserWarning: Invalid `oft_block_size` (32)! Adjusted `oft_block_size` to (8).
    warnings.warn(

tests/test_config.py: 24 warnings
  /app/src/peft/config.py:280: UserWarning: The configuration file contains a `runtime_config` key. This is ignored. Runtime configurations are only valid at runtime.
    warnings.warn(

tests/test_decoder_models.py: 16 warnings
  /app/src/peft/tuners/waveft/model.py:169: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_decoder_models.py: 18 warnings
  /app/src/peft/tuners/fourierft/model.py:116: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
    warnings.warn(

tests/test_decoder_models.py: 141 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in trl-internal-testing/tiny-random-LlamaForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 141 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in peft-internal-testing/tiny-dummy-qwen2 - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 141 warnings
tests/test_hub_features.py: 7 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-Gemma3ForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 141 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-OPTForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 93 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-GPT2LMHeadModel - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 141 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-BloomForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 141 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-gpt_neo - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 141 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-GPTJForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 141 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-GPTBigCodeForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_cpt.py: 2 warnings
tests/test_decoder_models.py: 16 warnings
  /app/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
    warnings.warn(warn_msg)

tests/test_decoder_models.py::TestDecoderModels::test_prompt_tuning_text_prepare_for_training[PromptTuningConfig-config_kwargs15-trl-internal-testing/tiny-random-LlamaForCausalLM]
tests/test_decoder_models.py::TestDecoderModels::test_prefix_tuning_mistral
tests/test_poly.py::TestPoly::test_poly
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute

tests/test_decoder_models.py::TestDecoderModels::test_prompt_tuning_text_prepare_for_training[PromptTuningConfig-config_kwargs15-trl-internal-testing/tiny-random-LlamaForCausalLM]
tests/test_decoder_models.py::TestDecoderModels::test_prefix_tuning_mistral
tests/test_poly.py::TestPoly::test_poly
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute

tests/test_decoder_models.py: 104 warnings
tests/test_multitask_prompt_tuning.py: 7 warnings
  /app/src/peft/peft_model.py:2101: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.
    warnings.warn("Position ids are not supported for parameter efficient tuning. Ignoring position ids.")

tests/test_decoder_models.py: 9 warnings
tests/test_encoder_decoder_models.py: 2 warnings
  /app/src/peft/tuners/adalora/config.py:96: UserWarning: Note that `r` is not used in AdaLora and will be ignored.If you intended to set the initial rank, use `init_r` instead.
    warnings.warn(

tests/test_encoder_decoder_models.py: 118 warnings
tests/test_hub_features.py: 2 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-BartForConditionalGeneration - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_encoder_decoder_models.py: 119 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in ybelkada/tiny-random-T5ForConditionalGeneration-calibrated - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_feature_extraction_models.py: 80 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-RobertaModel - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_feature_extraction_models.py: 79 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-DebertaModel - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_feature_extraction_models.py: 79 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-DebertaV2Model - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_feature_extraction_models.py: 81 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-BertModel - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_decoder_models.py: 18 warnings
  /app/src/peft/tuners/lora/variants.py:714: UserWarning: Cannot calculate aLoRA offsets when only inputs_embeds are provided. Disabling aLoRA for this forward pass.
    warnings.warn(

tests/test_decoder_models.py::TestDecoderModels::test_lora_layer_replication
  /app/tests/test_decoder_models.py:608: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
    layers[0].mlp.up_proj.base_layer.weight.data.storage().data_ptr()

tests/test_initialization.py::TestVeraInitialization::test_vera_mixing_save_projection_raises
tests/test_vera.py::TestVera::test_multiple_adapters_save_load_save_projection_false
tests/test_vera.py::TestVera::test_multiple_adapters_save_projection_false_contains_no_vera_A_vera_B
  /app/src/peft/tuners/vera/config.py:158: UserWarning: Specified to not save vera_A and vera_B within the state dictionary, instead they will be restored using the PRNG key store in `config.projection_prng_key`. Consider setting `config.save_projection` to `True` to guarantee restoring the checkpoint correctly on all system configurations.
    warnings.warn(

tests/test_initialization.py::TestHotSwapping::test_prepare_model_for_compiled_hotswap_lora_bias
  /app/src/peft/tuners/lora/layer.py:170: PeftWarning: `lora_bias=True` was passed but the targeted layer of type Linear has no bias. This means that merging LoRA weights won't be possible.
    warnings.warn(

tests/test_lora_variants.py::TestLoraVariants::test_variant_is_applied_to_layers[alora-LoraConfig-config_kwargs1]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets[input_ids0-alora_invocation_tokens0-expected_offsets0]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets[input_ids1-alora_invocation_tokens1-expected_offsets1]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets[input_ids2-alora_invocation_tokens2-expected_offsets2]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets_with_adapter_names[input_ids0-alora_invocations0-expected_offsets0]
tests/test_lora_variants.py::TestActivatedLora::test_calculate_alora_offsets_with_adapter_names[input_ids1-alora_invocations1-expected_offsets1]
tests/test_lora_variants.py::TestActivatedLora::test_alora_activation_matches_base_until_invocation
tests/test_lora_variants.py::TestActivatedLora::test_input_embeds_warning
tests/test_lora_variants.py::TestActivatedLora::test_num_beams_error
  /app/src/peft/tuners/lora/config.py:741: UserWarning: aLoRA is currently only supported for CAUSAL_LM task.
    warnings.warn("aLoRA is currently only supported for CAUSAL_LM task.")

tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_transformers_irregular_targets
tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_transformers_target_parameters_raises
tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_transformers_target_parameters_fails
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in facebook/opt-125m - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_extra_keys_warning
  /app/src/peft/tuners/tuners_utils.py:877: UserWarning: You have passed exclude_modules=['model.decoder.layers.5.self_attn.q_proj'] but no modules were excluded. Please check that exclude_modules was set correctly.
    warnings.warn(

tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rank_pattern[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rank_pattern[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_alpha_pattern[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_alpha_pattern[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rslora[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rslora[kpm]
  /app/src/peft/tuners/lora/config.py:729: UserWarning: `corda_config` specified but will be ignored when `init_lora_weights` is not 'corda'.
    warnings.warn("`corda_config` specified but will be ignored when `init_lora_weights` is not 'corda'.")

tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rank_pattern[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rank_pattern[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_alpha_pattern[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_alpha_pattern[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rslora[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_conversion_same_output_after_loading_with_rslora[kpm]
  /app/src/peft/peft_model.py:1261: UserWarning: CorDA changes the base weights of the model and should thus not be used with other adapters. Consider converting the CorDA adapter into a normal LoRA adapter: https://github.com/huggingface/peft/tree/main/examples/corda_finetuning#convert-corda-to-lora
    warnings.warn(msg)

tests/test_initialization.py::TestCordaInitialization::test_lora_corda_rank_pattern_and_rslora_raises[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_rank_pattern_and_rslora_raises[kpm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_alpha_pattern_and_rslora_raises[ipm]
tests/test_initialization.py::TestCordaInitialization::test_lora_corda_alpha_pattern_and_rslora_raises[kpm]
tests/test_initialization.py::TestLoraInitialization::test_olora_rank_pattern_and_rslora_raises
tests/test_initialization.py::TestLoraInitialization::test_olora_alpha_pattern_and_rslora_raises
tests/test_initialization.py::TestLoraInitialization::test_pissa_rank_pattern_and_rslora_raises
tests/test_initialization.py::TestLoraInitialization::test_pissa_alpha_pattern_and_rslora_raises
  /app/src/peft/tuners/lora/config.py:762: UserWarning: Using Rank-Stabilized LoRA with rank_pattern/alpha_pattern and post-training conversion of modified base weights PiSSA/CorDA/OLoRA means that you won't be able to pass `path_initial_model_for_weight_conversion` to `save_pretrained` to restore the initial values of the base weights; if you intend to do this, please ensure not to use rslora or rank_pattern/alpha_pattern.
    warnings.warn(msg)

tests/test_initialization.py::TestEvaInitialization::test_eva_state_dict_adjust_scaling_factors[eva_config0]
tests/test_initialization.py::TestEvaInitialization::test_load_eva_state_dict[True]
tests/test_initialization.py::TestEvaInitialization::test_load_eva_state_dict[False]
tests/test_initialization.py::TestEvaInitialization::test_missing_eva_inits
tests/test_initialization.py::TestEvaInitialization::test_load_eva_model
  /app/src/peft/mapping_func.py:96: UserWarning: lora with eva initialization used with low_cpu_mem_usage=False. Setting low_cpu_mem_usage=True can improve the maximum batch size possible for eva initialization.
    warnings.warn(

tests/test_other.py::TestTargetingAuxiliaryTrainingWrapper::test_targeting_trainable_tokens_raises
tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/tuners/tuners_utils.py:899: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
    warnings.warn(

tests/test_seq_classifier.py: 95 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-BertForSequenceClassification - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_seq_classifier.py: 95 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in hf-internal-testing/tiny-random-RobertaForSequenceClassification - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_seq_classifier.py: 95 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in trl-internal-testing/tiny-LlamaForSequenceClassification-3.2 - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_randlora.py::TestRandLora::test_multiple_adapters_save_load_save_projection_false
tests/test_randlora.py::TestRandLora::test_multiple_adapters_save_projection_false_contains_no_randlora_A_randlora_B
  /app/src/peft/tuners/randlora/config.py:195: UserWarning: Specified to not save basis_A and basis_B within the state dictionary, instead they will be restored using the PRNG key store in `config.projection_prng_key`. Consider setting `config.save_projection` to `True` to guarantee restoring the checkpoint correctly on all system configurations.
    warnings.warn(

tests/test_target_parameters.py: 80 warnings
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssExperts'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_shira.py::TestShira::test_mlp_single_adapter_shapes
  /app/.venv/lib/python3.11/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_shira.py::TestShira::test_mlp_single_adapter_shapes returned <class 'peft.peft_model.PeftModel'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_shira.py::TestShira::test_multiple_adapters_save_load
  /app/.venv/lib/python3.11/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_shira.py::TestShira::test_multiple_adapters_save_load returned <class 'peft.peft_model.PeftModel'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_shira.py::TestShira::test_save_load_custom_mask_function
  /app/src/peft/tuners/shira/config.py:126: UserWarning: Argument self.mask_type='custom' is not recognized, please supply your own masking function by calling `config.mask_fn = my_mask_fn`.
    warnings.warn(

tests/test_shira.py::TestShira::test_save_load_custom_mask_function
  /app/.venv/lib/python3.11/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_shira.py::TestShira::test_save_load_custom_mask_function returned <class 'peft.peft_model.PeftModel'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_shira.py::TestShira::test_save_load_default_random_mask_with_seed_function
  /app/.venv/lib/python3.11/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_shira.py::TestShira::test_save_load_default_random_mask_with_seed_function returned <class 'peft.peft_model.PeftModel'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_target_parameters.py: 81 warnings
  /app/src/peft/tuners/tuners_utils.py:206: UserWarning: Unsupported layer type '<class 'transformers.models.llama4.modeling_llama4.Llama4TextExperts'>' encountered, proceed at your own risk.
    warnings.warn(f"Unsupported layer type '{type(module)}' encountered, proceed at your own risk.", UserWarning)

tests/test_target_parameters.py: 19 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in trl-internal-testing/tiny-Llama4ForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_target_parameters.py: 20 warnings
  /app/src/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in trl-internal-testing/tiny-GptOssForCausalLM - will assume that the vocabulary was not modified.
    warnings.warn(

tests/test_trainable_tokens.py::TestTrainableTokens::test_save_pretrained_auto[peft_config0-True]
tests/test_trainable_tokens.py::TestTrainableTokens::test_save_pretrained_auto[peft_config1-True]
  /app/src/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
    warnings.warn(

tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_0_HuggingFaceH4_tiny_random_LlamaForCausalLM
tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_1_HuggingFaceH4_tiny_random_LlamaForCausalLM
tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_2_hf_internal_testing_tiny_random_gpt2
tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_3_hf_internal_testing_tiny_random_t5
tests/test_tuners_utils.py::PeftCustomKwargsTester::test_all_linear_nested_targets_correct_layers_4_hf_internal_testing_tiny_random_GPTNeoXForCausalLM
  /app/src/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
    warnings.warn(

tests/test_tuners_utils.py::TestModelAndLayerStatus::test_merge_adapters_large
  /app/src/peft/tuners/tuners_utils.py:1678: UserWarning: Already following adapters were merged other. You are now additionally merging default.
    warnings.warn(

tests/test_tuners_utils.py::TestModelAndLayerStatus::test_model_merged_adapters_large
  /app/src/peft/tuners/lora/layer.py:984: UserWarning: Already unmerged. Nothing to do.
    warnings.warn("Already unmerged. Nothing to do.")

tests/test_tuners_utils.py::TestModelAndLayerStatus::test_model_merged_adapters_large
  /app/src/peft/tuners/lora/layer.py:1317: UserWarning: Already unmerged. Nothing to do.
    warnings.warn("Already unmerged. Nothing to do.")

tests/test_vera.py::TestVera::test_multiple_adapters_save_load_save_projection_false
  /app/src/peft/utils/save_and_load.py:531: UserWarning: Specified to not load vera_A and vera_B from state dictionary. This means we will be relying on PRNG initialisation to restore these projections using `config.projection_prng_key`, which may not be accurate on all system configurations.
    warnings.warn(

tests/test_initialization.py::TestLoraInitialization::test_olora_conversion_same_output_after_loading
tests/test_initialization.py::TestLoraInitialization::test_olora_conversion_same_output_after_loading_with_rank_pattern
tests/test_initialization.py::TestLoraInitialization::test_olora_conversion_same_output_after_loading_with_alpha_pattern
tests/test_initialization.py::TestLoraInitialization::test_olora_conversion_same_output_after_loading_with_rslora
  /app/src/peft/peft_model.py:1268: UserWarning: OLoRA changes the base weights of the model and should thus not be used with other adapters. Consider converting the OLoRA adapter into a normal LoRA adapter: https://github.com/huggingface/peft/tree/main/examples/olora_finetuning#olora-and-lora
    warnings.warn(msg)

tests/test_initialization.py::TestLoraInitialization::test_lora_pissa_conversion_same_output_after_loading
tests/test_initialization.py::TestLoraInitialization::test_lora_pissa_conversion_same_output_after_loading_with_rank_pattern
tests/test_initialization.py::TestLoraInitialization::test_lora_pissa_conversion_same_output_after_loading_with_alpha_pattern
tests/test_initialization.py::TestLoraInitialization::test_lora_pissa_conversion_same_output_after_loading_with_rslora
  /app/src/peft/peft_model.py:1254: UserWarning: PiSSA changes the base weights of the model and should thus not be used with other adapters. Consider converting the PiSSA adapter into a normal LoRA adapter: https://github.com/huggingface/peft/tree/main/examples/pissa_finetuning#convert-pissa-to-lora
    warnings.warn(msg)

tests/test_initialization.py::TestLoraInitialization::test_lora_with_bias_incompatible_arguments[extra_kwargs1]
  /app/src/peft/tuners/lora/config.py:718: UserWarning: `init_lora_weights` is 'eva' but `eva_config` is not specified. Using default EVA config.
    warnings.warn("`init_lora_weights` is 'eva' but `eva_config` is not specified. Using default EVA config.")

tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/utils/save_and_load.py:279: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.
    warnings.warn("Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.")

tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/tuners/tuners_utils.py:899: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
    warnings.warn(

tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/tuners/tuners_utils.py:899: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
    warnings.warn(

tests/test_xlora.py::TestXlora::test_functional_embedding
  /app/src/peft/tuners/tuners_utils.py:899: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens', 'model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
_____________ coverage: platform linux, python 3.11.0-candidate-1 ______________

___________________________ coverage: failed workers ___________________________

The following workers failed to return coverage data, ensure that pytest-cov is installed on these workers.
gw2
Name                                                  Stmts   Miss  Cover   Missing
-----------------------------------------------------------------------------------
src/peft/__init__.py                                     10      0   100%
src/peft/auto.py                                         71      4    94%   61, 92, 99, 111
src/peft/config.py                                      168      8    95%   63-64, 72, 144, 211, 297, 322-323
src/peft/functional.py                                    4      4     0%   21-26
src/peft/helpers.py                                      72     21    71%   48-58, 84-98, 124-132, 235
src/peft/import_utils.py                                 94     41    56%   31, 41-46, 55-73, 87-98, 131-147, 158, 161-166
src/peft/mapping.py                                      18      3    83%   44, 78, 81
src/peft/mapping_func.py                                 36      0   100%
src/peft/mixed_model.py                                 154     25    84%   48, 51-54, 112, 120, 137, 143, 163-165, 231-234, 272, 289, 370, 379, 432, 436-439, 443, 448, 452
src/peft/optimizers/__init__.py                           3      0   100%
src/peft/optimizers/lorafa.py                           105     25    76%   67, 69, 71, 73, 93, 113, 127-131, 157, 176, 182-210
src/peft/optimizers/loraplus.py                          36      3    92%   72, 76, 79
src/peft/peft_model.py                                 1345    450    67%   160-166, 172, 219, 228, 245, 260, 300, 324-327, 444, 451, 471, 474-501, 506, 509, 513-535, 579, 632, 656, 681, 700-701, 748-749, 757, 767-769, 793, 808, 842, 848-854, 873-875, 910-912, 920, 957, 1005, 1052, 1073, 1171-1238, 1243, 1373-1420, 1452, 1459, 1476, 1529, 1534-1536, 1540-1543, 1549, 1675-1727, 1740-1793, 1866-1868, 1879, 1901-1902, 1904-1905, 2041, 2049, 2059-2064, 2073-2095, 2105-2108, 2119, 2233-2320, 2337, 2339-2342, 2344-2347, 2359-2360, 2381-2387, 2399, 2477-2478, 2510-2517, 2531-2584, 2597-2633, 2698-2699, 2731-2738, 2755-2811, 2825-2876, 2942, 2963-2964, 2966-2967, 3068, 3213
src/peft/tuners/__init__.py                              30      0   100%
src/peft/tuners/_buffer_dict.py                          62      9    85%   76, 83, 92-94, 123, 137, 141, 159
src/peft/tuners/adalora/__init__.py                      16      7    56%   33-43
src/peft/tuners/adalora/bnb.py                           74     60    19%   38-44, 48-75, 78-79, 96-102, 106-139, 142-143
src/peft/tuners/adalora/config.py                        36      2    94%   88, 92
src/peft/tuners/adalora/gptq.py                          31     26    16%   30-37, 40-67
src/peft/tuners/adalora/layer.py                        219     94    57%   31, 53, 58, 126, 142, 152-153, 169, 217, 236-252, 256-273, 278, 281-283, 286-335, 339-347, 351-360
src/peft/tuners/adalora/model.py                        152     76    50%   78, 104, 129, 136, 168, 181-188, 190-198, 200, 204-208, 217, 230-252, 256-284, 287-300, 323-342, 346
src/peft/tuners/adaption_prompt/__init__.py               6      0   100%
src/peft/tuners/adaption_prompt/config.py                25      1    96%   81
src/peft/tuners/adaption_prompt/layer.py                 86      5    94%   39, 54, 171, 185, 192
src/peft/tuners/adaption_prompt/model.py                 85      5    94%   64, 72, 98, 100, 168
src/peft/tuners/adaption_prompt/utils.py                 68     34    50%   47-58, 73, 84-89, 100-128, 141-146
src/peft/tuners/boft/__init__.py                          6      0   100%
src/peft/tuners/boft/config.py                           30      3    90%   152, 154, 158
src/peft/tuners/boft/fbd/__init__.py                      0      0   100%
src/peft/tuners/boft/layer.py                           495    115    77%   65-71, 78, 130-132, 136-138, 168, 234, 240-244, 247-254, 257-261, 283, 288, 301, 306-311, 319, 324-329, 336, 342-346, 388, 422-446, 520-532, 550-551, 583, 595, 614, 625, 639, 713, 718, 724, 736, 741-746, 754, 759-764, 771, 777-781, 833-847, 872-873, 916, 928, 951, 962, 975
src/peft/tuners/boft/model.py                            35      6    83%   78, 96, 111, 117-121, 126
src/peft/tuners/bone/__init__.py                          6      0   100%
src/peft/tuners/bone/config.py                           26      2    92%   120, 124
src/peft/tuners/bone/layer.py                           186     44    76%   46, 64, 74, 79, 99-106, 109-113, 160-173, 188-189, 222, 243-246, 268, 273-295, 306-309, 327, 338, 342-343
src/peft/tuners/bone/model.py                            29      4    86%   89, 103, 115, 122
src/peft/tuners/c3a/__init__.py                           6      0   100%
src/peft/tuners/c3a/config.py                            23      2    91%   133, 137
src/peft/tuners/c3a/layer.py                            112     13    88%   47, 51, 61, 97, 103-108, 155, 171-172, 192
src/peft/tuners/c3a/model.py                             32      3    91%   63, 84, 90
src/peft/tuners/c3a/utils.py                             30      0   100%
src/peft/tuners/cpt/__init__.py                           5      0   100%
src/peft/tuners/cpt/config.py                            35      1    97%   106
src/peft/tuners/cpt/model.py                             84      0   100%
src/peft/tuners/fourierft/__init__.py                     6      0   100%
src/peft/tuners/fourierft/config.py                      30      3    90%   199, 203, 206
src/peft/tuners/fourierft/layer.py                      104      7    93%   52, 58, 60, 149, 159-160, 182
src/peft/tuners/fourierft/model.py                       47      6    87%   67, 96, 102, 108-112, 121
src/peft/tuners/hra/__init__.py                           6      0   100%
src/peft/tuners/hra/config.py                            27      3    89%   125, 129, 133
src/peft/tuners/hra/layer.py                            248     53    79%   50, 70, 85, 99-100, 111-118, 121-125, 172-181, 193-194, 213-220, 250, 262, 315-334, 358-359, 385-392, 424, 447
src/peft/tuners/hra/model.py                             31      4    87%   89, 104, 117, 126
src/peft/tuners/ia3/__init__.py                          15      1    93%   39
src/peft/tuners/ia3/bnb.py                               67     53    21%   36-42, 46-69, 72-73, 88-94, 98-125, 128-129
src/peft/tuners/ia3/config.py                            22      0   100%
src/peft/tuners/ia3/layer.py                            194     11    94%   44, 50, 139-140, 169, 246, 265-266, 298, 322, 330
src/peft/tuners/ia3/model.py                            120     21    82%   92, 97-105, 107-115, 138, 173, 192, 198, 219, 222, 234, 241, 247, 251, 256, 284, 307, 314
src/peft/tuners/ln_tuning/__init__.py                     5      0   100%
src/peft/tuners/ln_tuning/config.py                      13      0   100%
src/peft/tuners/ln_tuning/layer.py                       64      7    89%   76, 82-83, 93-94, 106, 112
src/peft/tuners/ln_tuning/model.py                       44      1    98%   102
src/peft/tuners/loha/__init__.py                          6      0   100%
src/peft/tuners/loha/config.py                           25      1    96%   143
src/peft/tuners/loha/layer.py                           207      2    99%   126, 166
src/peft/tuners/loha/model.py                            22      0   100%
src/peft/tuners/lokr/__init__.py                          6      0   100%
src/peft/tuners/lokr/config.py                           28      1    96%   155
src/peft/tuners/lokr/layer.py                           230     15    93%   82-88, 146-147, 156, 186, 239, 286, 479-481
src/peft/tuners/lokr/model.py                            23      0   100%
src/peft/tuners/lora/__init__.py                         21      2    90%   60-62
src/peft/tuners/lora/aqlm.py                             48     31    35%   25, 42-49, 62-85, 88-89, 111-112
src/peft/tuners/lora/arrow.py                           210     16    92%   177, 212, 217, 316, 318, 340-341, 346-347, 389, 400, 411, 420, 434, 444, 450
src/peft/tuners/lora/awq.py                              55     37    33%   39-50, 62-85, 88-89, 105-119
src/peft/tuners/lora/bnb.py                             316    257    19%   74-76, 81-86, 101-146, 152-182, 185, 198-246, 249-293, 296-297, 340-345, 360-373, 388-432, 438-467, 470, 483-531, 534-585, 588-589, 601-609
src/peft/tuners/lora/config.py                          134     11    92%   117, 119, 707, 709, 714-715, 724-727, 781-783
src/peft/tuners/lora/corda.py                           173     17    90%   51, 111, 168, 181, 186, 246, 262, 284, 293, 298, 303, 333, 337, 342, 347, 352, 357
src/peft/tuners/lora/dora.py                            107      4    96%   48, 63, 97, 164
src/peft/tuners/lora/eetq.py                             55     42    24%   24-96, 112-116
src/peft/tuners/lora/eva.py                             333     55    83%   63, 67, 82-102, 132-135, 156-157, 164-165, 221-222, 234-238, 250, 253, 273, 277, 310-313, 322, 332-334, 383-385, 392, 434-436, 454, 461, 470, 544, 632, 713, 718, 722, 727
src/peft/tuners/lora/gptq.py                             65     43    34%   42-52, 66-80, 84-114, 117-118, 142-146, 151-152
src/peft/tuners/lora/hqq.py                             132    115    13%   30-237, 249
src/peft/tuners/lora/inc.py                              23     10    57%   31-59, 71-76
src/peft/tuners/lora/layer.py                          1182    135    89%   58, 63, 91, 167, 220-221, 250, 260, 271, 281, 285, 299-306, 308-313, 322, 341, 358, 368, 380, 384, 388, 394, 400, 406, 428-446, 479, 492, 505, 522-526, 531-532, 539-540, 696, 701, 715, 892, 901, 923, 967, 1036-1059, 1083, 1091, 1136, 1181, 1197, 1225, 1265, 1278, 1286, 1291, 1305, 1360, 1367, 1389, 1399, 1431, 1448, 1465, 1516, 1534, 1635, 1643, 1679-1680, 1771, 1789-1790, 1870-1873, 1913, 1920, 1922, 1924, 1926, 1928, 1965, 1971, 1978, 1991, 2001-2002, 2004-2005, 2007-2008, 2010-2011, 2013, 2015-2016, 2023, 2042, 2047, 2075-2076, 2081, 2085, 2104, 2126, 2166-2167, 2179, 2203, 2255-2259
src/peft/tuners/lora/model.py                           346     61    82%   170, 221, 243, 270, 272, 291-306, 341, 372, 385, 393, 428, 430, 437, 449, 453, 467, 497, 501, 503, 509, 515, 573, 601-605, 615-619, 624, 680, 694, 698-702, 704, 713-717, 719-720, 740-744, 763, 779
src/peft/tuners/lora/torchao.py                          80     57    29%   35-40, 44-54, 57-91, 94-124, 127-128, 150-156
src/peft/tuners/lora/tp_layer.py                        169    140    17%   56-99, 117-186, 189-216, 231-256, 262-270, 280-304, 307-308, 325, 333-346
src/peft/tuners/lora/variants.py                        396     66    83%   55, 121, 129, 144-152, 229-230, 248, 338, 407-408, 454-486, 490, 494, 498, 502, 512-541, 551, 555, 559, 580, 621, 637-639, 694-695, 726, 750
src/peft/tuners/lycoris_utils.py                        104     26    75%   98-101, 140, 153-156, 159-166, 173-174, 181-188, 241-242, 257-258
src/peft/tuners/miss/__init__.py                          6      0   100%
src/peft/tuners/miss/config.py                           26      2    92%   136, 140
src/peft/tuners/miss/layer.py                           214     57    73%   50, 70, 75, 86, 91, 94-100, 116, 122-129, 132-136, 185-201, 207-208, 219-220, 231, 255, 276-279, 301, 307, 310-332, 343-346, 364, 375, 378, 383-384
src/peft/tuners/miss/model.py                            29      4    86%   89, 105, 119, 126
src/peft/tuners/mixed/__init__.py                         2      0   100%
src/peft/tuners/mixed/model.py                          166     40    76%   82, 102-107, 119, 127-131, 150-160, 167, 172, 182-187, 195-196, 204, 220, 249-255, 260, 270, 276
src/peft/tuners/multitask_prompt_tuning/__init__.py       5      0   100%
src/peft/tuners/multitask_prompt_tuning/config.py        21      0   100%
src/peft/tuners/multitask_prompt_tuning/model.py         48      4    92%   38, 65, 71-73
src/peft/tuners/oft/__init__.py                          19     10    47%   37-52
src/peft/tuners/oft/aqlm.py                              41     25    39%   25, 45-49, 64-82, 85-86, 97, 102-103
src/peft/tuners/oft/awq.py                               49     32    35%   42-50, 64-83, 86-87, 98, 103-117
src/peft/tuners/oft/bnb.py                              188    149    21%   48-53, 79-115, 121-150, 153, 156-179, 182-183, 189, 195-203, 227-232, 258-293, 299-326, 329, 332-362, 365-366, 372, 378-386
src/peft/tuners/oft/config.py                            39      4    90%   171, 173, 177, 191
src/peft/tuners/oft/eetq.py                              48     36    25%   24-94, 105, 110-114
src/peft/tuners/oft/gptq.py                              49     30    39%   41-48, 63-84, 87-88, 99, 106-110, 115-116
src/peft/tuners/oft/hqq.py                               94     79    16%   29-172, 179, 184
src/peft/tuners/oft/inc.py                               23     11    52%   31-59, 66, 71-76
src/peft/tuners/oft/layer.py                            425     95    78%   52-69, 193, 218, 249, 344-368, 380-384, 387-394, 397-401, 452-454, 457, 499, 506-508, 512-519, 586-597, 614-615, 654, 729, 737, 742-748, 751-753, 756, 811-824, 848-849, 898, 923, 931-935
src/peft/tuners/oft/model.py                             54      6    89%   101, 122, 130, 183, 197, 199
src/peft/tuners/p_tuning/__init__.py                      5      0   100%
src/peft/tuners/p_tuning/config.py                       17      0   100%
src/peft/tuners/p_tuning/model.py                        34      7    79%   84-96, 119, 124, 128
src/peft/tuners/poly/__init__.py                          6      0   100%
src/peft/tuners/poly/config.py                           21      0   100%
src/peft/tuners/poly/layer.py                            89      9    90%   49, 56, 103-108, 137
src/peft/tuners/poly/model.py                            52      5    90%   43, 52, 58, 65, 73
src/peft/tuners/poly/router.py                           34      3    91%   31, 64, 66
src/peft/tuners/prefix_tuning/__init__.py                 5      0   100%
src/peft/tuners/prefix_tuning/config.py                  10      0   100%
src/peft/tuners/prefix_tuning/model.py                   19      4    79%   65-66, 76-77
src/peft/tuners/prompt_tuning/__init__.py                 5      0   100%
src/peft/tuners/prompt_tuning/config.py                  23      0   100%
src/peft/tuners/prompt_tuning/model.py                   30      0   100%
src/peft/tuners/randlora/__init__.py                     15      1    93%   40
src/peft/tuners/randlora/bnb.py                         209    181    13%   46-51, 75-105, 111-135, 147-183, 194-201, 218-251, 254-255, 274-278, 302-327, 333-350, 364-398, 408-415, 418-452, 455-456
src/peft/tuners/randlora/config.py                       26      0   100%
src/peft/tuners/randlora/layer.py                       176     13    93%   80-81, 106, 109, 133, 147, 150, 159, 162, 235, 251-252, 339
src/peft/tuners/randlora/model.py                       144     25    83%   60, 119-123, 133-134, 239, 255, 285, 304, 309-317, 319-327, 330-343
src/peft/tuners/road/__init__.py                         15      1    93%   47
src/peft/tuners/road/bnb.py                             195    157    19%   43-47, 67-113, 119-158, 161-190, 193-194, 200, 206-214, 232-236, 256-301, 307-344, 347-381, 384-385, 391, 397-405
src/peft/tuners/road/config.py                           21      1    95%   124
src/peft/tuners/road/layer.py                           199     30    85%   70, 102, 151-162, 175, 182, 202-228, 265, 276, 298-299, 380, 411
src/peft/tuners/road/model.py                            77     23    70%   34-35, 55, 88, 136-163
src/peft/tuners/shira/__init__.py                         6      0   100%
src/peft/tuners/shira/config.py                          25      0   100%
src/peft/tuners/shira/layer.py                          102     16    84%   48, 64, 72, 95, 103, 106-109, 128, 158-167, 174-175
src/peft/tuners/shira/mask_functions.py                  14      0   100%
src/peft/tuners/shira/model.py                           42      6    86%   72, 81, 109, 115-121
src/peft/tuners/trainable_tokens/__init__.py              6      0   100%
src/peft/tuners/trainable_tokens/config.py               13      0   100%
src/peft/tuners/trainable_tokens/layer.py               115     13    89%   88-105, 127, 180, 187-188, 242
src/peft/tuners/trainable_tokens/model.py                41      0   100%
src/peft/tuners/tuners_utils.py                         775     78    90%   78-79, 88-105, 110, 114-123, 140-141, 160, 177, 181, 184, 187, 190, 193, 196, 199, 284, 346, 456-460, 478, 499, 648, 857, 859, 870, 872, 987, 1035-1039, 1046, 1048, 1051-1054, 1214, 1226, 1229, 1251, 1373, 1497, 1530, 1575, 1603, 1671, 1718, 1725-1730, 1732, 1749-1754, 1789-1790
src/peft/tuners/vblora/__init__.py                        6      0   100%
src/peft/tuners/vblora/config.py                         29      1    97%   196
src/peft/tuners/vblora/layer.py                         130      6    95%   75, 77, 165, 175-176, 245
src/peft/tuners/vblora/model.py                          77     13    83%   90, 121, 127, 133-137, 146, 184-189, 205-206
src/peft/tuners/vera/__init__.py                         15      1    93%   40
src/peft/tuners/vera/bnb.py                             208    182    12%   46-51, 62-98, 101-125, 143-178, 195-237, 240-241, 260-265, 276-309, 312-329, 334-361, 364-407, 410-411
src/peft/tuners/vera/config.py                           28      1    96%   156
src/peft/tuners/vera/layer.py                           149     13    91%   81, 99, 115, 125, 201, 208-209, 239-242, 251, 268
src/peft/tuners/vera/model.py                           119     16    87%   57, 123, 133-134, 194, 222, 241, 246-254, 256-264, 267-271, 280
src/peft/tuners/waveft/__init__.py                        6      0   100%
src/peft/tuners/waveft/config.py                         36      2    94%   253, 257
src/peft/tuners/waveft/constants.py                       1      0   100%
src/peft/tuners/waveft/layer.py                         145      6    96%   64, 132, 243, 257-258, 280
src/peft/tuners/waveft/model.py                          97     18    81%   46-49, 52-55, 59, 84, 108, 118, 149, 155, 161-165, 174, 192-195
src/peft/tuners/waveft/wavelet.py                        46      8    83%   65, 69, 113, 118, 123, 127, 130, 513
src/peft/tuners/waveft/waverec2d.py                     201     59    71%   45, 68, 85, 90-91, 96, 102, 120-126, 130, 147-150, 154-156, 160-163, 167-168, 172, 177, 180, 182, 184-188, 191, 194-195, 197, 203, 205, 207, 210-212, 214-218, 233, 251-255, 283, 288, 299-302
src/peft/tuners/xlora/__init__.py                         5      0   100%
src/peft/tuners/xlora/classifier.py                      88     11    88%   74-77, 80, 118-120, 142-143, 185-186
src/peft/tuners/xlora/config.py                          36      3    92%   94, 97, 102
src/peft/tuners/xlora/layer.py                          110     29    74%   114, 158, 161, 170-171, 186, 194-223
src/peft/tuners/xlora/model.py                          209     17    92%   75-85, 148, 241, 257, 261, 266-267, 350, 399, 405, 437, 442, 445
src/peft/utils/__init__.py                                7      0   100%
src/peft/utils/constants.py                              64     16    75%   23-32, 37-43, 50, 59
src/peft/utils/hotswap.py                               204     29    86%   46, 50, 75-76, 130, 136, 172, 175, 210, 216, 356, 360, 423, 457, 474-500, 542, 607
src/peft/utils/incremental_pca.py                       148     10    93%   74, 76-77, 103, 121, 142, 146, 148, 199-200
src/peft/utils/integrations.py                          162     87    46%   33, 45-49, 61-62, 65-66, 70-73, 79-86, 94-127, 133, 135, 145-166, 178-194, 217, 231-233, 237, 245-249, 254, 256, 261, 263
src/peft/utils/loftq_utils.py                           234    202    14%   37-49, 53-61, 65-87, 90-103, 106-113, 116-154, 158-170, 177-187, 192-237, 242-258, 270-308, 311-327, 366-409
src/peft/utils/merge_utils.py                            79      7    91%   91-92, 94, 100, 120-123
src/peft/utils/other.py                                 664    183    72%   115, 117, 119, 121, 123, 189-192, 200, 225-234, 265, 269, 276, 318, 324, 335, 362-370, 379-383, 386, 391, 395, 408, 458, 472-477, 481, 485, 494, 502, 523, 527, 546-549, 556-559, 593, 602, 624, 634, 649-678, 693, 713-715, 772-775, 835, 841, 845, 868-869, 882-883, 900, 912, 926-928, 973, 1049, 1060, 1073, 1100, 1115-1153, 1170-1174, 1184, 1202, 1213-1239, 1246-1276, 1295-1297, 1319-1323, 1339, 1358-1359, 1467-1509
src/peft/utils/peft_types.py                             61      6    90%   143, 146, 149, 162, 165, 169
src/peft/utils/save_and_load.py                         348     56    84%   54, 102, 110-112, 117-119, 138-149, 171, 193, 206-211, 228, 236, 325, 362-381, 450, 502, 505, 521, 525, 549-553, 577-587, 626, 651, 683-684, 691, 709
src/peft/utils/warning.py                                 1      0   100%
-----------------------------------------------------------------------------------
TOTAL                                                 17510   4549    74%
============================= slowest 10 durations =============================
26.23s call     tests/test_poly.py::TestPoly::test_poly
22.44s call     tests/test_auto.py::TestPeftAutoModel::test_embedding_size_not_reduced_if_greater_vocab_size
14.44s call     tests/test_xlora.py::TestXlora::test_save_load_functional_pt
13.55s call     tests/test_xlora.py::TestXlora::test_save_load_functional
12.54s call     tests/test_custom_models.py::TestPeftCustomModel::test_dora_save_and_load_remapping
10.58s call     tests/test_auto.py::TestPeftAutoModel::test_peft_whisper
10.38s call     tests/test_xlora.py::TestXlora::test_scalings_logging_methods
9.60s call     tests/test_encoder_decoder_models.py::TestEncoderDecoderModels::test_save_pretrained[RoadConfig-config_kwargs13-hf-internal-testing/tiny-random-BartForConditionalGeneration]
6.80s call     tests/test_decoder_models.py::TestDecoderModels::test_prompt_tuning_text_prepare_for_training[PromptTuningConfig-config_kwargs15-hf-internal-testing/tiny-random-Gemma3ForCausalLM]
6.69s setup    tests/test_xlora.py::TestXlora::test_functional
=========================== short test summary info ============================
FAILED tests/test_gpu_examples.py::TestFSDPWrap::test_bnb_4bit_wrap_fsdp - At...
FAILED tests/test_gpu_examples.py::TestLowCpuMemUsageDifferentDevices::test_low_cpu_mem_usage_with_quantization[bnb-4bit]
FAILED tests/test_loraplus.py::test_lora_plus_optimizer_sucess - AttributeErr...
FAILED tests/test_low_level_api.py::TestInjectAdapterFromStateDict::test_inject_from_state_dict_stable_diffusion
FAILED tests/test_initialization.py::TestHotSwapping::test_hotswap_works[True-config0]
FAILED tests/test_initialization.py::TestHotSwapping::test_hotswap_works[True-config1]
FAILED tests/test_tuners_utils.py::PeftCustomKwargsTester::test_maybe_include_all_linear_layers_diffusion
FAILED tests/test_hub_features.py::TestBaseModelRevision::test_load_different_peft_and_base_model_revision
FAILED tests/test_helpers.py::TestScalingAdapters::test_diffusers_pipeline - ...
FAILED tests/test_hub_features.py::TestBaseModelRevision::test_save_and_load_base_model_revision
FAILED tests/test_initialization.py::TestLoraInitialization::test_lora_incompatible_mamba_modules
ERROR tests/test_stablediffusion.py - TypeError: CLIPTextModel.__init__() got...
ERROR tests/test_stablediffusion.py - TypeError: CLIPTextModel.__init__() got...
ERROR tests/test_stablediffusion.py - TypeError: CLIPTextModel.__init__() got...
ERROR tests/test_stablediffusion.py - TypeError: CLIPTextModel.__init__() got...
ERROR tests/test_incremental_pca.py::test_incremental_pca - huggingface_hub.e...
ERROR tests/test_incremental_pca.py::test_incremental_pca_lowrank - huggingfa...
= 11 failed, 14955 passed, 4651 skipped, 10 xfailed, 11735 warnings, 6 errors in 966.82s (0:16:06) =
make: *** [Makefile:20: test] Error 1
